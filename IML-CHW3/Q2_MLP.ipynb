{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_FQqz9h3BW3"
      },
      "source": [
        "\n",
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
        "<font color=0F5298 size=7>\n",
        "Introduction to Machine Learning - 25732  <br>\n",
        "<font color=2565AE size=5>\n",
        "Department of Electrical Enginnering <br>\n",
        "Fall 2024<br>\n",
        "<font color=3C99D size=5>\n",
        "Dr. Mohammad Hossein Yassaee <br>\n",
        "<font color=696880 size=4>\n",
        "Armin Dehghan\n",
        "\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkdwhXBV3BW9"
      },
      "source": [
        "### Information:\n",
        "\n",
        "1.   Full Name: Seyyed Amirmahdi Sadrzadeh\n",
        "2.   Student Number: 401102015\n",
        "\n",
        "---\n",
        "\n",
        "*I. You are just allowded to change those parts that start with \"TO DO\". Please do not change other parts.*\n",
        "\n",
        "*II. It is highly recommended to read each codeline carefully and try to understand what it exactly does.*\n",
        "\n",
        "*III. Do not copy codes completely from internet sources such as Chat-GPT or etc. If you are using any sources, please put its link in the beging of the block.*\n",
        "\n",
        "If you have any question you can contact related TAs:\n",
        " - Homeworks coordinator: @danialayati\n",
        " - This Notebook: @armin_dh\n",
        "\n",
        " Best of luck and have fun!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz5CmA1W3BW-"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NhaKP7Ch3BW-"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3m_ahbk3BXB"
      },
      "source": [
        "## Introduction:\n",
        "\n",
        "In this Notebook, we introduce the fundamental concepts of neural networks, focusing specifically on the architecture and functionality of Multilayer Perceptrons (MLPs). MLPs are a class of feedforward artificial neural networks that consist of multiple layers of nodes, each layer fully connected to the next. These networks are widely used for a variety of machine learning tasks, including classification and regression. The core idea behind MLPs is to learn complex functions by adjusting the weights and biases of the network through a process known as training, typically involving backpropagation and an optimization algorithm like gradient descent.\n",
        "\n",
        "\n",
        "To provide a practical context to these concepts, we will be working with the MNIST dataset, a large collection of handwritten digits commonly used for training and testing in the field of machine learning. The dataset consists of 60,000 training images and 10,000 test images, each being a 28x28 pixel grayscale image of digits ranging from 0 to 9. The simplicity and size of the MNIST dataset make it an excellent choice for exploring and understanding the mechanics of neural networks, particularly in recognizing patterns and classifying images. Throughout this notebook, we will apply MLPs to this dataset, exploring every step of the process from data preparation and exploration to model training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSh-nL0H3BXB"
      },
      "source": [
        "## Getting the Data\n",
        "\n",
        "\n",
        "The following Python cell performs two primary actions: it loads the MNIST dataset and then splits it into two distinct sets - one for training and one for testing.\n",
        "\n",
        "- `X_train, Y_train`: These variables represent the training set. `X_train` contains the images of handwritten digits, and `Y_train` contains the corresponding labels that indicate the actual digit each image represents (0 through 9).\n",
        "- `X_test, Y_test`: Similarly, these variables represent the test set. `X_test` includes the images reserved for testing the model's performance, while `Y_test` contains the labels for these images.\n",
        "\n",
        "By executing the next cell, you will download the dataset and prepare these variables.\n",
        "\n",
        "**There is no need to change the following cell.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eadaEra3BXC",
        "outputId": "b2a0ef9b-af9c-4e1c-ba0c-31c2e98634b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 39.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.23MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.15MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "X_train_torch = trainset.data\n",
        "Y_train_torch = trainset.targets\n",
        "\n",
        "X_test_torch = testset.data\n",
        "Y_test_torch = testset.targets\n",
        "\n",
        "\n",
        "\n",
        "# Training Images:\n",
        "X_train = X_train_torch.numpy()\n",
        "# Training Labels:\n",
        "Y_train = Y_train_torch.numpy()\n",
        "\n",
        "# Test Images:\n",
        "X_test = X_test_torch.numpy()\n",
        "# Test Labels:\n",
        "Y_test = Y_test_torch.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llIFk2vt3BXC"
      },
      "source": [
        "## Exploratory Data Analysis: (5 Points)\n",
        "\n",
        "Before diving into model building, it's crucial to perform an exploratory data analysis (EDA) to understand the characteristics and structure of the dataset at hand. EDA involves visualizing the data, identifying patterns, understanding the distribution of variables, and spotting any anomalies or outliers. This step not only helps in making informed decisions about data preprocessing and model design but also provides insights that could be useful for feature engineering.\n",
        "\n",
        "Here are two examples of EDA for the MNIST dataset, which consists of handwritten digits:\n",
        "\n",
        "1. **Visualizing Sample Images**:\n",
        "   - Display a few sample images from the dataset to get a sense of what the handwritten digits look like. This can help in understanding the variety of handwriting styles and the clarity of images.\n",
        "\n",
        "</br>\n",
        "\n",
        "2. **Checking the Distribution of Classes**:\n",
        "   - Plot the frequency of each digit (0-9) in the dataset to check if the dataset is balanced. This is crucial as an imbalanced dataset may need special treatment during training, such as class weighting or resampling techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "ZG5eqjJ23BXD",
        "outputId": "8455283c-4035-44fa-92ce-6fd8d25ddc74"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACvCAYAAACVbcM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbyUlEQVR4nO3de1SVVf7H8e9REfCCjIpalqh5y8lbXocxL4lZXgqTNMtbOebKG8uljqNjysykecMUb7l0eSFdi1wqajZNNiNWloOS6SwyjLxEGMtAA8Qbw/D8/pifTs/ZWzkezuZwDu/XWv6xP+7znK+0A7487Gc7LMuyBAAAAAA8rIq3CwAAAADgn2g2AAAAABhBswEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYATNBgAAAAAjKn2zceHCBXE4HLJ8+XKPXfPw4cPicDjk8OHDHrsm/BPrD97E+oO3sQbhTay/8uGTzcbWrVvF4XBIamqqt0sxIjY2VhwOh/InKCjI26VB/H/9iYhcvHhRhg8fLqGhoRISEiLPPfecnDt3zttlQSrH+vul/v37i8PhkClTpni7FPw/f1+DZ86ckenTp0tERIQEBQWJw+GQCxcueLss/D9/X38iIomJifL4449LUFCQhIWFyfjx4yU3N9fbZbmtmrcLwN2tX79eatWqdWdctWpVL1aDyqKwsFD69u0r+fn5MnfuXAkICJC3335bevfuLSdPnpR69ep5u0RUEnv27JGjR496uwxUMkePHpX4+Hhp27atPProo3Ly5Elvl4RKZP369TJp0iTp16+frFixQrKysmTVqlWSmpoqKSkpPvmDZ5qNCiw6Olrq16/v7TJQyaxbt04yMjLk2LFj0rVrVxEReeaZZ+Sxxx6TuLg4WbRokZcrRGVw8+ZNmTFjhsyePVvmz5/v7XJQiTz77LOSl5cntWvXluXLl9NsoNwUFRXJ3LlzpVevXvLxxx+Lw+EQEZGIiAgZMmSIbNy4UaZOnerlKu+fT/4alSuKiopk/vz50rlzZ6lTp47UrFlTnnjiCUlOTr7ra95++20JDw+X4OBg6d27t6SlpSlz0tPTJTo6WurWrStBQUHSpUsX2b9/f6n1XL9+XdLT0+/rNphlWVJQUCCWZbn8GlQMvrz+du3aJV27dr3TaIiItGnTRvr16yc7d+4s9fXwPl9ef7ctXbpUSkpKZObMmS6/BhWHL6/BunXrSu3atUudh4rLV9dfWlqa5OXlyYgRI+40GiIigwcPllq1akliYmKp71UR+W2zUVBQIJs2bZI+ffrIkiVLJDY2VnJycmTAgAHan1IkJCRIfHy8TJ48WebMmSNpaWny5JNPyqVLl+7M+frrr6VHjx7yzTffyB/+8AeJi4uTmjVrSlRUlCQlJd2znmPHjsmjjz4qa9ascfnf0Lx5c6lTp47Url1bRo0aZasFFZuvrr+SkhL517/+JV26dFH+rlu3bnL27Fm5evWqax8EeI2vrr/bMjMzZfHixbJkyRIJDg6+r387KgZfX4Pwbb66/m7duiUiov28FxwcLF999ZWUlJS48BGoYCwftGXLFktErOPHj991TnFxsXXr1i1b9vPPP1sNGza0Xn311TvZ+fPnLRGxgoODraysrDt5SkqKJSLW9OnT72T9+vWz2rVrZ928efNOVlJSYkVERFgtW7a8kyUnJ1siYiUnJyvZggULSv33rVy50poyZYq1Y8cOa9euXVZMTIxVrVo1q2XLllZ+fn6pr4dZ/rz+cnJyLBGx/vznPyt/t3btWktErPT09HteA2b58/q7LTo62oqIiLgzFhFr8uTJLr0W5lWGNXjbsmXLLBGxzp8/f1+vgzn+vP5ycnIsh8NhjR8/3panp6dbImKJiJWbm3vPa1REfntno2rVqlK9enUR+e9Pa69cuSLFxcXSpUsXOXHihDI/KipKGjdufGfcrVs36d69u/z1r38VEZErV67IoUOHZPjw4XL16lXJzc2V3NxcuXz5sgwYMEAyMjLk4sWLd62nT58+YlmWxMbGllp7TEyMrF69Wl566SUZNmyYrFy5UrZt2yYZGRmybt26+/xIwBt8df3duHFDREQCAwOVv7u9Ke32HFRcvrr+RESSk5Nl9+7dsnLlyvv7R6NC8eU1CN/nq+uvfv36Mnz4cNm2bZvExcXJuXPn5LPPPpMRI0ZIQECAiPjm12C/bTZERLZt2ybt27eXoKAgqVevnoSFhckHH3wg+fn5ytyWLVsqWatWre487u67774Ty7LkjTfekLCwMNufBQsWiIjITz/9ZOzf8tJLL0mjRo3k73//u7H3gGf54vq7fev29q3cX7p586ZtDio2X1x/xcXFMm3aNBk9erRtzxB8ky+uQfgPX11/GzZskIEDB8rMmTPlkUcekV69ekm7du1kyJAhIiK2p5T6Cr99GtX27dtl3LhxEhUVJbNmzZIGDRpI1apV5a233pKzZ8/e9/Vu/47czJkzZcCAAdo5LVq0KFPNpXn44YflypUrRt8DnuGr669u3boSGBgo2dnZyt/dzh588MEyvw/M8tX1l5CQIGfOnJENGzYo5xpcvXpVLly4IA0aNJAaNWqU+b1glq+uQfgHX15/derUkX379klmZqZcuHBBwsPDJTw8XCIiIiQsLExCQ0M98j7lyW+bjV27dknz5s1lz549th39tztQZxkZGUr27bffStOmTUXkv5u1RUQCAgIkMjLS8wWXwrIsuXDhgnTq1Knc3xv3z1fXX5UqVaRdu3baw5JSUlKkefPmPKXFB/jq+svMzJR///vf8tvf/lb5u4SEBElISJCkpCSJiooyVgM8w1fXIPyDP6y/Jk2aSJMmTUREJC8vT7788ksZNmxYuby3p/ntr1HdPgDP+sVjY1NSUu56QNTevXttv2937NgxSUlJkWeeeUZERBo0aCB9+vSRDRs2aH/qm5OTc8967uexe7prrV+/XnJycuTpp58u9fXwPl9ef9HR0XL8+HFbw3HmzBk5dOiQvPDCC6W+Ht7nq+vvxRdflKSkJOWPiMjAgQMlKSlJunfvfs9roGLw1TUI/+Bv62/OnDlSXFws06dPd+v13ubTdzY2b94sf/vb35Q8JiZGBg8eLHv27JGhQ4fKoEGD5Pz58/LOO+9I27ZtpbCwUHlNixYtpGfPnvL666/LrVu3ZOXKlVKvXj35/e9/f2fO2rVrpWfPntKuXTuZMGGCNG/eXC5duiRHjx6VrKwsOXXq1F1rPXbsmPTt21cWLFhQ6gah8PBwGTFihLRr106CgoLkyJEjkpiYKB07dpSJEye6/gGCUf66/iZNmiQbN26UQYMGycyZMyUgIEBWrFghDRs2lBkzZrj+AYJR/rj+2rRpI23atNH+XbNmzbijUcH44xoUEcnPz5fVq1eLiMjnn38uIiJr1qyR0NBQCQ0NlSlTprjy4YFh/rr+Fi9eLGlpadK9e3epVq2a7N27Vw4ePChvvvmm7+5lK/8HYJXd7cee3e3PDz/8YJWUlFiLFi2ywsPDrcDAQKtTp07WgQMHrLFjx1rh4eF3rnX7sWfLli2z4uLirIcfftgKDAy0nnjiCevUqVPKe589e9YaM2aM1ahRIysgIMBq3LixNXjwYGvXrl135pT1sXu/+93vrLZt21q1a9e2AgICrBYtWlizZ8+2CgoKyvJhg4f4+/qzLMv64YcfrOjoaCskJMSqVauWNXjwYCsjI8PdDxk8qDKsP2fCo28rFH9fg7dr0v35Ze3wDn9ffwcOHLC6detm1a5d26pRo4bVo0cPa+fOnWX5kHmdw7I4nhoAAACA5/ntng0AAAAA3kWzAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAETQbAAAAAIxw+VC/Xx73DtxWXk9OZv1Bpzyf3M0ahA6fA+FNrD94k6vrjzsbAAAAAIyg2QAAAABgBM0GAAAAACNoNgAAAAAYQbMBAAAAwAiaDQAAAABG0GwAAAAAMIJmAwAAAIARNBsAAAAAjKDZAAAAAGAEzQYAAAAAI2g2AAAAABhBswEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYEQ1bxcAoOw6d+6sZFOmTLGNx4wZo8xJSEhQstWrVyvZiRMnylAdAACorLizAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAEQ7LsiyXJjocpmvxuqpVqypZnTp13L6e8wbdGjVqKHNat26tZJMnT1ay5cuX28YjR45U5ty8eVPJFi9erGR/+tOf1GLd5OLyKbPKsP5c1bFjRyU7dOiQkoWEhLh1/fz8fCWrV6+eW9cyrbzWnwhr0Nv69etnG+/YsUOZ07t3byU7c+aMsZpE+Bzo6+bNm6dkuq+RVarYfzbbp08fZc4nn3zisbpcxfqDN7m6/rizAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAET5/gniTJk2UrHr16koWERGhZD179rSNQ0NDlTnDhg1zvzgXZGVlKVl8fLySDR061Da+evWqMufUqVNK5o0Na/Ccbt26Kdnu3buVTPcgA+eNW7o1U1RUpGS6zeA9evSwjXUniuuuBb1evXopme7jnpSUVB7l+ISuXbvaxsePH/dSJfBV48aNU7LZs2crWUlJSanXKs+HUwC+jjsbAAAAAIyg2QAAAABgBM0GAAAAACN8as+Gq4eZleUgPpN0vweqO1CosLBQyZwPsMrOzlbm/Pzzz0pm+kAruM/5kMfHH39cmbN9+3Yle+CBB9x6v4yMDCVbunSpkiUmJirZ559/bhvr1u1bb73lVl2Vke5AsJYtWypZZd2z4XyAmohIs2bNbOPw8HBlDgeP4V50ayYoKMgLlaAi6t69u5KNGjVKyXSHh/76178u9fozZ85Ush9//FHJnPcTi6jfC6SkpJT6fhUJdzYAAAAAGEGzAQAAAMAImg0AAAAARtBsAAAAADDCpzaIZ2ZmKtnly5eVzPQGcd3GnLy8PCXr27evbaw79Ozdd9/1WF3wLRs2bLCNR44cafT9dBvQa9WqpWS6gyCdNzS3b9/eY3VVRmPGjFGyo0ePeqGSikn3EIQJEybYxrqHJ6SnpxurCb4nMjLSNp46dapLr9Oto8GDB9vGly5dcr8wVAgjRoywjVetWqXMqV+/vpLpHkRx+PBhJQsLC7ONly1b5lJduus7X+vFF1906VoVBXc2AAAAABhBswEAAADACJoNAAAAAEbQbAAAAAAwwqc2iF+5ckXJZs2apWTOG7lERL766isli4+PL/U9T548qWT9+/dXsmvXrimZ84mSMTExpb4f/FPnzp2VbNCgQbaxq6cf6zZwv//++0q2fPly21h3Uqnu/wvdSfRPPvmkbcxJzWWjOyEb/7Np06ZS52RkZJRDJfAVulOXt2zZYhu7+vAY3Ube77//3r3CUO6qVVO/te3SpYuSbdy40TauUaOGMufTTz9Vsr/85S9KduTIESULDAy0jXfu3KnMeeqpp5RMJzU11aV5FRVf8QAAAAAYQbMBAAAAwAiaDQAAAABG0GwAAAAAMMKnNojr7N27V8kOHTqkZFevXlWyDh062Mbjx49X5jhvshXRbwbX+frrr23j1157zaXXwbd17NhRyT7++GMlCwkJsY0ty1LmfPjhh0qmO2m8d+/eSjZv3jzbWLfpNicnR8lOnTqlZCUlJbax8+Z2Ef0J5SdOnFCyykZ32nrDhg29UInvcGUjr+7/KVReY8eOVbIHH3yw1NfpTn5OSEjwREnwklGjRimZKw+d0H1OcT5lXESkoKDApTqcX+vqZvCsrCwl27Ztm0uvrai4swEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBE+v0Fcx9XNO/n5+aXOmTBhgpK99957Sua8gRaVQ6tWrZRMd6q9bsNrbm6ubZydna3M0W0KKywsVLIPPvjApcxTgoODlWzGjBlK9vLLLxurwVcMHDhQyXQfv8pKt1m+WbNmpb7u4sWLJsqBD6hfv76Svfrqq0rm/HU5Ly9PmfPmm296rC6UP91p3nPnzlUy3QNY1q1bZxs7P1RFxPXvJ3X++Mc/uvW6adOmKZnuYS6+hDsbAAAAAIyg2QAAAABgBM0GAAAAACP8cs+Gq2JjY23jzp07K3N0h6VFRkYq2cGDBz1WFyqmwMBAJdMd+qj7HX3doZJjxoyxjVNTU5U5vvS7/U2aNPF2CRVS69atXZrnfAhoZaH7f0i3j+Pbb7+1jXX/T8H/NG3aVMl2797t1rVWr16tZMnJyW5dC+Vv/vz5Sqbbn1FUVKRkH330kZLNnj3bNr5x44ZLdQQFBSmZ7sA+56+JDodDmaPbM7Rv3z6X6vAl3NkAAAAAYATNBgAAAAAjaDYAAAAAGEGzAQAAAMCISr1B/Nq1a7ax7gC/EydOKNnGjRuVTLfJzHnD79q1a5U5uoNmUDF16tRJyXSbwXWee+45Jfvkk0/KXBP8x/Hjx71dQpmEhIQo2dNPP20bjxo1Spmj21ip43x4l+6ANvgf5zUkItK+fXuXXvuPf/zDNl61apVHakL5CA0NtY0nTZqkzNF9D6XbDB4VFeVWDS1atFCyHTt2KJnuAUPOdu3apWRLly51qy5fw50NAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACMqNQbxJ2dPXtWycaNG6dkW7ZsUbLRo0eXmtWsWVOZk5CQoGTZ2dn3KhNesmLFCiXTnQiq2/jt65vBq1Sx/1yipKTES5X4r7p163rsWh06dFAy3VqNjIy0jR966CFlTvXq1ZXs5ZdfVjLnNSKinsibkpKizLl165aSVaumfmn68ssvlQz+RbeJd/HixS699siRI0o2duxY2zg/P9+tuuAdzp976tev79Lrpk2bpmQNGjRQsldeecU2fvbZZ5U5jz32mJLVqlVLyXQb1Z2z7du3K3OcH1Tkr7izAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAEWwQL0VSUpKSZWRkKJlu83C/fv1s40WLFilzwsPDlWzhwoVKdvHixXvWCc8bPHiwbdyxY0dljm5T2P79+02V5DXOG8J1/+6TJ0+WUzW+xXmTtIj+4/fOO+8o2dy5c916T90Jy7oN4sXFxbbx9evXlTmnT59Wss2bNytZamqqkjk/GOHSpUvKnKysLCULDg5WsvT0dCWDb2vatKltvHv3brevde7cOSXTrTf4jqKiIts4JydHmRMWFqZk58+fVzLd51xX/Pjjj0pWUFCgZA888ICS5ebm2sbvv/++WzX4A+5sAAAAADCCZgMAAACAETQbAAAAAIyg2QAAAABgBBvE3ZCWlqZkw4cPV7IhQ4bYxrqTxydOnKhkLVu2VLL+/fvfT4nwAOdNqrqTlH/66Scle++994zV5GmBgYFKFhsbW+rrDh06pGRz5szxREl+Z9KkSUr2/fffK1lERITH3jMzM1PJ9u7dq2TffPONbfzPf/7TYzXovPbaa0qm2+Cp2+wL/zN79mzb2PlBFPfD1ZPG4Tvy8vJsY90J8wcOHFCyunXrKtnZs2eVbN++fbbx1q1blTlXrlxRssTERCXTbRDXzausuLMBAAAAwAiaDQAAAABG0GwAAAAAMII9Gx7i/LuFIiLvvvuubbxp0yZlTrVq6n+CXr16KVmfPn1s48OHD99XfTDj1q1bSpadne2FSkqn258xb948JZs1a5aSOR+8FhcXp8wpLCwsQ3WVy5IlS7xdglc4H3R6N2U53A0Vk+5Q1Keeesqtazn/rr2IyJkzZ9y6FnxHSkqKkun2fHmS7vux3r17K5luvxF7z/6HOxsAAAAAjKDZAAAAAGAEzQYAAAAAI2g2AAAAABjBBnE3tG/fXsmio6OVrGvXrraxbjO4zunTp5Xs008/dbE6lKf9+/d7u4S7ct6Qqdv4PWLECCXTbb4cNmyYx+oCSpOUlOTtEuBhBw8eVLJf/epXpb5Od9DkuHHjPFESUCrnw31F9JvBLctSMg71+x/ubAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYAQbxH+hdevWSjZlyhQle/7555WsUaNGbr3nf/7zHyXTnUCt25AEsxwOxz3HIiJRUVFKFhMTY6qku5o+fbqSvfHGG7ZxnTp1lDk7duxQsjFjxniuMAAQkXr16imZK1/X1q1bp2SFhYUeqQkozUcffeTtEvwCdzYAAAAAGEGzAQAAAMAImg0AAAAARtBsAAAAADCi0mwQ123gHjlypG2s2wzetGlTj9WQmpqqZAsXLlSyinwqdWXifCKo7oRQ3bqKj49Xss2bNyvZ5cuXbeMePXooc0aPHq1kHTp0ULKHHnpIyTIzM21j3UY33eZLoDzpHrzQqlUrJdOdJI2KacuWLUpWpYp7P9v84osvyloO4LYBAwZ4uwS/wJ0NAAAAAEbQbAAAAAAwgmYDAAAAgBE+v2ejYcOGSta2bVslW7NmjZK1adPGY3WkpKQo2bJly2zjffv2KXM4rM+3Va1aVckmTZqkZMOGDVOygoIC27hly5Zu16H7vebk5GTbeP78+W5fHzBFtxfK3d/vR/nr2LGjkkVGRiqZ7mtdUVGRbbx27VplzqVLl9wvDiij5s2be7sEv8BndAAAAABG0GwAAAAAMIJmAwAAAIARNBsAAAAAjKjQG8Tr1q1rG2/YsEGZo9uc5skNPbqNt3FxcUqmOzDtxo0bHqsD5e/o0aO28fHjx5U5Xbt2delausP/dA83cOZ88J+ISGJiopLFxMS4VAfgC37zm98o2datW8u/EJQqNDRUyXSf73QuXrxoG8+cOdMTJQEe89lnnymZ7gEWPOzn3rizAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAEV7ZIN69e3clmzVrlpJ169bNNm7cuLFH67h+/bptHB8fr8xZtGiRkl27ds2jdaBiysrKso2ff/55Zc7EiROVbN68eW6936pVq5Rs/fr1Svbdd9+5dX2gInI4HN4uAQC00tLSlCwjI0PJdA8meuSRR2zjnJwczxXmY7izAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAEV7ZID506FCXMlecPn1ayQ4cOKBkxcXFSuZ8EnheXp5bNaByyM7OVrLY2FiXMgAiH374oZK98MILXqgEnpKenq5kX3zxhZL17NmzPMoBjNM9OGjTpk1KtnDhQtt46tSpyhzd97D+iDsbAAAAAIyg2QAAAABgBM0GAAAAACNoNgAAAAAY4bAsy3JpIqe8QsPF5VNmrD/olNf6E2ENQo/PgfAm1l/5CwkJUbKdO3cqWWRkpG28Z88eZc4rr7yiZNeuXStDdeXL1fXHnQ0AAAAARtBsAAAAADCCZgMAAACAEezZQJnw+6LwJvZswNv4HAhvYv1VDLp9HM6H+r3++uvKnPbt2yuZLx30x54NAAAAAF5FswEAAADACJoNAAAAAEbQbAAAAAAwgg3iKBM2p8Gb2CAOb+NzILyJ9QdvYoM4AAAAAK+i2QAAAABgBM0GAAAAACNoNgAAAAAY4fIGcQAAAAC4H9zZAAAAAGAEzQYAAAAAI2g2AAAAABhBswEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYMT/Af6T9PifD5VrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Exploratory data analysis\n",
        "plt.figure(figsize=(10, 2))\n",
        "for i in range(5):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(X_train[i], cmap='gray')\n",
        "    plt.title(f\"Label: {Y_train[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "unique, counts = np.unique(Y_train, return_counts=True)\n",
        "plt.bar(unique, counts, color='skyblue')\n",
        "plt.xlabel('Digit')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Digits')\n",
        "plt.xticks(range(10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uMJTjyk3BXD"
      },
      "source": [
        "## Data Split: (5 Points)\n",
        "\n",
        "To evaluate the performance of our MLP model effectively, we divide the dataset into three distinct sets: training, validation, and test sets. The training set is used to train the model, where the model learns to make predictions by adjusting its parameters. The validation set is used to fine-tune the hyperparameters and make informed decisions about the model architecture without overfitting to the training data. Finally, the test set is utilized to assess the model's performance on unseen data, providing an unbiased evaluation of its ability to generalize.\n",
        "\n",
        "In the next cell, you should take the initial training set `(X_train, Y_train)` and further split it into validation `(X_val, Y_val)` and revised training sets `(X_train, Y_train)` using a 1:5 ratio. This process ensures that we have a separate dataset to validate the model's performance during the training phase, helping us to avoid overfitting and ensuring that our model generalizes well to new, unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DW3Zwaks3BXD"
      },
      "outputs": [],
      "source": [
        "indices = np.arange(len(X_train))\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "X_train_shuffled, Y_train_shuffled = X_train[indices], Y_train[indices]\n",
        "\n",
        "X_train = X_train_shuffled[len(X_train) // 6:]\n",
        "Y_train = Y_train_shuffled[len(X_train) // 6:]\n",
        "\n",
        "X_val = X_train_shuffled[:len(X_train) // 6]\n",
        "Y_val = Y_train_shuffled[:len(X_train) // 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2giYKhcl3BXE"
      },
      "source": [
        "## Preparation of the Data:\n",
        "\n",
        "Data preparation is a critical step in the machine learning pipeline. This process includes cleaning the data, handling missing values, normalizing or standardizing the data, and reshaping it into a format suitable for the neural network. Proper data preparation ensures that the model learns effectively and achieves high performance.\n",
        "\n",
        "In the next cell, we will focus on preparing the MNIST dataset for our MLP model. Given that the MNIST images are grayscale with pixel values ranging from 0 to 255, we will start by normalizing these values to a range of 0 to 1. This normalization helps in speeding up the convergence of the neural network by providing a consistent scale of input values. Additionally, we will reshape the images from their original 2D shape (28x28 pixels) into a flat 1D vector of 784 elements. This step is necessary because our MLP model expects input vectors of a fixed size.\n",
        "\n",
        "**There is no need to change the following cell.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MFkB2eGM3BXE"
      },
      "outputs": [],
      "source": [
        "# Normalize the images\n",
        "X_train = X_train / 255\n",
        "X_val = X_val / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# Flatten the images\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_val = X_val.reshape(X_val.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEPEqbiW3BXE"
      },
      "source": [
        "## Preparation of Model Parameters:\n",
        "\n",
        "Initializing the parameters of an MLP, including weights and biases, is an important step that can significantly affect the model's learning process. Proper initialization can help in preventing issues related to vanishing or exploding gradients, thereby ensuring a stable and efficient training process.\n",
        "\n",
        "In the next cell, we define the architecture of our neural network and initialize its weights and biases. Our network consists of an input layer, three hidden layers, and an output layer, structured as follows:\n",
        "\n",
        "- `n_inp`: Represents the number of neurons in the input layer, determined by the number of features in `X_train`.\n",
        "- `n_hid1`, `n_hid2`, `n_hid3`: Denote the number of neurons in the first, second, and third hidden layers, respectively, set to 256, 128, and 64 for a progressively narrowing architecture.\n",
        "- `n_out`: The number of neurons in the output layer, set to 10, corresponding to the number of possible classifications (for example, digits 0-9 in the MNIST dataset).\n",
        "\n",
        "For each layer, we initialize the weights and biases as follows:\n",
        "- `W1`, `W2`, `W3`, `W4`: Weight matrices for the first, second, third, and output layers, initialized with small random values drawn from a normal distribution, scaled by 0.01 to keep the values small.\n",
        "- `B1`, `B2`, `B3`, `B4`: Bias vectors for each layer, also initialized with small random values, similarly scaled.\n",
        "\n",
        "A random seed (`manual_seed(42)`) is set for reproducibility, ensuring that the random numbers generated are the same each time the code is run. This aids in debugging and comparing model performances across different runs.\n",
        "\n",
        "**There is no need to change the following cell.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0QfHDJJd3BXE"
      },
      "outputs": [],
      "source": [
        "# Defining the Sizes of the Layers\n",
        "n_inp = X_train.shape[1]\n",
        "n_hid1 = 256\n",
        "n_hid2 = 128\n",
        "n_hid3 = 64\n",
        "n_out = 10\n",
        "\n",
        "# Seed for Reproducibility\n",
        "g = torch.Generator().manual_seed(42)\n",
        "\n",
        "# Weights and Biases of the First Layer:\n",
        "W1 = torch.randn(n_inp, n_hid1, generator=g, requires_grad=True) * 0.01\n",
        "B1 = torch.randn(1, n_hid1, generator=g, requires_grad=True) * 0.01\n",
        "\n",
        "# Weights and Biases of the Second Layer:\n",
        "W2 = torch.randn(n_hid1, n_hid2, generator=g, requires_grad=True) * 0.01\n",
        "B2 = torch.randn(1, n_hid2, generator=g, requires_grad=True) * 0.01\n",
        "\n",
        "# Weights and Biases of the Third Layer:\n",
        "W3 = torch.randn(n_hid2, n_hid3, generator=g, requires_grad=True) * 0.01\n",
        "B3 = torch.randn(1, n_hid3, generator=g, requires_grad=True) * 0.01\n",
        "\n",
        "# Weights and Biases of the Fourth Layer:\n",
        "W4 = torch.randn(n_hid3, n_out, generator=g, requires_grad=True) * 0.01\n",
        "B4 = torch.randn(1, n_out, generator=g, requires_grad=True) * 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-2fCxby3BXE"
      },
      "source": [
        "## Forward Pass:\n",
        "\n",
        "The forward pass refers to the computation process where the input data is passed through the network layer by layer to produce the output. At each layer, the input is transformed using a linear transformation (weighted sum followed by the addition of a bias term) and then typically passed through a non-linear activation function. The output of the final layer is considered the prediction of the network. This step is crucial for both training and evaluation of the model.\n",
        "\n",
        "In the next cell, we will illustrate the forward pass using our MLP model designed for the MNIST dataset. We will walk through the process step by step, starting with the input layer, moving through the hidden layers, and finally to the output layer. For each layer, we will apply the linear transformation, followed by an activation function for hidden layers. The final layer will use the softmax function to produce a probability distribution over the 10 possible digit classes.\n",
        "\n",
        "\n",
        "Here's a step-by-step breakdown:\n",
        "\n",
        "- `batch_size = 32`: This specifies the number of training examples to process simultaneously, which affects the efficiency of training and the generalization capability of the model.\n",
        "\n",
        "**Layer 1:**\n",
        "- `PreActivation1`: Result of the linear transformation applied to the input, computed as `sample_inp @ W1 + B1`, resulting in dimensions `[batch_size, n_hid1]`.\n",
        "- `Hidden1`: Output after applying the sigmoid activation function to `PreActivation1`, maintaining dimensions `[batch_size, n_hid1]`.\n",
        "\n",
        "**Layer 2:**\n",
        "- `PreActivation2`: Linear transformation for the second layer, using `Hidden1` as input, resulting in dimensions `[batch_size, n_hid2]`.\n",
        "- `Hidden2`: Output after applying the ReLU activation function to `PreActivation2`, with dimensions `[batch_size, n_hid2]`.\n",
        "\n",
        "**Layer 3:**\n",
        "- `PreActivation3`: Linear transformation for the third layer, with dimensions `[batch_size, n_hid3]`.\n",
        "- `Hidden3`: Output after applying the tanh activation function to `PreActivation3`, maintaining dimensions `[batch_size, n_hid3]`.\n",
        "\n",
        "**Layer 4 (Output Layer):**\n",
        "- `Logits`: Result of the final linear transformation applied to `Hidden3`, with dimensions `[batch_size, n_out]`, where `n_out` is the number of output classes.\n",
        "- `Probs`: Probabilities for each class, obtained by applying the softmax function to `Logits`, with dimensions `[batch_size, n_out]`. The softmax ensures these probabilities sum to 1 for each sample.\n",
        "\n",
        "**Loss Computation:**\n",
        "- `LogProbs`: Logarithm of `Probs`, necessary for computing cross-entropy loss, maintaining dimensions `[batch_size, n_out]`.\n",
        "- `Loss`: The cross-entropy loss for the batch, calculated by taking the negative log likelihood of the correct class probabilities (indexed by `sample_out`) and averaging, resulting in a single scalar value. This metric quantifies the model's prediction accuracy, guiding parameter adjustments during training.\n",
        "\n",
        "**There is no need to change the following cell.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-CkldNbC3BXF"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "sample_inp = torch.Tensor(X_train[:batch_size])\n",
        "sample_out = torch.Tensor(Y_train[:batch_size]).int()\n",
        "\n",
        "# Forward pass\n",
        "\n",
        "# Layer 1\n",
        "# Applying Linear transformation\n",
        "PreActivation1 = sample_inp @ W1 + B1\n",
        "# Applying Sigmoid activation function\n",
        "Hidden1 = torch.sigmoid(PreActivation1)\n",
        "\n",
        "# Layer 2\n",
        "# Applying Linear transformation\n",
        "PreActivation2 = Hidden1 @ W2 + B2\n",
        "# Applying ReLU activation function\n",
        "Hidden2 = torch.relu(PreActivation2)\n",
        "\n",
        "# Layer 3\n",
        "# Applying Linear transformation\n",
        "PreActivation3 = Hidden2 @ W3 + B3\n",
        "# Applying Tanh activation function\n",
        "Hidden3 = torch.tanh(PreActivation3)\n",
        "\n",
        "\n",
        "# Layer 4\n",
        "# Applying Linear transformation\n",
        "Logits = Hidden3 @ W4 + B4\n",
        "# Applying Softmax activation function\n",
        "Exp4 = torch.exp(Logits)\n",
        "Sum4 = torch.sum(Exp4, dim=1).view(-1, 1)\n",
        "SumInverse4 = 1 / Sum4\n",
        "Probs = Exp4 * SumInverse4\n",
        "\n",
        "# Loss computation (Cross-entropy loss)\n",
        "LogProbs = Probs.log()\n",
        "Loss = -LogProbs[range(batch_size), sample_out].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOqx2YYk3BXF"
      },
      "source": [
        "## Backward Pass and Gradient Computation:\n",
        "\n",
        "Following the forward pass, the next critical phase in training a neural network is the backward pass, which involves computing the gradients of the loss function with respect to each parameter in the model. This process, often facilitated by automatic differentiation libraries like PyTorch's `autograd`, allows us to understand how each parameter needs to be adjusted to minimize the loss.\n",
        "\n",
        "In the next cell, we will compute the gradients of our model using `autograd` and save all the values as numpy arrays.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Here's a breakdown of the variables in the code:\n",
        "\n",
        "- **`parameters`:** A list containing all the trainable parameters of the network, including the weights (`W1`, `W2`, `W3`, `W4`) and biases (`B1`, `B2`, `B3`, `B4`) of each layer.\n",
        "\n",
        "- **`middle_values`:** A list of intermediate values computed during the forward pass. These include pre-activation values (`PreActivation1`, `PreActivation2`, `PreActivation3`), hidden layer outputs (`Hidden1`, `Hidden2`, `Hidden3`), and other intermediate computations (`Logits`, `Exp4`, `Sum4`, `SumInverse4`, `Probs`, `LogProbs`) crucial for the loss calculation.\n",
        "\n",
        "- **Weights (`W1`, `W2`, `W3`, `W4`):** These matrices represent the wights of different layers of our model saved as NumPy arrays.\n",
        "\n",
        "- **Biases (`B1`, `B2`, `B3`, `B4`):** These matrices represent the biases of different layers of our model saved as NumPy arrays.\n",
        "\n",
        "- **Weight Gradients (`W1_grad`, `W2_grad`, `W3_grad`, `W4_grad`):** After the backward pass, `W1_grad` holds the gradients of the loss function with respect to `W1`, and similarly for the other weight matrices. All these matrices are saved as NumPy arrays.\n",
        "\n",
        "- **Bias Gradients (`B1_grad`, `B2_grad`, `B3_grad`, `B4_grad`):** Similar to weight gradients, these vectors contain the gradients of the loss with respect to the biases. `B1_grad` is associated with the biases of the first hidden layer, `B2_grad` with the second, and so on. All these vectors are saved as NumPy arrays.\n",
        "\n",
        "**There is no need to change the following cell.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vdIHjv7L3BXF"
      },
      "outputs": [],
      "source": [
        "parameters = [W1, B1, W2, B2, W3, B3, W4, B4]\n",
        "\n",
        "middle_values = [PreActivation1, Hidden1,\n",
        "                 PreActivation2, Hidden2,\n",
        "                 PreActivation3, Hidden3,\n",
        "                 Logits, Exp4, Sum4, SumInverse4, Probs,\n",
        "                 LogProbs]\n",
        "\n",
        "# Backward pass (autograd)\n",
        "for p in parameters:\n",
        "    p.grad = None\n",
        "    p.retain_grad()\n",
        "for m in middle_values:\n",
        "    m.retain_grad()\n",
        "Loss.backward()\n",
        "\n",
        "Loss = np.array(Loss.detach())\n",
        "\n",
        "LogProbs_grad = np.array(LogProbs.grad)\n",
        "LogProbs = np.array(LogProbs.detach())\n",
        "\n",
        "Probs_grad = np.array(Probs.grad)\n",
        "Probs = np.array(Probs.detach())\n",
        "\n",
        "SumInverse4_grad = np.array(SumInverse4.grad)\n",
        "SumInverse4 = np.array(SumInverse4.detach())\n",
        "\n",
        "Sum4_grad = np.array(Sum4.grad)\n",
        "Sum4 = np.array(Sum4.detach())\n",
        "\n",
        "Exp4_grad = np.array(Exp4.grad)\n",
        "Exp4 = np.array(Exp4.detach())\n",
        "\n",
        "Logits_grad = np.array(Logits.grad)\n",
        "Logits = np.array(Logits.detach())\n",
        "\n",
        "B4_grad = np.array(B4.grad)\n",
        "B4 = np.array(B4.detach())\n",
        "\n",
        "W4_grad = np.array(W4.grad)\n",
        "W4 = np.array(W4.detach())\n",
        "\n",
        "Hidden3_grad = np.array(Hidden3.grad)\n",
        "Hidden3 = np.array(Hidden3.detach())\n",
        "\n",
        "PreActivation3_grad = np.array(PreActivation3.grad)\n",
        "PreActivation3 = np.array(PreActivation3.detach())\n",
        "\n",
        "B3_grad = np.array(B3.grad)\n",
        "B3 = np.array(B3.detach())\n",
        "\n",
        "W3_grad = np.array(W3.grad)\n",
        "W3 = np.array(W3.detach())\n",
        "\n",
        "Hidden2_grad = np.array(Hidden2.grad)\n",
        "Hidden2 = np.array(Hidden2.detach())\n",
        "\n",
        "PreActivation2_grad = np.array(PreActivation2.grad)\n",
        "PreActivation2 = np.array(PreActivation2.detach())\n",
        "\n",
        "B2_grad = np.array(B2.grad)\n",
        "B2 = np.array(B2.detach())\n",
        "\n",
        "W2_grad = np.array(W2.grad)\n",
        "W2 = np.array(W2.detach())\n",
        "\n",
        "Hidden1_grad = np.array(Hidden1.grad)\n",
        "Hidden1 = np.array(Hidden1.detach())\n",
        "\n",
        "PreActivation1_grad = np.array(PreActivation1.grad)\n",
        "PreActivation1 = np.array(PreActivation1.detach())\n",
        "\n",
        "B1_grad = np.array(B1.grad)\n",
        "B1 = np.array(B1.detach())\n",
        "\n",
        "W1_grad = np.array(W1.grad)\n",
        "W1 = np.array(W1.detach())\n",
        "\n",
        "sample_inp = np.array(sample_inp)\n",
        "sample_out = np.array(sample_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlpMkk3Z3BXF"
      },
      "source": [
        "## Manual Gradient Calculation by Chain Rule: (50 Points)\n",
        "\n",
        "The chain rule is a fundamental principle in calculus that facilitates the computation of complex derivatives by breaking them down into simpler parts. In the context of neural networks, the chain rule enables the efficient calculation of gradients of the loss function with respect to any parameter in the network, regardless of its position. This process is crucial for understanding how changes in parameters affect the overall loss, guiding the optimization process towards lower loss values.\n",
        "\n",
        "In the next cells, we will start a hands-on exploration of this concept by manually calculating the gradients for selected parts of our model. This exercise will not only deepen your understanding of backpropagation  but also reinforce the mathematical intuition behind the learning process.\n",
        "\n",
        "We will proceed as follows:\n",
        "\n",
        "1. **Identify Key Operations:** We will start by pinpointing the key operations in our model that contribute to the final prediction. This includes linear transformations, activation functions, and the final loss computation.\n",
        "   \n",
        "2. **Compute Local Gradients:** For each operation, we will compute the local gradient, which represents the rate of change of the operation's output with respect to its input.\n",
        "\n",
        "3. **Apply the Chain Rule:** Using the chain rule, we will then piece together these local gradients to compute the gradient of the loss function with respect to each parameter in our model. This involves multiplying the local gradients along the paths from the output back to each parameter.\n",
        "\n",
        "4. **Verify Your Calculations:** Finally, we will compare our manually computed gradients with those obtained using automatic differentiation (e.g., PyTorch's `autograd`), ensuring the correctness of our computations. For example `dLogProbs` that you manualy calculate should be equal to the `LogProbs_grad` that we calculated before using PyTorch's autograd and we will verify that by `check()` function.\n",
        "\n",
        "\n",
        "**HINT:** As you start doing manual gradient calculation, keep in mind that the gradient of any parameter or intermediate variable should retain the same dimensions as the original array.\n",
        "\n",
        "Additionally, all calculations in this exercise **should be performed using NumPy** and its functions.\n",
        "\n",
        "**Please complete the #TODO parts of the following cells**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0MtrH3ux3BXF"
      },
      "outputs": [],
      "source": [
        "# Function to compare manually computed gradients with PyTorch (autograd) gradients\n",
        "def check(s, manual_grad, autograd_grad):\n",
        "  corrcetness = np.allclose(manual_grad, autograd_grad)\n",
        "  print(f'{s:15s} | Gradient Correctness: {str(corrcetness):5s}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8iTcljj3BXG",
        "outputId": "d419f433-7cfa-40f3-a60f-3d98ff36bce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogProbs        | Gradient Correctness: True \n"
          ]
        }
      ],
      "source": [
        "# Backward pass (manual)\n",
        "# Loss\n",
        "# TODO\n",
        "dLogProbs = np.zeros_like(LogProbs)\n",
        "for i in range(batch_size):\n",
        "    dLogProbs[i, sample_out[i]] = -1 / batch_size\n",
        "\n",
        "\n",
        "# Verify the correctness of the gradients that you calculated\n",
        "check('LogProbs', dLogProbs, LogProbs_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E62O5TGX3BXG",
        "outputId": "ea6d3c9a-54c1-4dcf-edf2-f77ee43a4f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probs           | Gradient Correctness: True \n",
            "SumInverse4     | Gradient Correctness: True \n",
            "Sum4            | Gradient Correctness: True \n",
            "Exp4            | Gradient Correctness: True \n",
            "Logits          | Gradient Correctness: True \n",
            "B4              | Gradient Correctness: True \n",
            "W4              | Gradient Correctness: True \n",
            "Hidden3         | Gradient Correctness: True \n"
          ]
        }
      ],
      "source": [
        "# Backward pass (manual)\n",
        "# Layer 4\n",
        "\n",
        "# TODO\n",
        "dProbs = dLogProbs / Probs\n",
        "\n",
        "# TODO\n",
        "dSumInverse4 = np.sum(dProbs * Exp4, axis=1, keepdims=True)\n",
        "\n",
        "# TODO\n",
        "dSum4 = dSumInverse4 * -1 * SumInverse4 ** 2\n",
        "# TODO\n",
        "dExp4 = dProbs * SumInverse4 + dSum4\n",
        "\n",
        "# TODO\n",
        "dLogits = dExp4 * Exp4\n",
        "\n",
        "# TODO\n",
        "dB4 = dLogits.sum(axis=0)\n",
        "\n",
        "# TODO\n",
        "dW4 = (Hidden3.T @ dLogits)\n",
        "\n",
        "# TODO\n",
        "dHidden3 =  dLogits @ W4.T\n",
        "\n",
        "\n",
        "# Verify the correctness of the gradients that you calculated\n",
        "check('Probs', dProbs, Probs_grad)\n",
        "check('SumInverse4', dSumInverse4, SumInverse4_grad)\n",
        "check('Sum4', dSum4, Sum4_grad)\n",
        "check('Exp4', dExp4, Exp4_grad)\n",
        "check('Logits', dLogits, Logits_grad)\n",
        "check('B4', dB4, B4_grad)\n",
        "check('W4', dW4, W4_grad)\n",
        "check('Hidden3', dHidden3, Hidden3_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kupvJRTN3BXG",
        "outputId": "9488dcf7-c885-4bb9-d0b0-53ed0f5188c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreActivation3  | Gradient Correctness: True \n",
            "B3              | Gradient Correctness: True \n",
            "W3              | Gradient Correctness: True \n",
            "Hidden2         | Gradient Correctness: True \n"
          ]
        }
      ],
      "source": [
        "# Backward pass (manual)\n",
        "# Layer 3\n",
        "\n",
        "# TODO\n",
        "dPreActivation3 = dHidden3 * (1 - Hidden3 ** 2)\n",
        "\n",
        "# TODO\n",
        "dB3 = dPreActivation3.sum(axis=0)\n",
        "\n",
        "# TODO\n",
        "dW3 = Hidden2.T @ dPreActivation3\n",
        "\n",
        "# TODO\n",
        "dHidden2 = dPreActivation3 @ W3.T\n",
        "\n",
        "\n",
        "# Verify the correctness of the gradients that you calculated\n",
        "check('PreActivation3', dPreActivation3, PreActivation3_grad)\n",
        "check('B3', dB3, B3_grad)\n",
        "check('W3', dW3, W3_grad)\n",
        "check('Hidden2', dHidden2, Hidden2_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtUoqWVS3BXG",
        "outputId": "ec98376b-fdc4-4b27-9785-a6bebcd376f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreActivation2  | Gradient Correctness: True \n",
            "B2              | Gradient Correctness: True \n",
            "W2              | Gradient Correctness: True \n",
            "Hidden1         | Gradient Correctness: True \n"
          ]
        }
      ],
      "source": [
        "# Backward pass (manual)\n",
        "# Layer 2\n",
        "\n",
        "# TODO\n",
        "dPreActivation2 = dHidden2 * (Hidden2 > 0)\n",
        "\n",
        "# TODO\n",
        "dB2 = dPreActivation2.sum(axis=0)\n",
        "\n",
        "# TODO\n",
        "dW2 = Hidden1.T @ dPreActivation2\n",
        "\n",
        "# TODO\n",
        "dHidden1 = dPreActivation2 @ W2.T\n",
        "\n",
        "\n",
        "# Verify the correctness of the gradients that you calculated\n",
        "check('PreActivation2', dPreActivation2, PreActivation2_grad)\n",
        "check('B2', dB2, B2_grad)\n",
        "check('W2', dW2, W2_grad)\n",
        "check('Hidden1', dHidden1, Hidden1_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TkskFqS3BXG",
        "outputId": "1069cf3a-4558-4db7-e4b4-947a43a8a370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreActivation1  | Gradient Correctness: True \n",
            "B1              | Gradient Correctness: True \n",
            "W1              | Gradient Correctness: True \n"
          ]
        }
      ],
      "source": [
        "# Backward pass (manual)\n",
        "# Layer 1\n",
        "\n",
        "# TODO\n",
        "dPreActivation1 = dHidden1 * (np.array(torch.sigmoid(torch.from_numpy(PreActivation1))) - np.array(torch.sigmoid(torch.from_numpy(PreActivation1))) ** 2)\n",
        "\n",
        "# TODO\n",
        "dB1 = dPreActivation1.sum(axis=0)\n",
        "\n",
        "# TODO\n",
        "dW1 = sample_inp.T @ dPreActivation1\n",
        "\n",
        "\n",
        "# Verify the correctness of the gradients that you calculated\n",
        "check('PreActivation1', dPreActivation1, PreActivation1_grad)\n",
        "check('B1', dB1, B1_grad)\n",
        "check('W1', dW1, W1_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW1mIK7j3BXH"
      },
      "source": [
        "## Updating Model Parameters: (5 Points)\n",
        "\n",
        "With the gradients computed during the backpropagation step, the next critical phase in training our neural network involves updating the model's parameters. This step is essential for minimizing the loss function and improving the model's predictions over time.\n",
        "\n",
        "For our model, this translates to the following updates:\n",
        "- For the first layer's weights (`W1`) and biases (`B1`), we subtract the product of the learning rate and their respective gradients (`dW1`, `dB1`).\n",
        "- Similarly, this pattern is repeated for the subsequent layers' weights (`W2`, `W3`, `W4`) and biases (`B2`, `B3`, `B4`), ensuring that each parameter is nudged in the direction that minimizes the loss.\n",
        "\n",
        "**Please complete the #TODO parts of the following cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "62ECgQaW3BXH"
      },
      "outputs": [],
      "source": [
        "# Update parameters\n",
        "# TODO\n",
        "learning_rate = 0.01\n",
        "\n",
        "# TODO\n",
        "W1 -= learning_rate * dW1\n",
        "B1 -= learning_rate * dB1\n",
        "\n",
        "# TODO\n",
        "W2 -= learning_rate * dW2\n",
        "B2 -= learning_rate * dB2\n",
        "\n",
        "# TODO\n",
        "W3 -= learning_rate * dW3\n",
        "B3 -= learning_rate * dB3\n",
        "\n",
        "# TODO\n",
        "W4 -= learning_rate * dW4\n",
        "B4 -= learning_rate * dB4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhsKdksv3BXH"
      },
      "source": [
        "## Activation Functions Using Numpy Instead of PyTorch: (10 Points)\n",
        "\n",
        "In the next phase of this notebook, we will try to implement a complete multilayer perceptron (MLP) model using only the NumPy library. For this purpose, you should implement the following activation functions using only NumPy functions:\n",
        "\n",
        "- **Sigmoid:**\n",
        "  The Sigmoid function is defined as \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). It maps any input value to a value between 0 and 1.\n",
        "\n",
        "- **ReLU:**\n",
        "  The Rectified Linear Unit (ReLU) function is defined as \\( \\text{ReLU}(x) = \\max(0, x) \\). It introduces non-linearity with less computational complexity and has become one of the most popular activation functions in deep learning.\n",
        "\n",
        "- **Tanh:**\n",
        "  The Hyperbolic Tangent (Tanh) function is defined as \\( \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \\). It maps any input value to a range between -1 and 1.\n",
        "\n",
        "**Please complete the #TODO parts of the following cells**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "otiaenTe3BXH"
      },
      "outputs": [],
      "source": [
        "# Activation functions\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function for the input array.\n",
        "\n",
        "    Parameters:\n",
        "    - x (np.array): The input array for which to compute the sigmoid function.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: The output array where the sigmoid function has been applied element-wise.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Compute the Rectified Linear Unit (ReLU) for the input array.\n",
        "\n",
        "    Parameters:\n",
        "    - x (np.array): The input array for which to compute the ReLU function.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: The output array where the ReLU function has been applied element-wise, keeping only positive values and setting negative values to zero.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"\n",
        "    Compute the hyperbolic tangent (tanh) for the input array.\n",
        "\n",
        "    Parameters:\n",
        "    - x (np.array): The input array for which to compute the tanh function.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: The output array where the tanh function has been applied element-wise.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    return np.tanh(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpCplOkG3BXH"
      },
      "source": [
        "## Writing the MLP Class: (10 Points)\n",
        "\n",
        "In the next cell, you are tasked with completing the MLP class, which will serve as the foundation for training an MLP model on the MNIST dataset. The class should have the essential components of an MLP, including:\n",
        "\n",
        "- **Initialization of Parameters:** Define the architecture (number of layers, size of each layer) and initialize the weights and biases accordingly. This sets up the structure of your neural network and prepares it for the forward and backward passes. You can use NumPy random functions for this purpose.\n",
        "\n",
        "- **Forward Pass Method:** Implement the forward propagation algorithm to compute the output of the network for a given input. This involves applying a series of linear transformations and non-linear activations to the input data to obtain the final output. You are free to choose the size of the layers and also the activation functions\n",
        "\n",
        "- **Loss Computation:** Calculate the loss to evaluate how well the model is performing. The loss function measures the discrepancy between the predicted outputs and the actual targets, guiding the training process. Use Cross-entropy loss for this example.\n",
        "\n",
        "- **Backward Pass Method:** Compute the gradients of the loss with respect to the parameters **manually**.\n",
        "\n",
        "- **Step Method:**\n",
        "  The step method is responsible for updating the model's parameters (weights and biases) using the gradients computed after the backward pass. Typically, the step method will iterate over all parameters and apply the update:\n",
        "  \\[ \\text{parameter} = \\text{parameter} - (\\text{learning_rate} \\times \\text{gradient}) \\]\n",
        "\n",
        "\n",
        "\n",
        "Feel free to design the architecture of your model, select appropriate hyperparameters, and choose activation functions based on your specific requirements and objectives! This flexibility allows you to tailor the model to optimally address your unique dataset and performance goals.\n",
        "\n",
        "\n",
        "All calculations in this exercise **should be performed using NumPy** and its functions.\n",
        "\n",
        "**Please complete the #TODO parts of the following cell**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JVlRjqOX3BXH"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "class MLP:\n",
        "    \"\"\"\n",
        "    A class representing a Multilayer Perceptron (MLP).\n",
        "\n",
        "    Attributes:\n",
        "        W1, W2, ... (np.ndarray): Weights for each layer.\n",
        "        B1, B2, ... (np.ndarray): Biases for each layer.\n",
        "        dW1, dW2, ... (np.ndarray): Gradients of weights for each layer during backpropagation.\n",
        "        dB1, dB2, ... (np.ndarray): Gradients of biases for each layer during backpropagation.\n",
        "        (You can add other attributes related to activations and gradients during forward and backward passes)\n",
        "\n",
        "    Methods:\n",
        "        forward(X): Performs the forward pass using the input X.\n",
        "        Loss(Y): Computes the cross-entropy loss for the output compared to true labels Y.\n",
        "        backward(X, Y): Performs the backward pass using the input X and labels Y to compute gradients.\n",
        "        step(learning_rate): Updates the weights and biases according to the computed gradients and learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_inp, n_hid1, n_hid2, n_hid3, n_out):\n",
        "        \"\"\"\n",
        "        Initializes a new MLP instance with specified layer sizes.\n",
        "        (Don't forget to initialize the intermediate values and their gradients as well!)\n",
        "\n",
        "        Parameters:\n",
        "            n_inp (int): Number of input neurons.\n",
        "            n_hid1 (int): Number of neurons in the first hidden layer.\n",
        "            n_hid2 (int): Number of neurons in the second hidden layer.\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "            n_hidN (int): Number of neurons in the Nth hidden layer.\n",
        "            n_out (int): Number of output neurons (typically the number of classes).\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        self.n_inp = n_inp\n",
        "        self.n_hid1 = n_hid1\n",
        "        self.n_hid2 = n_hid2\n",
        "        self.n_hid3 = n_hid3\n",
        "        # self.n_hid4 = n_hid4\n",
        "        # self.n_hid5 = n_hid5\n",
        "        self.n_out = n_out\n",
        "\n",
        "        # Initialize weights and biases for each layer\n",
        "        self.W1 = np.random.randn(n_inp, n_hid1) * np.sqrt(2 / n_inp)\n",
        "        self.B1 = np.zeros((1, n_hid1))\n",
        "        self.W2 = np.random.randn(n_hid1, n_hid2) * np.sqrt(2 / n_hid1)\n",
        "        self.B2 = np.zeros((1, n_hid2))\n",
        "        self.W3 = np.random.randn(n_hid2, n_hid3) * np.sqrt(2 / n_hid2)\n",
        "        self.B3 = np.zeros((1, n_hid3))\n",
        "        self.W4 = np.random.randn(n_hid3, n_out) * np.sqrt(2 / n_hid3)\n",
        "        self.B4 = np.zeros((1, n_out))\n",
        "        # self.W5 = np.random.randn(n_hid4, n_hid5) * np.sqrt(2 / n_hid4)\n",
        "        # self.B5 = np.zeros((1, n_hid5))\n",
        "        # self.W6 = np.random.randn(n_hid5, n_out) * np.sqrt(2 / n_hid5)\n",
        "        # self.B6 = np.zeros((1, n_out))\n",
        "\n",
        "        # Initialize gradients\n",
        "        self.dW1 = np.zeros_like(self.W1)\n",
        "        self.dB1 = np.zeros_like(self.B1)\n",
        "        self.dW2 = np.zeros_like(self.W2)\n",
        "        self.dB2 = np.zeros_like(self.B2)\n",
        "        self.dW3 = np.zeros_like(self.W3)\n",
        "        self.dB3 = np.zeros_like(self.B3)\n",
        "        self.dW4 = np.zeros_like(self.W4)\n",
        "        self.dB4 = np.zeros_like(self.B4)\n",
        "        # self.dW5 = np.zeros_like(self.W5)\n",
        "        # self.dB5 = np.zeros_like(self.B5)\n",
        "        # self.dW6 = np.zeros_like(self.W6)\n",
        "        # self.dB6 = np.zeros_like(self.B6)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Perform the forward pass through the network and return the probability of each class.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Input data matrix, where each row represents a sample.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probabilities of each class for each input sample.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # Layer 1\n",
        "        self.Z1 = X @ self.W1 + self.B1\n",
        "        self.A1 = relu(self.Z1)  # Apply relu activation\n",
        "\n",
        "        # Layer 2\n",
        "        self.Z2 = self.A1 @ self.W2 + self.B2\n",
        "        self.A2 = relu(self.Z2)  # Apply relu activation\n",
        "\n",
        "        # Layer 3\n",
        "        self.Z3 = self.A2 @ self.W3 + self.B3\n",
        "        self.A3 = relu(self.Z3)  # Apply relu activation\n",
        "\n",
        "        # # Layer 4\n",
        "        # self.Z4 = self.A3 @ self.W4 + self.B4\n",
        "        # self.A4 = relu(self.Z4)  # Apply relu activation\n",
        "\n",
        "        # # Layer 5\n",
        "        # self.Z5 = self.A4 @ self.W5 + self.B5\n",
        "        # self.A5 = relu(self.Z5)  # Apply relu activation\n",
        "\n",
        "        # Output Layer\n",
        "        self.Z4 = self.A3 @ self.W4 + self.B4\n",
        "        self.A4 = softmax(self.Z4)\n",
        "\n",
        "        return self.A4\n",
        "\n",
        "\n",
        "    def Loss(self, Y):\n",
        "        \"\"\"\n",
        "        Compute the cross-entropy loss for the output of the network compared to the true labels.\n",
        "\n",
        "        Parameters:\n",
        "            Y (np.ndarray): True labels for the input data, where each entry is an index of the true class.\n",
        "\n",
        "        Returns:\n",
        "            float: The mean cross-entropy loss across all input samples.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        m = Y.shape[0]\n",
        "        log_likelihood = -np.log(self.A4[range(m), Y])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def backward(self, X, Y):\n",
        "        \"\"\"\n",
        "        Perform the backward pass using the input and labels to compute gradients for learning.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Input data matrix, where each row represents a sample.\n",
        "            Y (np.ndarray): True labels for the input data, where each entry is an index of the true class.\n",
        "\n",
        "        Returns:\n",
        "            None: This method updates class attributes corresponding to gradients.\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # One-hot encode Y\n",
        "        Y_one_hot = np.zeros((m, self.n_out))\n",
        "        Y_one_hot[np.arange(m), Y] = 1\n",
        "\n",
        "        # dZ6 = self.A6 - Y_one_hot\n",
        "        # self.dW6 = self.A5.T @ dZ6 / m\n",
        "        # self.dB6 = np.sum(dZ6, axis=0, keepdims=True) / m\n",
        "\n",
        "        # # hidden layer 5 relu\n",
        "        # dA5 = dZ6 @ self.W6.T\n",
        "        # dZ5 = dA5 * (self.Z5 > 0)\n",
        "\n",
        "        # self.dW5 = self.A4.T @ dZ5 / m\n",
        "        # self.dB5 = np.sum(dZ5, axis=0, keepdims=True) / m\n",
        "\n",
        "        # hidden layer 4 (relu)\n",
        "        dZ4 = self.A4 - Y_one_hot\n",
        "        # dZ4 = dA4 * (self.Z4 > 0)\n",
        "\n",
        "        self.dW4 = self.A3.T @ dZ4 / m\n",
        "        self.dB4 = np.sum(dZ4, axis=0, keepdims=True) / m\n",
        "\n",
        "        # hidden layer 3 (ReLU)\n",
        "        dA3 = dZ4 @ self.W4.T\n",
        "        dZ3 = dA3 * (self.Z3 > 0)\n",
        "\n",
        "        self.dW3 = self.A2.T @ dZ3 / m\n",
        "        self.dB3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
        "\n",
        "        # hidden layer 2 (relu)\n",
        "        dA2 = dZ3 @ self.W3.T\n",
        "        dZ2 = dA2 * (self.Z2 > 0)\n",
        "\n",
        "        self.dW2 = self.A1.T @ dZ2 / m\n",
        "        self.dB2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "        # hidden layer 1 (relu)\n",
        "        dA1 = dZ2 @ self.W2.T\n",
        "        dZ1 = dA1 * (self.Z1 > 0)\n",
        "\n",
        "        self.dW1 = X.T @ dZ1 / m\n",
        "        self.dB1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Update the model's weights and biases based on the gradients computed during backpropagation.\n",
        "\n",
        "        Parameters:\n",
        "            learning_rate (float): The learning rate to use for the update step.\n",
        "\n",
        "        Returns:\n",
        "            None: This method updates the weights and biases of the MLP.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        self.W1 -= learning_rate * self.dW1\n",
        "        self.B1 -= learning_rate * self.dB1\n",
        "        self.W2 -= learning_rate * self.dW2\n",
        "        self.B2 -= learning_rate * self.dB2\n",
        "        self.W3 -= learning_rate * self.dW3\n",
        "        self.B3 -= learning_rate * self.dB3\n",
        "        self.W4 -= learning_rate * self.dW4\n",
        "        self.B4 -= learning_rate * self.dB4\n",
        "        # self.W5 -= learning_rate * self.dW5\n",
        "        # self.B5 -= learning_rate * self.dB5\n",
        "        # self.W6 -= learning_rate * self.dW6\n",
        "        # self.B6 -= learning_rate * self.dB6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExkjHV6B3BXI"
      },
      "source": [
        "## Setting the Hyperparameters and Model Configuration:\n",
        "\n",
        "Hyperparameters, such as the learning rate, number of epochs, and batch size, play a crucial role in the training process and significantly influence the model's performance and efficiency. Unlike model parameters that are learned during training, hyperparameters must be predefined and can have a profound impact on the learning dynamics and final outcomes.\n",
        "\n",
        "- **Learning Rate:** Determines the step size at each iteration while moving toward a minimum of the loss function. An appropriately chosen learning rate ensures that the model converges efficiently to a good solution.\n",
        "\n",
        "- **Number of Epochs:** The total number of times the training dataset is passed through the network. Properly setting this value ensures that the model is adequately trained without falling into the pitfalls of underfitting or overfitting.\n",
        "\n",
        "- **Batch Size:** Dictates the number of training examples used in one iteration. It balances the trade-off between the regularization effect and the stability of the gradient, also impacting memory usage and computational speed.\n",
        "\n",
        "- **Model Size and Parameters:** Refers to the architecture of the MLP, including the number of layers and the number of neurons in each layer. The model's size directly affects its capacity to learn, with more parameters allowing for more complex functions to be modeled. However, larger models also require more data to train effectively without overfitting and entail higher computational costs. You could define this part in the previous cell as you wrote tha MLP class as well.\n",
        "\n",
        "In the next cell, you will define these hyperparameters and configure the model's architecture.\n",
        "\n",
        "**Please complete the #TODO parts of the following cell**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-PR0eXE33BXI"
      },
      "outputs": [],
      "source": [
        "# Setting the hyperparameters\n",
        "# TODO\n",
        "n_inp = 784\n",
        "n_hid1 = 128\n",
        "n_hid2 = 64\n",
        "n_hid3 = 32\n",
        "# n_hid4 = 64\n",
        "# n_hid5 = 32\n",
        "n_out = 10\n",
        "\n",
        "batch_size = 100\n",
        "learning_rate = 0.01\n",
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRfpz92C3BXI"
      },
      "source": [
        "## Creating Batches for Different Splits of Data (5 Points)\n",
        "\n",
        "In the next cell, you should focus on batching the different splits of the MNIST dataset: the training, validation, and test sets.\n",
        "\n",
        "Implementing this batching process will prepare your data for the training loop, where each batch will be fed into the model sequentially to perform forward and backward passes and update the model parameters.\n",
        "\n",
        "**Please complete the #TODO parts of the following cell**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PpnukMrv3BXI"
      },
      "outputs": [],
      "source": [
        "# Creating batches\n",
        "# TODO\n",
        "training_batches = [(X_train[i:i+batch_size], Y_train[i:i+batch_size]) for i in range(0, len(X_train), batch_size)]\n",
        "\n",
        "\n",
        "# TODO\n",
        "validation_batches = [(X_val[i:i+batch_size], Y_val[i:i+batch_size]) for i in range(0, len(X_val), batch_size)]\n",
        "\n",
        "\n",
        "# TODO\n",
        "test_batches = [(X_test[i:i+batch_size], Y_test[i:i+batch_size]) for i in range(0, len(X_test), batch_size)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Xp_0-O3BXI"
      },
      "source": [
        "## Writing the Training Loop (15 Points)\n",
        "\n",
        "Training a neural network involves iteratively adjusting its parameters to minimize a loss function. This process is encapsulated within a training loop, where the model learns from the training data over multiple epochs. Each epoch represents one complete pass through the entire dataset.\n",
        "\n",
        "In the next cell, you should do the following:\n",
        "\n",
        "1. **Initialize the MLP Model:** Before you can start training, you need to instantiate your MLP class.\n",
        "\n",
        "2. **Implement the Training Loop:** The training loop should include the following key steps for each epoch:\n",
        "   - **Forward Pass:** For each batch of data, compute the model's predictions by passing the data through the model from input to output.\n",
        "   - **Compute Loss:** Evaluate how well the model's predictions match the actual labels by calculating the loss, using a suitable loss function for classification (e.g., cross-entropy loss).\n",
        "   - **Backward Pass:** Compute the gradients of the loss function with respect to the model's parameters.\n",
        "   - **Update Parameters:** Adjust the model's parameters in the opposite direction of the gradients to minimize the loss.\n",
        "   - **Validation:** Periodically, after a set number of epochs, evaluate the model's performance on the validation set to monitor its learning progress and adjust training parameters if necessary.\n",
        "\n",
        "Remember to track the loss during training to monitor the model's performance over time. This information is valuable for debugging and optimizing the training process.\n",
        "\n",
        "**Please complete the #TODO parts of the following cell**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OId0ysbg3BXI",
        "outputId": "16179250-80f5-46cb-924c-5f62ff9d9944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10  |  Training Loss: 2.3128  |  Validation Loss: 2.3079\n",
            "Epoch 2/10  |  Training Loss: 2.3048  |  Validation Loss: 2.3057\n",
            "Epoch 3/10  |  Training Loss: 2.3024  |  Validation Loss: 2.3072\n",
            "Epoch 4/10  |  Training Loss: 2.3008  |  Validation Loss: 2.3083\n",
            "Epoch 5/10  |  Training Loss: 2.2996  |  Validation Loss: 2.3095\n",
            "Epoch 6/10  |  Training Loss: 2.2986  |  Validation Loss: 2.3102\n",
            "Epoch 7/10  |  Training Loss: 2.2977  |  Validation Loss: 2.3104\n",
            "Epoch 8/10  |  Training Loss: 2.2968  |  Validation Loss: 2.3109\n",
            "Epoch 9/10  |  Training Loss: 2.2961  |  Validation Loss: 2.3117\n",
            "Epoch 10/10  |  Training Loss: 2.2953  |  Validation Loss: 2.3119\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "# TODO\n",
        "mlp = MLP(n_inp=n_inp, n_hid1=n_hid1, n_hid2=n_hid2, n_hid3=n_hid3, n_out=n_out)\n",
        "\n",
        "# Training loop\n",
        "# TODO\n",
        "epoch_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for X, Y in training_batches:\n",
        "        # Forward pass\n",
        "        predictions = mlp.forward(X)\n",
        "        # Calculate the loss\n",
        "        loss = mlp.Loss(Y)\n",
        "        epoch_loss += loss\n",
        "        # Backward pass\n",
        "        mlp.backward(X, Y)\n",
        "        mlp.step(learning_rate)\n",
        "\n",
        "\n",
        "    val_loss = 0\n",
        "    for X, Y in validation_batches:\n",
        "        # Forward pass\n",
        "        predictions = mlp.forward(X)\n",
        "        # Calculate the loss\n",
        "        val_loss += mlp.Loss(Y)\n",
        "\n",
        "    epoch_losses.append(epoch_loss / len(training_batches))\n",
        "    val_losses.append(val_loss / len(validation_batches))\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs}  |  Training Loss: {(epoch_loss/len(training_batches)):.4f}  |  Validation Loss: {(val_loss/len(validation_batches)):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K72kGCga3BXI"
      },
      "source": [
        "## Plotting the Validation and Training Loss (5 Points)\n",
        "\n",
        "Visualizing the training and validation loss over time is a crucial aspect of the model development process. It provides valuable insights into how well the model is learning, helps identify issues such as overfitting or underfitting, and can guide decisions on model adjustments or hyperparameter tuning.\n",
        "\n",
        "In the next cell, you will create a plot to visualize the training and validation loss over time.\n",
        "\n",
        "**Please complete the #TODO parts of the following cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s08VFSXZ3BXN"
      },
      "outputs": [],
      "source": [
        "# plot losses\n",
        "# TODO\n",
        "plt.plot(epoch_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hpVu4W43BXN"
      },
      "source": [
        "## Visualizing Some Examples (5 Points)\n",
        "\n",
        "In the next cell, visualize a subset of the test images along with their true and predicted labels.\n",
        "\n",
        "**Please complete the #TODO parts of the following cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aNvqc9_3BXO"
      },
      "outputs": [],
      "source": [
        "# Visualize test images\n",
        "# TODO\n",
        "indices = np.random.choice(len(X_test), 5, replace=False)\n",
        "selected_images = X_test[indices]\n",
        "true_labels = Y_test[indices]\n",
        "\n",
        "# Predict labels using the trained model\n",
        "predictions = np.argmax(mlp.forward(selected_images), axis=1)\n",
        "\n",
        "# Plot the images along with their true and predicted labels\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, idx in enumerate(indices):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(selected_images[i].reshape(28, 28), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"True: {true_labels[i]}\\nPred: {predictions[i]}\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAzLa3cP3BXO"
      },
      "source": [
        "## Calculating Model Metrics for Different Splits (15 Points)\n",
        "\n",
        "Evaluating the performance of your MLP model across different data splits (training, validation, and test sets) is critical for understanding its generalization ability and overall effectiveness. This evaluation helps ensure that the model performs well not only on the data it was trained on but also on new, unseen data.\n",
        "\n",
        "In the next cell, you should calculate model metrics(precision, recall, F1, ...) for each split and report them.\n",
        "\n",
        "You can use sklearn library for this part, if you want to.\n",
        "\n",
        "**Please complete the #TODO parts of the following cells**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t9BcYxP3BXO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Training data metrics\n",
        "# TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdtZcJ0i3BXO"
      },
      "outputs": [],
      "source": [
        "# Validation data metrics\n",
        "# TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdEpnjdc3BXO"
      },
      "outputs": [],
      "source": [
        "# Test data metrics\n",
        "# TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV2-h6g83BXO"
      },
      "source": [
        "## Reporting the Findings (5 Points)\n",
        "\n",
        "In the next cell you should do the following:\n",
        "\n",
        "1. **Analyze Results:** Compare the metrics across the different splits. A model that performs exceptionally well on the training set but poorly on the validation/test sets may be overfitting. Conversely, underfitting may be indicated by poor performance across all sets.\n",
        "\n",
        "2. **Report Findings:** Summarize the results in a clear and concise manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JjGaYvA3BXP"
      },
      "source": [
        "### Your Explanation:"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}