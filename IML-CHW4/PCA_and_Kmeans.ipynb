{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\">An Introduction to Machine Learning - 25737</h1>\n",
        "<h4 align=\"center\">Dr. Yasaei</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Autumn 2024</h4>\n",
        "\n",
        "**Student Name**:\n",
        "\n",
        "**Student ID**:"
      ],
      "metadata": {
        "id": "_RMlwII9ewGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementing PCA from Scratch and Using it with K-Means\n",
        "\n",
        "## Introduction and Purpose\n",
        "\n",
        "In this exercise, you will:\n",
        "\n",
        "1. **Implement Principal Component Analysis (PCA) from scratch** using NumPy (without relying on `sklearn.decomposition.PCA`).\n",
        "2. Apply your custom PCA to a dataset to reduce its dimensionality.\n",
        "3. Run **K-Means clustering** on both the original data and the PCA-reduced data.\n",
        "4. Compare the clustering results and visualize them.\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a dimensionality reduction technique that finds linear combinations of features (principal components) that explain the most variance in the data. By implementing it yourself, you will gain a deeper understanding of the underlying math (e.g., covariance matrix, eigenvalues, eigenvectors).\n",
        "\n",
        "After reducing the data dimensions with your custom PCA, you will run K-Means clustering and evaluate whether PCA helps or hinders the clustering performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "tc3cxZkMdLNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data Loading and Exploration\n",
        "\n",
        "**Tasks:**  \n",
        "- Load the Iris dataset using `sklearn.datasets.load_iris`.\n",
        "- Print the shape of `X` and a few samples to understand the data.\n",
        "\n",
        "**Question:** The Iris dataset has 4 features and 3 classes. Without looking at the labels, do you expect PCA to help cluster the data better in fewer dimensions?\n",
        "\n",
        "asnwer:\n"
      ],
      "metadata": {
        "id": "PiH4RZa5dV6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Load Iris dataset and print basic info\n",
        "# from sklearn.datasets import load_iris\n",
        "# import numpy as np\n",
        "\n",
        "# iris = ...\n",
        "# X = ...\n",
        "# y = ...\n",
        "# print(\"Shape of X:\", ...)\n",
        "# print(\"First 5 rows:\\n\", ...)"
      ],
      "metadata": {
        "id": "E-UaF6bmdU4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data Preprocessing (Scaling)\n",
        "\n",
        "PCA and K-Means are sensitive to feature scales. Standardize the data before applying PCA.\n",
        "\n",
        "**Tasks:**  \n",
        "- Scale the data using StandardScaler.\n",
        "- Print the mean and variance of the scaled data to confirm.\n",
        "\n",
        "**Hint:** Use `StandardScaler` from `sklearn.preprocessing`.\n"
      ],
      "metadata": {
        "id": "Axp___vHdkg5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iENYluredKZw"
      },
      "outputs": [],
      "source": [
        "# TODO: Scale the data\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scaler = ...\n",
        "# X_scaled = ...\n",
        "# print(\"Mean after scaling:\", X_scaled.mean(axis=0))\n",
        "# print(\"Std after scaling:\", X_scaled.std(axis=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Implementing PCA from Scratch\n",
        "\n",
        "We will:\n",
        "1. Compute the covariance matrix of the scaled data.\n",
        "2. Find its eigenvalues and eigenvectors.\n",
        "3. Sort eigenvectors by their eigenvalues in descending order.\n",
        "4. Select the top `n_components` principal components.\n",
        "5. Project the data onto these principal components.\n",
        "\n",
        "**Tasks:**\n",
        "Implement a `my_pca(X, n_components)` function that:\n",
        " - Computes the covariance matrix.\n",
        " - Performs eigen decomposition.\n",
        " - Sorts eigenvectors by eigenvalues.\n",
        " - Returns the projected data and the selected eigenvectors, as well as explained variance ratio.\n",
        "\n",
        "**Question:** Why do we pick the eigenvectors with the largest eigenvalues as the principal components?\n",
        "\n",
        "answer:\n"
      ],
      "metadata": {
        "id": "RHB6PJycdpND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement PCA from scratch\n",
        "\n",
        "# def my_pca(X, n_components=2):\n",
        "#     # Step 1: Compute covariance matrix: shape (D, D)\n",
        "#     # cov_matrix = ...\n",
        "\n",
        "#     # Step 2: Eigen decomposition\n",
        "#     # eigenvalues, eigenvectors = ...\n",
        "\n",
        "#     # Step 3: Sort eigenvalues and eigenvectors in descending order\n",
        "#     # idx = ...\n",
        "#     # eigenvalues = ...\n",
        "#     # eigenvectors = ...\n",
        "\n",
        "#     # Step 4: Select top n_components\n",
        "#     # selected_eigenvectors = ...\n",
        "#     # selected_eigenvalues = ...\n",
        "\n",
        "#     # Step 5: Project data: Z = X * selected_eigenvectors\n",
        "#     # Z = ...\n",
        "\n",
        "#     # Compute explained variance ratio\n",
        "#     # explained_variance_ratio = selected_eigenvalues / np.sum(eigenvalues)\n",
        "\n",
        "#     # return Z, selected_eigenvectors, explained_variance_ratio\n",
        "#     pass\n",
        "\n",
        "# Test your my_pca function\n",
        "# Z, eigvecs, exp_var_ratio = my_pca(X_scaled, n_components=2)\n",
        "# print(\"Shape of projected data:\", Z.shape)\n",
        "# print(\"Explained variance ratio:\", exp_var_ratio)\n"
      ],
      "metadata": {
        "id": "NvrwECLWdq8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Comparing K-Means Clustering Before and After PCA\n",
        "\n",
        "We will:\n",
        "1. Run K-Means clustering on the original scaled data.\n",
        "2. Run K-Means clustering on the PCA-reduced data (from `my_pca`).\n",
        "\n",
        "**Tasks:**  \n",
        "- Perform K-Means with `k=3` (since Iris has 3 classes).\n",
        "- Extract cluster labels from both runs.\n",
        "\n",
        "**Hint:** Use `KMeans` from `sklearn.cluster`.\n",
        "\n",
        "**Question:** Do you think clustering on just the first two principal components will yield a similar or better ARI score than using all features?\n",
        "\n",
        "answer:"
      ],
      "metadata": {
        "id": "mHHN7XQ3dupe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: K-Means on original scaled data\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# kmeans_original = ...\n",
        "# labels_original = ...\n",
        "\n",
        "# TODO: K-Means on PCA data\n",
        "# kmeans_pca = ...\n",
        "# labels_pca = ...\n"
      ],
      "metadata": {
        "id": "aw0M_Peldwo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Evaluating the Clusters\n",
        "\n",
        "Use Adjusted Rand Index (ARI) to see how well clusters align with the true labels.\n",
        "\n",
        "**Tasks:**  \n",
        "- Compute ARI for both sets of labels against `y`.\n",
        "- Print and compare.\n",
        "\n",
        "**Hint:** `from sklearn.metrics import adjusted_rand_score`.\n"
      ],
      "metadata": {
        "id": "CNxtlgIYdyt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute ARI scores\n",
        "# from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "# ari_original = ...\n",
        "# ari_pca = ...\n",
        "# print(\"ARI (Original):\", ari_original)\n",
        "# print(\"ARI (PCA):\", ari_pca)\n"
      ],
      "metadata": {
        "id": "d4nTh9ybd0bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Visualizing the PCA Results\n",
        "\n",
        "Visualize the PCA-reduced data and the clusters.\n",
        "\n",
        "**Tasks:**  \n",
        "- Plot `Z[:,0]` vs `Z[:,1]` and color by `labels_pca`.\n",
        "- (Optional) Also plot true labels to compare how well the clusters match.\n",
        "\n",
        "**Hint:** Use `matplotlib.pyplot.scatter`.\n"
      ],
      "metadata": {
        "id": "zN15L4D7d2qN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Visualize PCA results\n",
        "\n"
      ],
      "metadata": {
        "id": "Ztc7XZi9d4I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6-Xor_IFd-dK"
      }
    }
  ]
}