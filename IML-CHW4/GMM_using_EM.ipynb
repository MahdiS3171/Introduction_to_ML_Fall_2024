{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\">An Introduction to Machine Learning - 25737</h1>\n",
        "<h4 align=\"center\">Dr. Yasaei</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Autumn 2024</h4>\n",
        "\n",
        "**Student Name**: Seyyed Amirmahdi Sadrzadeh\n",
        "\n",
        "**Student ID**: 401102015"
      ],
      "metadata": {
        "id": "UUVoHJM1Xed5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Mixture Models with EM\n",
        "\n",
        "## Introduction and Purpose\n",
        "\n",
        "In this exercise, you will:\n",
        "\n",
        "1. Implement a **Gaussian Mixture Model (GMM)** using the Expectation-Maximization (EM) algorithm **from scratch** (using NumPy and basic Python operations).\n",
        "2. Implement the **same GMM model using PyTorch**.\n",
        "3. Compare and contrast the two implementations (performance, complexity, ease of coding, etc.).\n",
        "\n",
        "**Gaussian Mixture Models** assume that data is generated from a mixture of several Gaussian distributions. The EM algorithm iteratively updates the parameters (means, covariances, and mixture weights) of these Gaussians to maximize the likelihood of observed data.\n",
        "\n"
      ],
      "metadata": {
        "id": "_q3lWTPWu_Wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data Loading and Exploration\n",
        "\n",
        "**Tasks:**  \n",
        "- Load the Iris dataset and store the features in `X` and labels in `y`.\n",
        "- Print the shape of `X` and examine a few rows.\n",
        "- **Hint:** Use `sklearn.datasets.load_iris()` to load the data."
      ],
      "metadata": {
        "id": "R7YUHxtMvPOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yTjogaS8u-Sf",
        "outputId": "d7a1e1d0-ad30-4b48-c335-a0a8cc1857f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (150, 4)\n",
            "First 5 samples:\n",
            " [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load the Iris dataset and print shape\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"First 5 samples:\\n\", X[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data Preprocessing (Scaling)\n",
        "\n",
        "**Tasks:**  \n",
        "- Scale the data using `StandardScaler` so that each feature has zero mean and unit variance.\n",
        "- **Hint:** `from sklearn.preprocessing import StandardScaler`.\n"
      ],
      "metadata": {
        "id": "5MxjH7L4vVVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Scale the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Mean after scaling:\", X_scaled.mean(axis=0))\n",
        "print(\"Std after scaling:\", X_scaled.std(axis=0))\n"
      ],
      "metadata": {
        "id": "kIxnM1ybvSpg",
        "outputId": "1ed452bc-0ee8-4673-ea6c-fe582daaea28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean after scaling: [-1.69031455e-15 -1.84297022e-15 -1.69864123e-15 -1.40924309e-15]\n",
            "Std after scaling: [1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Implementing GMM with EM **from scratch** (NumPy-based)\n",
        "\n",
        "We will first implement GMM using NumPy arrays and basic operations, without PyTorch.\n",
        "\n",
        "**Tasks:**  \n",
        "- Choose the number of components `K` (e.g., K=3).\n",
        "- Initialize the parameters: means, covariances (diagonal), and mixture weights.\n",
        "- Write functions for the E-step and M-step of the EM algorithm.\n",
        "- Run the EM algorithm for a fixed number of iterations.\n",
        "\n",
        "**Hints for Implementation:**\n",
        "\n",
        "- Means: K x D array.\n",
        "- Covariances: K x D x D (diagonal only, so you mainly store variances per feature).\n",
        "- Weights: K-dimensional array, summing to 1.\n",
        "- To compute Gaussian densities, recall the formula for the probability density of a multivariate Gaussian.\n",
        "- For the E-step, compute responsibilities using the mixture components and their densities.\n",
        "- For the M-step, update means, covariances, and weights using the responsibilities.\n",
        "\n",
        "After implementing and running EM, extract cluster assignments by taking `argmax` of responsibilities.\n"
      ],
      "metadata": {
        "id": "QlrQIMl7vYde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set number of components\n",
        "K = 3\n",
        "N, D = X_scaled.shape\n",
        "\n",
        "# TODO: Initialize means, covariances, and weights\n",
        "means = np.random.rand(K, D) * np.ptp(X_scaled, axis=0) + np.min(X_scaled, axis=0)\n",
        "covariances = np.array([np.eye(D) for _ in range(K)])\n",
        "weights = np.ones(K) / K\n",
        "\n",
        "# TODO: Define a function to compute Gaussian PDF values for each component\n",
        "def gaussian_pdf(X, mean, cov):\n",
        "    det_cov = np.linalg.det(cov)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "    norm_const = 1 / (np.sqrt((2 * np.pi) ** D * det_cov))\n",
        "    diff = X - mean\n",
        "    return norm_const * np.exp(-0.5 * np.sum(diff @ inv_cov * diff, axis=1))\n",
        "\n",
        "# TODO: E-step\n",
        "def e_step(X, means, covariances, weights):\n",
        "    responsibilities = np.zeros((N, K))\n",
        "    for k in range(K):\n",
        "        responsibilities[:, k] = weights[k] * gaussian_pdf(X, means[k], covariances[k])\n",
        "    responsibilities /= np.sum(responsibilities, axis=1, keepdims=True)\n",
        "    return responsibilities\n",
        "\n",
        "# TODO: M-step\n",
        "def m_step(X, responsibilities):\n",
        "    Nk = np.sum(responsibilities, axis=0)\n",
        "    means = (responsibilities.T @ X) / Nk[:, None]\n",
        "    covariances = np.zeros((K, D, D))\n",
        "    for k in range(K):\n",
        "        diff = X - means[k]\n",
        "        covariances[k] = (responsibilities[:, k, None] * diff).T @ diff / Nk[k]\n",
        "    weights = Nk / N\n",
        "    return means, covariances, weights\n",
        "\n",
        "# Run EM\n",
        "max_iters = 100\n",
        "for _ in range(max_iters):\n",
        "    responsibilities = e_step(X_scaled, means, covariances, weights)\n",
        "    means, covariances, weights = m_step(X_scaled, responsibilities)\n",
        "\n",
        "cluster_labels_numpy = np.argmax(responsibilities, axis=1)\n"
      ],
      "metadata": {
        "id": "EclPwJbfvah5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Implementing GMM with EM **using PyTorch**\n",
        "\n",
        "Now, we will implement the same algorithm using PyTorch tensors. The steps are similar, but you will use `torch` operations. This might simplify certain operations and open the door to GPU acceleration.\n",
        "\n",
        "**Tasks:**  \n",
        "- Convert `X_scaled` to a PyTorch tensor.\n",
        "- Initialize parameters as `torch.tensor`s.\n",
        "- Implement E-step and M-step in PyTorch.\n",
        "- Run EM for a fixed number of iterations.\n",
        "- Extract cluster labels.\n",
        "\n",
        "**Hints:**\n",
        "- Use `torch.tensor(X_scaled, dtype=torch.float32)` to create a PyTorch tensor.\n",
        "- Operations are similar but use `torch.sum`, `torch.exp`, etc.\n",
        "- Watch out for broadcasting rules and ensure shapes align.\n"
      ],
      "metadata": {
        "id": "deQSDjycvmIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# TODO: Convert data to torch tensor\n",
        "X_torch = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "\n",
        "# TODO: Initialize means, covariances, weights as torch tensors\n",
        "means_torch = torch.rand((K, D), dtype=torch.float32) * (X_torch.max(0).values - X_torch.min(0).values) + X_torch.min(0).values\n",
        "covariances_torch = torch.stack([torch.eye(D, dtype=torch.float32) for _ in range(K)])\n",
        "weights_torch = torch.ones(K, dtype=torch.float32) / K\n",
        "\n",
        "# TODO: Implement gaussian_pdf using torch operations\n",
        "def gaussian_pdf_torch(X, mean, cov):\n",
        "    det_cov = torch.det(cov)\n",
        "    inv_cov = torch.linalg.inv(cov)\n",
        "    norm_const = 1 / (torch.sqrt((2 * torch.pi) ** D * det_cov))\n",
        "    diff = X - mean\n",
        "    return norm_const * torch.exp(-0.5 * torch.sum(diff @ inv_cov * diff, dim=1))\n",
        "\n",
        "# TODO: E-step in torch\n",
        "def e_step_torch(X, means, covariances, weights):\n",
        "    responsibilities = torch.zeros((N, K), dtype=torch.float32)\n",
        "    for k in range(K):\n",
        "        responsibilities[:, k] = weights[k] * gaussian_pdf_torch(X, means[k], covariances[k])\n",
        "    responsibilities /= responsibilities.sum(dim=1, keepdim=True)\n",
        "    return responsibilities\n",
        "\n",
        "# TODO: M-step in torch\n",
        "def m_step_torch(X, responsibilities):\n",
        "    Nk = responsibilities.sum(dim=0)\n",
        "    means = (responsibilities.T @ X) / Nk[:, None]\n",
        "    covariances = torch.zeros((K, D, D), dtype=torch.float32)\n",
        "    for k in range(K):\n",
        "        diff = X - means[k]\n",
        "        covariances[k] = (responsibilities[:, k, None] * diff).T @ diff / Nk[k]\n",
        "    weights = Nk / N\n",
        "    return means, covariances, weights\n",
        "\n",
        "# Run EM in torch\n",
        "max_iters = 100\n",
        "for _ in range(max_iters):\n",
        "    responsibilities_torch = e_step_torch(X_torch, means_torch, covariances_torch, weights_torch)\n",
        "    means_torch, covariances_torch, weights_torch = m_step_torch(X_torch, responsibilities_torch)\n",
        "\n",
        "cluster_labels_torch = torch.argmax(responsibilities_torch, dim=1)\n"
      ],
      "metadata": {
        "id": "_-G1vfhGvi4y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Evaluating and Comparing Both Implementations\n",
        "\n",
        "**Tasks:**  \n",
        "- Use `adjusted_rand_score` to compare the cluster labels from both methods against the true labels `y`.\n",
        "- Print the ARI for both NumPy and PyTorch implementations.\n",
        "- Visually inspect if both implementations yield similar results.\n",
        "\n",
        "**Questions:**\n",
        "- Are the ARI scores similar or different between the two implementations?\n",
        "- Which code was easier to write and maintain?\n",
        "- Which implementation might be easier to extend to more complex models?\n"
      ],
      "metadata": {
        "id": "EcEETR0Hvsh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "# TODO: Compute ARI for numpy-based clustering\n",
        "ari_numpy = adjusted_rand_score(y, cluster_labels_numpy)\n",
        "print(\"ARI (NumPy):\", ari_numpy)\n",
        "\n",
        "# TODO: Compute ARI for torch-based clustering\n",
        "ari_torch = adjusted_rand_score(y, cluster_labels_torch.numpy())\n",
        "print(\"ARI (PyTorch):\", ari_torch)\n"
      ],
      "metadata": {
        "id": "xmumvOglvuJ1",
        "outputId": "e0085ba0-75cf-43dd-802c-cc4b42397bf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARI (NumPy): 0.560078733112642\n",
            "ARI (PyTorch): 0.5596495773242369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.1**\n",
        "\n",
        "The ARI scores are very similar or even identical because both implementations are correct and they use the same initialization. The minor difference between these two, is due to numerical precision and differences in NumPy and PyTorch.\n",
        "\n",
        "**Q.2**\n",
        "\n",
        "NumPy was very easier to follow and had simpler mathematical notations. But the because of the built-in function of PyTorch for matrix operations, this could be easier to maintain and makes the debugging easier for the readers.\n",
        "\n",
        "**Q.3**\n",
        "\n",
        "Since the PyTorch has better modular design and more auro-differentiation functions, it is easier to add constraints on parameters using this way."
      ],
      "metadata": {
        "id": "S6B1hxLhL2sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**  \n",
        "1. **Implementation Detail:** What are the main differences in code complexity between a plain NumPy-based implementation and a PyTorch-based one?  \n",
        "answer:\n",
        "\n",
        "2. **Performance:** Which implementation is likely to be more efficient or easier to parallelize and why?  \n",
        "answer:\n",
        "3. **Numerical Stability:** How might PyTorchâ€™s built-in functions improve numerical stability compared to a manual implementation?  \n",
        "answer:\n",
        "\n",
        "4. **Extendability:** If you wanted to add more complex features (e.g., full covariance matrices, regularization), which approach would be simpler and why?\n",
        "answer:"
      ],
      "metadata": {
        "id": "TMzyGBV1wP4D"
      }
    }
  ]
}