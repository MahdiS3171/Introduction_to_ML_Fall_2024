{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\">An Introduction to Machine Learning - 25737</h1>\n",
        "<h4 align=\"center\">Dr. Yasaei</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Autumn 2024</h4>\n",
        "\n",
        "**Student Name**:\n",
        "\n",
        "**Student ID**:"
      ],
      "metadata": {
        "id": "UUVoHJM1Xed5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Mixture Models with EM\n",
        "\n",
        "## Introduction and Purpose\n",
        "\n",
        "In this exercise, you will:\n",
        "\n",
        "1. Implement a **Gaussian Mixture Model (GMM)** using the Expectation-Maximization (EM) algorithm **from scratch** (using NumPy and basic Python operations).\n",
        "2. Implement the **same GMM model using PyTorch**.\n",
        "3. Compare and contrast the two implementations (performance, complexity, ease of coding, etc.).\n",
        "\n",
        "**Gaussian Mixture Models** assume that data is generated from a mixture of several Gaussian distributions. The EM algorithm iteratively updates the parameters (means, covariances, and mixture weights) of these Gaussians to maximize the likelihood of observed data.\n",
        "\n"
      ],
      "metadata": {
        "id": "_q3lWTPWu_Wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data Loading and Exploration\n",
        "\n",
        "**Tasks:**  \n",
        "- Load the Iris dataset and store the features in `X` and labels in `y`.\n",
        "- Print the shape of `X` and examine a few rows.\n",
        "- **Hint:** Use `sklearn.datasets.load_iris()` to load the data."
      ],
      "metadata": {
        "id": "R7YUHxtMvPOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTjogaS8u-Sf"
      },
      "outputs": [],
      "source": [
        "# TODO: Load the Iris dataset and print shape\n",
        "# from sklearn.datasets import load_iris\n",
        "\n",
        "# iris = ...\n",
        "# X = ...\n",
        "# y = ...\n",
        "# print(\"Shape of X:\", ...)\n",
        "# print(\"First 5 samples:\\n\", ...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data Preprocessing (Scaling)\n",
        "\n",
        "**Tasks:**  \n",
        "- Scale the data using `StandardScaler` so that each feature has zero mean and unit variance.\n",
        "- **Hint:** `from sklearn.preprocessing import StandardScaler`.\n"
      ],
      "metadata": {
        "id": "5MxjH7L4vVVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Scale the data\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scaler = ...\n",
        "# X_scaled = ...\n",
        "# print(\"Mean after scaling:\", X_scaled.mean(axis=0))\n",
        "# print(\"Std after scaling:\", X_scaled.std(axis=0))\n"
      ],
      "metadata": {
        "id": "kIxnM1ybvSpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Implementing GMM with EM **from scratch** (NumPy-based)\n",
        "\n",
        "We will first implement GMM using NumPy arrays and basic operations, without PyTorch.\n",
        "\n",
        "**Tasks:**  \n",
        "- Choose the number of components `K` (e.g., K=3).\n",
        "- Initialize the parameters: means, covariances (diagonal), and mixture weights.\n",
        "- Write functions for the E-step and M-step of the EM algorithm.\n",
        "- Run the EM algorithm for a fixed number of iterations.\n",
        "\n",
        "**Hints for Implementation:**\n",
        "\n",
        "- Means: K x D array.\n",
        "- Covariances: K x D x D (diagonal only, so you mainly store variances per feature).\n",
        "- Weights: K-dimensional array, summing to 1.\n",
        "- To compute Gaussian densities, recall the formula for the probability density of a multivariate Gaussian.\n",
        "- For the E-step, compute responsibilities using the mixture components and their densities.\n",
        "- For the M-step, update means, covariances, and weights using the responsibilities.\n",
        "\n",
        "After implementing and running EM, extract cluster assignments by taking `argmax` of responsibilities.\n"
      ],
      "metadata": {
        "id": "QlrQIMl7vYde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set number of components\n",
        "K = 3\n",
        "N, D = X_scaled.shape\n",
        "\n",
        "# TODO: Initialize means, covariances, and weights\n",
        "# means = ...\n",
        "# covariances = ...  # Diagonal covariances, so you can store just var per component and construct diag\n",
        "# weights = ...\n",
        "\n",
        "# TODO: Define a function to compute Gaussian PDF values for each component\n",
        "# def gaussian_pdf(X, mean, cov):\n",
        "#     # X: N x D\n",
        "#     # mean: D\n",
        "#     # cov: D x D (diagonal)\n",
        "#     # return pdf values: N-dim array\n",
        "#     pass\n",
        "\n",
        "# TODO: E-step\n",
        "# def e_step(X, means, covariances, weights):\n",
        "#     # Compute responsibilities\n",
        "#     pass\n",
        "\n",
        "# TODO: M-step\n",
        "# def m_step(X, responsibilities):\n",
        "#     # Update means, covariances, weights\n",
        "#     pass\n",
        "\n",
        "# Run EM\n",
        "\n",
        "# cluster_labels_numpy = ... # argmax of responsibilities\n"
      ],
      "metadata": {
        "id": "EclPwJbfvah5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Implementing GMM with EM **using PyTorch**\n",
        "\n",
        "Now, we will implement the same algorithm using PyTorch tensors. The steps are similar, but you will use `torch` operations. This might simplify certain operations and open the door to GPU acceleration.\n",
        "\n",
        "**Tasks:**  \n",
        "- Convert `X_scaled` to a PyTorch tensor.\n",
        "- Initialize parameters as `torch.tensor`s.\n",
        "- Implement E-step and M-step in PyTorch.\n",
        "- Run EM for a fixed number of iterations.\n",
        "- Extract cluster labels.\n",
        "\n",
        "**Hints:**\n",
        "- Use `torch.tensor(X_scaled, dtype=torch.float32)` to create a PyTorch tensor.\n",
        "- Operations are similar but use `torch.sum`, `torch.exp`, etc.\n",
        "- Watch out for broadcasting rules and ensure shapes align.\n"
      ],
      "metadata": {
        "id": "deQSDjycvmIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# TODO: Convert data to torch tensor\n",
        "# X_torch = ...\n",
        "\n",
        "# TODO: Initialize means, covariances, weights as torch tensors\n",
        "# means_torch = ...\n",
        "# covariances_torch = ...\n",
        "# weights_torch = ...\n",
        "\n",
        "# TODO: Implement gaussian_pdf using torch operations\n",
        "# def gaussian_pdf_torch(X, mean, cov):\n",
        "#     pass\n",
        "\n",
        "# TODO: E-step in torch\n",
        "# def e_step_torch(X, means, covariances, weights):\n",
        "#     pass\n",
        "\n",
        "# TODO: M-step in torch\n",
        "# def m_step_torch(X, responsibilities):\n",
        "#     pass\n",
        "\n",
        "# Run EM in torch\n",
        "\n",
        "# cluster_labels_torch = ... # argmax of responsibilities\n"
      ],
      "metadata": {
        "id": "_-G1vfhGvi4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Evaluating and Comparing Both Implementations\n",
        "\n",
        "**Tasks:**  \n",
        "- Use `adjusted_rand_score` to compare the cluster labels from both methods against the true labels `y`.\n",
        "- Print the ARI for both NumPy and PyTorch implementations.\n",
        "- Visually inspect if both implementations yield similar results.\n",
        "\n",
        "**Questions:**\n",
        "- Are the ARI scores similar or different between the two implementations?\n",
        "- Which code was easier to write and maintain?\n",
        "- Which implementation might be easier to extend to more complex models?\n"
      ],
      "metadata": {
        "id": "EcEETR0Hvsh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "# TODO: Compute ARI for numpy-based clustering\n",
        "# ari_numpy = adjusted_rand_score(y, cluster_labels_numpy)\n",
        "# print(\"ARI (NumPy):\", ari_numpy)\n",
        "\n",
        "# TODO: Compute ARI for torch-based clustering\n",
        "# ari_torch = adjusted_rand_score(y, cluster_labels_torch)\n",
        "# print(\"ARI (PyTorch):\", ari_torch)\n"
      ],
      "metadata": {
        "id": "xmumvOglvuJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**  \n",
        "1. **Implementation Detail:** What are the main differences in code complexity between a plain NumPy-based implementation and a PyTorch-based one?  \n",
        "answer:\n",
        "\n",
        "2. **Performance:** Which implementation is likely to be more efficient or easier to parallelize and why?  \n",
        "answer:\n",
        "3. **Numerical Stability:** How might PyTorchâ€™s built-in functions improve numerical stability compared to a manual implementation?  \n",
        "answer:\n",
        "\n",
        "4. **Extendability:** If you wanted to add more complex features (e.g., full covariance matrices, regularization), which approach would be simpler and why?\n",
        "answer:"
      ],
      "metadata": {
        "id": "TMzyGBV1wP4D"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mwGTXH7GwVeF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}