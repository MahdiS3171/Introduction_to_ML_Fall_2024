{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Machine Learning\n",
        "## Project Phase 2\n",
        "\n",
        "Sina Fathi | 402111261\n",
        "\n",
        "Seyyed Amirmahdi Sadrzadeh | 401102015"
      ],
      "metadata": {
        "id": "hniNtD-Zew1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Data Preproccessing"
      ],
      "metadata": {
        "id": "tvacMiiXb_2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we import the needed libraries:"
      ],
      "metadata": {
        "id": "8ziJ9zUdcH_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from sympy import mod_inverse\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "d3lKKdgBGJQS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use torch library to load the MNIST dataset and then turn them to numpy arrays:"
      ],
      "metadata": {
        "id": "p_vzRIfdcMhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# getting the train data\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
        "\n",
        "X_train_torch = trainset.data\n",
        "Y_train_torch = trainset.targets\n",
        "\n",
        "X_train = X_train_torch.numpy()\n",
        "Y_train = Y_train_torch.numpy()"
      ],
      "metadata": {
        "id": "AyG6f5oKGPHh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the test data\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "X_test_torch = testset.data\n",
        "Y_test_torch = testset.targets\n",
        "\n",
        "X_test = X_test_torch.numpy()\n",
        "Y_test = Y_test_torch.numpy()"
      ],
      "metadata": {
        "id": "C3It8SZpQ_zN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the datas, we binarize them by setting a threshold equal to 128. So all pixels with values higher than 128 will be set to 1 and others will be set to 0. We also flatten the datas to use them in our model:"
      ],
      "metadata": {
        "id": "vyLf42oLclGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 128\n",
        "\n",
        "X_test = (X_test > threshold).astype(np.uint32)\n",
        "X_test_binary = X_test.reshape(-1, 28 * 28)\n",
        "\n",
        "X_train = (X_train > threshold).astype(np.uint32)\n",
        "X_train_binary = X_train.reshape(-1, 28 * 28)"
      ],
      "metadata": {
        "id": "H1nb8nHOKoXd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot to show the samples:"
      ],
      "metadata": {
        "id": "_hNNKcVhdJL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_samples(samples, nrows=2, ncols=5):\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(10, 5))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(samples[i].reshape(28, 28), cmap='binary')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wlGlqEJ7Kt7M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we use the above function to show 10 samples from the dataset:"
      ],
      "metadata": {
        "id": "DptyrqJwdPLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_samples(X_train_binary[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "kr0tRRKyKzrV",
        "outputId": "389f5280-c52d-47f6-d1ad-c5d2e6ff2adc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Boltzman machine:"
      ],
      "metadata": {
        "id": "b0EKwQygdlPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Boltzmann machine is like the one we used in the first phase of the project and follow the same patterns to update the parameters."
      ],
      "metadata": {
        "id": "8rd8PBL8E2LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RBM:\n",
        "    def __init__(self, n_visible, n_hidden, learning_rate=0.1):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.W = np.random.normal(0, 0.1, (n_visible, n_hidden))\n",
        "        self.v_bias = np.zeros(n_visible)\n",
        "        self.h_bias = np.zeros(n_hidden)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (np.random.uniform(size=prob.shape) < prob).astype(np.int32)\n",
        "\n",
        "    def gibbs_sampling(self, hid, k):\n",
        "        v_prob_0 = self.sigmoid(np.dot(hid, self.W.T) + self.v_bias)\n",
        "        v = self.sample(v_prob_0)\n",
        "        for _ in range(k):\n",
        "            h_prob = self.sigmoid(np.dot(v, self.W) + self.h_bias)\n",
        "            h_sample = self.sample(h_prob)\n",
        "            v_prob = self.sigmoid(np.dot(h_sample, self.W.T) + self.v_bias)\n",
        "            v = self.sample(v_prob)\n",
        "        return v\n",
        "\n",
        "    def contrastive_divergence(self, v, k):\n",
        "        h_prob_0 = self.sigmoid(np.dot(v, self.W) + self.h_bias)\n",
        "        h_sample_0 = self.sample(h_prob_0)\n",
        "\n",
        "        v_neg = v.copy()\n",
        "\n",
        "        for _ in range(k):\n",
        "            h_prob = self.sigmoid(np.dot(v_neg, self.W) + self.h_bias)\n",
        "            h_sample = self.sample(h_prob)\n",
        "            v_prob_neg = self.sigmoid(np.dot(h_sample, self.W.T) + self.v_bias)\n",
        "            v_neg = self.sample(v_prob_neg)\n",
        "\n",
        "        h_prob_neg = self.sigmoid(np.dot(v_neg, self.W) + self.h_bias)\n",
        "\n",
        "        positive_grad = np.dot(v.T, h_prob_0)\n",
        "        negative_grad = np.dot(v_neg.T, h_prob_neg)\n",
        "\n",
        "        batch_size = v.shape[0]\n",
        "        self.W += self.learning_rate * (positive_grad - negative_grad) / batch_size\n",
        "        self.v_bias += self.learning_rate * np.mean(v - v_neg, axis=0)\n",
        "        self.h_bias += self.learning_rate * np.mean(h_prob_0 - h_prob_neg, axis=0)\n",
        "\n",
        "        return v_neg\n",
        "\n",
        "    def train(self, data, epochs=10, k=1, batch_size=64):\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(data)\n",
        "            epoch_loss = 0\n",
        "            for batch_start in range(0, len(data), batch_size):\n",
        "                batch = data[batch_start:batch_start + batch_size]\n",
        "                v_neg = self.contrastive_divergence(batch, k)\n",
        "                epoch_loss += np.mean((batch - v_neg) ** 2)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(data):.5f}\")\n",
        "\n",
        "    def generate_samples(self, n_samples=10, k=1):\n",
        "        samples = []\n",
        "        for i in range(n_samples):\n",
        "            sample = np.random.binomial(1, 0.5, self.n_hidden)\n",
        "            sample = self.gibbs_sampling(sample, k)\n",
        "            samples.append(sample)\n",
        "        return np.array(samples)"
      ],
      "metadata": {
        "id": "g6q689osK99h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the simulations in phase 1 we define the number of hidden layers and k as follow:"
      ],
      "metadata": {
        "id": "W14bINYQFBNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_visible = X_train_binary.shape[1]\n",
        "n_hidden = 32\n",
        "k_values = [1, 5, 10]"
      ],
      "metadata": {
        "id": "xPvmqTsUgXxU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train the model on the MNIST dataset:"
      ],
      "metadata": {
        "id": "VMdo3cSGLbxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "rbms = {}\n",
        "for k in k_values:\n",
        "    print(f\"Training RBM with k={k}\")\n",
        "    rbm = RBM(n_visible, n_hidden)\n",
        "    rbm.train(X_train_binary, epochs=10, k=k)\n",
        "    rbms[k] = rbm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLmlsavtLPnq",
        "outputId": "468761ec-c9b1-4af3-e9a1-339ee8f8bd46"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RBM with k=1\n",
            "Epoch 1/10, Loss: 0.00198\n",
            "Epoch 2/10, Loss: 0.00160\n",
            "Epoch 3/10, Loss: 0.00152\n",
            "Epoch 4/10, Loss: 0.00148\n",
            "Epoch 5/10, Loss: 0.00146\n",
            "Epoch 6/10, Loss: 0.00145\n",
            "Epoch 7/10, Loss: 0.00144\n",
            "Epoch 8/10, Loss: 0.00143\n",
            "Epoch 9/10, Loss: 0.00143\n",
            "Epoch 10/10, Loss: 0.00142\n",
            "Training RBM with k=5\n",
            "Epoch 1/10, Loss: 0.00221\n",
            "Epoch 2/10, Loss: 0.00174\n",
            "Epoch 3/10, Loss: 0.00163\n",
            "Epoch 4/10, Loss: 0.00158\n",
            "Epoch 5/10, Loss: 0.00154\n",
            "Epoch 6/10, Loss: 0.00152\n",
            "Epoch 7/10, Loss: 0.00151\n",
            "Epoch 8/10, Loss: 0.00150\n",
            "Epoch 9/10, Loss: 0.00149\n",
            "Epoch 10/10, Loss: 0.00148\n",
            "Training RBM with k=10\n",
            "Epoch 1/10, Loss: 0.00229\n",
            "Epoch 2/10, Loss: 0.00180\n",
            "Epoch 3/10, Loss: 0.00170\n",
            "Epoch 4/10, Loss: 0.00164\n",
            "Epoch 5/10, Loss: 0.00160\n",
            "Epoch 6/10, Loss: 0.00157\n",
            "Epoch 7/10, Loss: 0.00156\n",
            "Epoch 8/10, Loss: 0.00154\n",
            "Epoch 9/10, Loss: 0.00153\n",
            "Epoch 10/10, Loss: 0.00153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model is converging and the loss is getting lower to a fixed point.\n",
        "\n",
        "As we increase k, the final loss of the model on the training data is not getting better but based on the results of the first phase, we know that the model with a higher k performs better on the test datas and gives better resuls."
      ],
      "metadata": {
        "id": "FuUFNaePFT_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test these models, we use the test dataset of the MNIST:"
      ],
      "metadata": {
        "id": "TUukKdxeLx7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rbm(rbm, data):\n",
        "    loss = 0\n",
        "    for i in range(0, len(data), 64):\n",
        "        batch = data[i:i+64]\n",
        "        v_neg = rbm.contrastive_divergence(batch, k=1)\n",
        "        loss += np.mean((batch - v_neg) ** 2)\n",
        "    avg_loss = loss / (len(data) / 64)\n",
        "    return avg_loss\n",
        "\n",
        "simple_losses = {}\n",
        "for k in k_values:\n",
        "    loss = evaluate_rbm(rbms[k], X_test_binary)\n",
        "    simple_losses[k] = loss\n",
        "    print(f\"Simple RBM with k={k} Test Loss: {loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTs_QhhmL27i",
        "outputId": "95c904d1-30fe-4b64-a58c-86871f460c77"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple RBM with k=1 Test Loss: 0.08991\n",
            "Simple RBM with k=5 Test Loss: 0.08803\n",
            "Simple RBM with k=10 Test Loss: 0.08726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test loss is getting better by increasing k. This confirms the results discussed in the phase 1."
      ],
      "metadata": {
        "id": "tuADL8LgGBVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Secure Boltzman Machine:"
      ],
      "metadata": {
        "id": "lrbyHJ_mLsIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we learned from the project file, the Secure Boltzmann Machine works with two parties. Each party have a specific number of parameters and each have they're own weights and biases.\n",
        "\n",
        "To update the parameters, we use ElGamal encryption so that both models can calculate loss with needed values (e.g. $v_1h_2$). To implement this, we approach like the model below:"
      ],
      "metadata": {
        "id": "8IELhEF7GPEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SecureRBM:\n",
        "    def __init__(self, n_visible, n_hidden, mA, learning_rate=0.1, p=467):\n",
        "        \"\"\"\n",
        "        Initialize SecureRBM for two parties A and B.\n",
        "\n",
        "        Parameters:\n",
        "        - n_visible: total number of visible neurons (e.g., 784 for MNIST).\n",
        "        - n_hidden: number of hidden neurons.\n",
        "        - mA: number of visible features belonging to Party A.\n",
        "          (Then mB = n_visible - mA belong to Party B.)\n",
        "        - learning_rate: learning rate for weight updates.\n",
        "        - p: a prime number used for the ElGamal-based secure operations.\n",
        "        \"\"\"\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.mA = mA\n",
        "        self.mB = n_visible - mA\n",
        "        self.learning_rate = learning_rate\n",
        "        self.W = np.random.normal(0, 0.1, (n_visible, n_hidden))\n",
        "        self.v_bias = np.zeros(n_visible)\n",
        "        self.h_bias = np.zeros(n_hidden)\n",
        "        self.p = p\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (np.random.uniform(size=prob.shape) < prob).astype(np.int32)\n",
        "\n",
        "    def generate_keys(self):\n",
        "        \"\"\"\n",
        "        Generate ElGamal public and private keys.\n",
        "        \"\"\"\n",
        "        g = random.randint(2, self.p - 2)\n",
        "        x = random.randint(1, self.p - 2)\n",
        "        y = pow(g, x, self.p)\n",
        "        return (self.p, g, y), x\n",
        "\n",
        "    def encrypt(self, public_key, message):\n",
        "        \"\"\"\n",
        "        Encrypt a message (an integer) using the public key.\n",
        "        \"\"\"\n",
        "        p, g, y = public_key\n",
        "        k = random.randint(1, p - 2)\n",
        "        c1 = pow(g, k, p)\n",
        "        c2 = (int(message) * pow(y, k, p)) % p\n",
        "        return (c1, c2)\n",
        "\n",
        "    def decrypt_partial(self, private_key, public_key, ciphertext):\n",
        "        \"\"\"\n",
        "        Partially decrypt a ciphertext using the private key.\n",
        "        \"\"\"\n",
        "        p, g, y = public_key\n",
        "        c1, c2 = ciphertext\n",
        "        s = pow(c1, private_key, p)\n",
        "        return (c2, s)\n",
        "\n",
        "    def decrypt_final(self, partially_decrypted, p):\n",
        "        \"\"\"\n",
        "        Final decryption using the modular inverse.\n",
        "        \"\"\"\n",
        "        c2, s = partially_decrypted\n",
        "        s_inv = mod_inverse(s, p)\n",
        "        message = (c2 * s_inv) % p\n",
        "        return message\n",
        "\n",
        "    def sample_hidden_secure(self, v, public_key_a, private_key_a, public_key_b, private_key_b, random_r):\n",
        "        \"\"\"\n",
        "        Securely sample the hidden layer using contributions from both parties.\n",
        "\n",
        "        Parameters:\n",
        "        - v: input visible data (batch, n_visible)\n",
        "        - public_key_a, private_key_a: keys for Party A\n",
        "        - public_key_b, private_key_b: keys for Party B\n",
        "        - random_r: random mask array (batch, n_hidden)\n",
        "\n",
        "        Returns:\n",
        "        - Binary samples for hidden layer (batch, n_hidden)\n",
        "        \"\"\"\n",
        "        # Split visible input: Party A's part and Party B's part.\n",
        "        vA = v[:, :self.mA]\n",
        "        vB = v[:, self.mA:]\n",
        "\n",
        "        # Split the weight matrix W accordingly.\n",
        "        WA = self.W[:self.mA, :]\n",
        "        WB = self.W[self.mA:, :]\n",
        "\n",
        "        # Each party computes its partial activation. we assume all the bias is on party A's side\n",
        "        act_A = np.dot(vA, WA) + self.h_bias\n",
        "        act_B = np.dot(vB, WB)\n",
        "\n",
        "        # Joint secure activation: each party encrypts its share, then they are combined.\n",
        "        # Here we simulate secure computation by simply summing the parts.\n",
        "        joint_act = act_A + act_B\n",
        "        # Apply random mask removal.\n",
        "        final_act = joint_act - random_r\n",
        "        # Compute secure sigmoid and sample hidden units.\n",
        "        h_prob = self.sigmoid(final_act)\n",
        "        return self.sample(h_prob)\n",
        "\n",
        "    def sample_visible_secure(self, h, public_key_a, private_key_a, public_key_b, private_key_b, random_r):\n",
        "        \"\"\"\n",
        "        Securely sample the visible layer.\n",
        "\n",
        "        Parameters:\n",
        "        - h: hidden layer activations (batch, n_hidden)\n",
        "        - public_key_a, private_key_a: keys for Party A\n",
        "        - public_key_b, private_key_b: keys for Party B\n",
        "        - random_r: random mask array (batch, n_visible)\n",
        "\n",
        "        Returns:\n",
        "        - Binary reconstruction of the visible layer (batch, n_visible)\n",
        "        \"\"\"\n",
        "        # For the visible layer, assume Party A owns the first mA visible units and Party B the rest.\n",
        "        WA_T = self.W[:self.mA, :].T\n",
        "        WB_T = self.W[self.mA:, :].T\n",
        "        bA = self.v_bias[:self.mA]\n",
        "        bB = self.v_bias[self.mA:]\n",
        "\n",
        "        act_A = np.dot(h, WA_T) + bA\n",
        "        act_B = np.dot(h, WB_T) + bB\n",
        "\n",
        "        # Combine secure activations\n",
        "        joint_act = np.concatenate([act_A, act_B], axis=1)\n",
        "        final_act = joint_act - random_r\n",
        "        v_prob = self.sigmoid(final_act)\n",
        "        return self.sample(v_prob)\n",
        "\n",
        "    def reconstruct(self, v, public_key_a, private_key_a, public_key_b, private_key_b, random_r):\n",
        "        \"\"\"\n",
        "        Securely reconstruct the visible input.\n",
        "        \"\"\"\n",
        "        h = self.sample_hidden_secure(v, public_key_a, private_key_a, public_key_b, private_key_b, random_r[:, :self.h_bias.shape[0]])\n",
        "        v_recon = self.sample_visible_secure(h, public_key_a, private_key_a, public_key_b, private_key_b, random_r)\n",
        "        return v_recon\n",
        "\n",
        "    def contrastive_divergence_secure(self, v, k, public_key_a, private_key_a, public_key_b, private_key_b, random_r):\n",
        "        \"\"\"\n",
        "        Perform one step of secure Contrastive Divergence.\n",
        "        \"\"\"\n",
        "        # For simplicity, we perform ordinary Gibbs sampling.\n",
        "        # In a real implementation, secure operations would be applied at each step.\n",
        "        v_neg = v.copy()\n",
        "        for _ in range(k):\n",
        "            h_prob = self.sigmoid(np.dot(v_neg, self.W) + self.h_bias)\n",
        "            h_sample = self.sample(h_prob)\n",
        "            v_prob = self.sigmoid(np.dot(h_sample, self.W.T) + self.v_bias)\n",
        "            v_neg = self.sample(v_prob)\n",
        "        # Compute gradients using standard (non-secure) operations.\n",
        "        h_prob_data = self.sigmoid(np.dot(v, self.W) + self.h_bias)\n",
        "        h_prob_neg = self.sigmoid(np.dot(v_neg, self.W) + self.h_bias)\n",
        "        positive_grad = np.dot(v.T, h_prob_data)\n",
        "        negative_grad = np.dot(v_neg.T, h_prob_neg)\n",
        "        batch_size = v.shape[0]\n",
        "        self.W += self.learning_rate * (positive_grad - negative_grad) / batch_size\n",
        "        self.v_bias += self.learning_rate * np.mean(v - v_neg, axis=0)\n",
        "        self.h_bias += self.learning_rate * np.mean(h_prob_data - h_prob_neg, axis=0)\n",
        "        return v_neg\n",
        "\n",
        "    def train_secure(self, data, epochs=10, k=1, batch_size=64,\n",
        "                     public_key_a=None, private_key_a=None,\n",
        "                     public_key_b=None, private_key_b=None):\n",
        "        \"\"\"\n",
        "        Train the SecureRBM model using secure Contrastive Divergence.\n",
        "        \"\"\"\n",
        "        num_batches = len(data) // batch_size\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(data)\n",
        "            epoch_loss = 0\n",
        "            for batch_start in range(0, len(data), batch_size):\n",
        "                batch = data[batch_start:batch_start + batch_size]\n",
        "                # Generate a random mask for both hidden and visible layers.\n",
        "                # For visible: shape (batch, n_visible)\n",
        "                mask_scale = 0.1  # or another small constant\n",
        "                random_r = np.random.randint(1, int(self.p * mask_scale), size=(batch.shape[0], self.n_visible))\n",
        "                # Reconstruct the batch using secure operations.\n",
        "                v_recon = self.reconstruct(batch, public_key_a, private_key_a, public_key_b, private_key_b, random_r)\n",
        "                loss = np.mean((batch - v_recon) ** 2)\n",
        "                epoch_loss += loss\n",
        "                # Update parameters using secure Contrastive Divergence simulation.\n",
        "                self.contrastive_divergence_secure(batch, k, public_key_a, private_key_a, public_key_b, private_key_b, random_r)\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Secure Loss: {avg_loss:.5f}\")\n",
        "\n",
        "    def generate_samples(self, n_samples=10, k=1,\n",
        "                         public_key_a=None, private_key_a=None,\n",
        "                         public_key_b=None, private_key_b=None):\n",
        "        \"\"\"\n",
        "        Generate new samples from the trained SecureRBM model.\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        for _ in range(n_samples):\n",
        "            # Start with a random visible vector.\n",
        "            v = np.random.binomial(1, 0.5, self.n_visible).reshape(1, -1)\n",
        "            mask_scale = 0.1\n",
        "            random_r = np.random.randint(1, int(self.p * mask_scale), size=(1, self.n_visible))\n",
        "            for _ in range(k):\n",
        "                h = self.sample_hidden_secure(v, public_key_a, private_key_a, public_key_b, private_key_b, random_r[:, :self.h_bias.shape[0]])\n",
        "                v = self.sample_visible_secure(h, public_key_a, private_key_a, public_key_b, private_key_b, random_r)\n",
        "            samples.append(v.flatten())\n",
        "        return np.array(samples)\n"
      ],
      "metadata": {
        "id": "BLfmlrpDwl2y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD**"
      ],
      "metadata": {
        "id": "brL4L2VlJmV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Secure Model:"
      ],
      "metadata": {
        "id": "Dm5pL5zOLlUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the parameters\n",
        "n_visible = 784\n",
        "n_hidden = 32\n",
        "mA = 392\n",
        "\n",
        "secure_rbm = SecureRBM(n_visible, n_hidden, mA, learning_rate=0.01, p=467)\n",
        "public_key_a, private_key_a = secure_rbm.generate_keys()\n",
        "public_key_b, private_key_b = secure_rbm.generate_keys()\n",
        "\n",
        "# train\n",
        "secure_rbm.train_secure(X_train_binary, epochs=15, k=5,\n",
        "                        public_key_a=public_key_a, private_key_a=private_key_a,\n",
        "                        public_key_b=public_key_b, private_key_b=private_key_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rDM79G0wojO",
        "outputId": "d800c90d-b950-4bf2-889d-3f69082eb277"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Secure Loss: 0.13478\n",
            "Epoch 2/15, Secure Loss: 0.13329\n",
            "Epoch 3/15, Secure Loss: 0.13287\n",
            "Epoch 4/15, Secure Loss: 0.13260\n",
            "Epoch 5/15, Secure Loss: 0.13239\n",
            "Epoch 6/15, Secure Loss: 0.13224\n",
            "Epoch 7/15, Secure Loss: 0.13212\n",
            "Epoch 8/15, Secure Loss: 0.13203\n",
            "Epoch 9/15, Secure Loss: 0.13192\n",
            "Epoch 10/15, Secure Loss: 0.13183\n",
            "Epoch 11/15, Secure Loss: 0.13177\n",
            "Epoch 12/15, Secure Loss: 0.13169\n",
            "Epoch 13/15, Secure Loss: 0.13163\n",
            "Epoch 14/15, Secure Loss: 0.13158\n",
            "Epoch 15/15, Secure Loss: 0.13153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the secure model:"
      ],
      "metadata": {
        "id": "yhRBFpDPL787"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ارزیابی ماشین بولتزمن امن\n",
        "def evaluate_secure_rbm(secure_rbm, data, public_key_a, private_key_a, public_key_b, private_key_b, batch_size=64):\n",
        "    loss = 0.0\n",
        "    num_batches = len(data) // batch_size\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        # ایجاد یک مقدار تصادفی r برای هر دسته\n",
        "        random_r = np.random.randint(1, secure_rbm.p, size=(batch.shape[0], 1))\n",
        "\n",
        "        # بازسازی داده‌ها با استفاده از متدهای امن\n",
        "        v_recon = secure_rbm.reconstruct(batch, public_key_a, private_key_a, public_key_b, private_key_b, random_r)\n",
        "\n",
        "        # محاسبه خطا بین داده‌های اصلی و بازسازی شده\n",
        "        loss += np.mean((batch - v_recon) ** 2)\n",
        "\n",
        "    avg_loss = loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "# ارزیابی ماشین بولتزمن امن\n",
        "secure_loss = evaluate_secure_rbm(\n",
        "    secure_rbm,\n",
        "    X_test_binary,\n",
        "    public_key_a,\n",
        "    private_key_a,\n",
        "    public_key_b,\n",
        "    private_key_b,\n",
        "    batch_size=64\n",
        ")\n",
        "print(f\"Secure RBM Test Loss: {secure_loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muv4Bq4uL7PW",
        "outputId": "efd826e3-d031-4fa4-ed65-37157ba102dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secure RBM Test Loss: 0.13333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison:"
      ],
      "metadata": {
        "id": "G1UpSjguMCJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Comparing the Simple RBM with \")\n",
        "print(f\"Simple RBM with k=1 Test Loss: {simple_losses[1]:.5f}\")\n",
        "print(f\"Simple RBM with k=5 Test Loss: {simple_losses[5]:.5f}\")\n",
        "print(f\"Simple RBM with k=10 Test Loss: {simple_losses[10]:.5f}\")\n",
        "print(f\"Secure RBM Test Loss: {secure_loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q6jIAtMMDI8",
        "outputId": "80edb9c0-56d3-4e88-b87b-b1f2ae6ec4a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "مقایسه عملکرد ماشین بولتزمن ساده و امن:\n",
            "Simple RBM with k=1 Test Loss: 0.09028\n",
            "Simple RBM with k=5 Test Loss: 0.08821\n",
            "Simple RBM with k=10 Test Loss: 0.08768\n",
            "Secure RBM Test Loss: 0.13333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u6sRAhTYHL0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3kZ5maPQHLss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7shky0hbHLkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UeskQDY_HLaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "65nmjePCHLRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative RBM"
      ],
      "metadata": {
        "id": "kncasc4xHK-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the data with labels:"
      ],
      "metadata": {
        "id": "PeLJlK9oZpmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Data Preparation\n",
        "# ---------------------------\n",
        "def one_hot(labels, num_classes=10):\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# One-hot encode the labels (10 classes)\n",
        "Y_train_oh = one_hot(Y_train, num_classes=10)\n",
        "Y_test_oh  = one_hot(Y_test, num_classes=10)\n",
        "\n",
        "# For training the joint model, we concatenate x and y.\n",
        "# The total visible vector length = 784 (image) + 10 (label) = 794.\n",
        "V_train = np.concatenate([X_train.reshape(X_train.shape[0], -1), Y_train_oh], axis=1)"
      ],
      "metadata": {
        "id": "HfzNVgPBZpW7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model:"
      ],
      "metadata": {
        "id": "pEyPa2574XfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerativeRBM:\n",
        "    def __init__(self, n_visible, n_hidden, n_labels, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        n_visible: تعداد ویژگی‌های x (مثلاً 784)\n",
        "        n_labels: تعداد ابعاد y (مثلاً 10)\n",
        "        n_hidden: تعداد واحدهای مخفی\n",
        "        تعداد کل واحدهای قابل مشاهده = n_visible + n_labels (مثلاً 794)\n",
        "        \"\"\"\n",
        "        self.n_visible = n_visible\n",
        "        self.n_labels = n_labels\n",
        "        self.nv_total = n_visible + n_labels\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # مقداردهی اولیه به وزن‌ها و بایاس‌ها\n",
        "        self.W = np.random.normal(0, 0.1, (self.nv_total, n_hidden))\n",
        "        self.b = np.zeros(self.nv_total)  # بایاس برای x و y\n",
        "        self.c = np.zeros(n_hidden)       # بایاس لایه پنهان\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (np.random.rand(*prob.shape) < prob).astype(np.float32)\n",
        "\n",
        "    def contrastive_divergence(self, v, k=1):\n",
        "        # مرحله مثبت: محاسبه احتمالات لایه پنهان\n",
        "        h_prob = self.sigmoid(np.dot(v, self.W) + self.c)\n",
        "        h_sample = self.sample(h_prob)\n",
        "\n",
        "        # مرحله منفی: اجرای k مرحله Gibbs Sampling\n",
        "        v_neg = v.copy()\n",
        "        for _ in range(k):\n",
        "            h_prob_neg = self.sigmoid(np.dot(v_neg, self.W) + self.c)\n",
        "            h_sample_neg = self.sample(h_prob_neg)\n",
        "            v_prob_neg = self.sigmoid(np.dot(h_sample_neg, self.W.T) + self.b)\n",
        "            v_neg = self.sample(v_prob_neg)\n",
        "        return v_neg, h_prob\n",
        "\n",
        "    def update(self, v, v_neg, h_prob, h_prob_neg):\n",
        "        batch_size = v.shape[0]\n",
        "        positive_grad = np.dot(v.T, h_prob)\n",
        "        negative_grad = np.dot(v_neg.T, h_prob_neg)\n",
        "        self.W += self.learning_rate * (positive_grad - negative_grad) / batch_size\n",
        "        self.b += self.learning_rate * np.mean(v - v_neg, axis=0)\n",
        "        self.c += self.learning_rate * np.mean(h_prob - h_prob_neg, axis=0)\n",
        "\n",
        "    def train(self, data, epochs=20, k=1, batch_size=64):\n",
        "        n_samples = data.shape[0]\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(data)\n",
        "            epoch_error = 0\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                batch = data[i:i+batch_size]\n",
        "                v_neg, h_prob = self.contrastive_divergence(batch, k)\n",
        "                h_prob_neg = self.sigmoid(np.dot(v_neg, self.W) + self.c)\n",
        "                self.update(batch, v_neg, h_prob, h_prob_neg)\n",
        "                epoch_error += np.mean((batch - v_neg)**2)\n",
        "            avg_err = epoch_error / (n_samples / batch_size)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Reconstruction Error: {avg_err:.5f}\")\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        vbias_term = np.dot(v, self.b)\n",
        "        wx_b = np.dot(v, self.W) + self.c\n",
        "        hidden_term = np.sum(np.log(1 + np.exp(wx_b)), axis=1)\n",
        "        return -vbias_term - hidden_term\n",
        "\n",
        "    def classify(self, x):\n",
        "        \"\"\"\n",
        "        برای هر ورودی x (شکل: (n_samples, n_visible))، با تولید\n",
        "        بردارهای هم‌افزایی با برچسب‌های ممکن (One-hot) و محاسبه تابع انرژی آزاد،\n",
        "        برچسبی که انرژی آن کمینه است انتخاب می‌شود.\n",
        "        \"\"\"\n",
        "        n_samples = x.shape[0]\n",
        "        predictions = np.zeros(n_samples, dtype=np.int32)\n",
        "        for i in range(n_samples):\n",
        "            x_i = x[i].reshape(1, -1)\n",
        "            energies = []\n",
        "            for label in range(self.n_labels):\n",
        "                y_candidate = np.zeros((1, self.n_labels))\n",
        "                y_candidate[0, label] = 1\n",
        "                v_candidate = np.concatenate([x_i, y_candidate], axis=1)\n",
        "                energies.append(self.free_energy(v_candidate)[0])\n",
        "            predictions[i] = np.argmin(energies)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "v6ihNwNJD-IO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model:"
      ],
      "metadata": {
        "id": "w2YY4nfYZDWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# آموزش و ارزیابی مدل Generative RBM\n",
        "# ---------------------------\n",
        "n_visible = 784\n",
        "n_labels = 10\n",
        "n_hidden = 256\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "k = 1\n",
        "batch_size = 64\n",
        "\n",
        "rbm_joint = GenerativeRBM(n_visible, n_hidden, n_labels, learning_rate)\n",
        "\n",
        "print(\"Training Generative RBM on (x,y)...\")\n",
        "rbm_joint.train(V_train, epochs=epochs, k=k, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnnABfnEZC_4",
        "outputId": "ea184539-58e4-4ce7-b14a-0d4b8b3a5987"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Generative RBM on (x,y)...\n",
            "Epoch 1/10, Reconstruction Error: 0.07785\n",
            "Epoch 2/10, Reconstruction Error: 0.05614\n",
            "Epoch 3/10, Reconstruction Error: 0.04983\n",
            "Epoch 4/10, Reconstruction Error: 0.04602\n",
            "Epoch 5/10, Reconstruction Error: 0.04339\n",
            "Epoch 6/10, Reconstruction Error: 0.04139\n",
            "Epoch 7/10, Reconstruction Error: 0.03983\n",
            "Epoch 8/10, Reconstruction Error: 0.03864\n",
            "Epoch 9/10, Reconstruction Error: 0.03760\n",
            "Epoch 10/10, Reconstruction Error: 0.03674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model:"
      ],
      "metadata": {
        "id": "vejnYcIWY8W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_classification_results(x, true_labels, pred_labels, n=10):\n",
        "    fig, axes = plt.subplots(1, n, figsize=(15, 3))\n",
        "    for i, ax in enumerate(axes):\n",
        "        ax.imshow(x[i].reshape(28,28), cmap='binary')\n",
        "        ax.set_title(f\"True: {true_labels[i]}\\nPred: {pred_labels[i]}\")\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "predictions_joint = rbm_joint.classify(X_test)\n",
        "accuracy_joint = np.mean(predictions_joint == Y_test)\n",
        "print(f\"Generative RBM Classification Accuracy: {accuracy_joint*100:.2f}%\")\n",
        "\n",
        "show_classification_results(X_test, Y_test, predictions_joint, n=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "3PXp7rKwY8zC",
        "outputId": "82bd1dcb-beb6-4c2c-99d2-1f82a16b909d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative RBM Classification Accuracy: 93.34%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full DRBM model"
      ],
      "metadata": {
        "id": "m8ZfG7dMqhzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the data:"
      ],
      "metadata": {
        "id": "WxFUGe204xtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# بارگذاری و پیش‌پردازش داده‌ها\n",
        "# ---------------------------\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader  = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "5asN7F9q4wlZ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model:"
      ],
      "metadata": {
        "id": "V3_Vu7uv45dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminativeRBMModel(nn.Module):\n",
        "    def __init__(self, input_dim=784, num_classes=10, hidden_dim=256):\n",
        "        super(DiscriminativeRBMModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # پارامترهای مربوط به بخش تصویر\n",
        "        self.b = nn.Parameter(torch.zeros(input_dim))\n",
        "        # پارامترهای مربوط به برچسب\n",
        "        self.d = nn.Parameter(torch.zeros(num_classes))\n",
        "        # وزن‌های مربوط به اتصال تصویر به لایه پنهان\n",
        "        self.W = nn.Parameter(torch.randn(input_dim, hidden_dim) * 0.1)\n",
        "        # وزن‌های مربوط به اتصال برچسب به لایه پنهان\n",
        "        self.U = nn.Parameter(torch.randn(num_classes, hidden_dim) * 0.1)\n",
        "        # بایاس لایه پنهان\n",
        "        self.c = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    def free_energy(self, x, y):\n",
        "        vbias_term = torch.matmul(x, self.b) + torch.matmul(y, self.d)\n",
        "        hidden_linear = torch.matmul(x, self.W) + torch.matmul(y, self.U) + self.c\n",
        "        # Clamp کردن ورودی به exp برای جلوگیری از overflow\n",
        "        hidden_linear = torch.clamp(hidden_linear, max=50)\n",
        "        hidden_term = torch.sum(torch.log(1 + torch.exp(hidden_linear)), dim=1)\n",
        "        return -vbias_term - hidden_term\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        برای هر نمونه ورودی x (به شکل (batch, 784))، با استفاده از تابع free energy،\n",
        "        برای هر برچسب ممکن (به صورت One-hot) مقادیر -F(x,y) (که معادل logits هستند) محاسبه می‌شود.\n",
        "        خروجی logits به شکل (batch, num_classes) خواهد بود.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        logits = []\n",
        "        for label in range(self.num_classes):\n",
        "            y_candidate = torch.zeros(batch_size, self.num_classes, device=x.device)\n",
        "            y_candidate[:, label] = 1.0\n",
        "            F = self.free_energy(x, y_candidate)   # (batch,)\n",
        "            logits.append((-F).unsqueeze(1))\n",
        "        logits = torch.cat(logits, dim=1)  # (batch, num_classes)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "iL2eC6LrqnM-"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model:"
      ],
      "metadata": {
        "id": "TvKnKXhpq0Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# آموزش مدل تمایزگر\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DiscriminativeRBMModel(input_dim=784, num_classes=10, hidden_dim=256).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "num_epochs = 10\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # بایناریزه کردن تصاویر (اگر لازم باشد)\n",
        "        images = (images > 0.5).float()\n",
        "        images = images.view(images.size(0), -1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)  # (batch, num_classes)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(trainset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgnpYIsfq209",
        "outputId": "cb71561f-5a41-45a2-edee-aad32215abe2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.23599\n",
            "Epoch 2/10, Loss: 0.10353\n",
            "Epoch 3/10, Loss: 0.07536\n",
            "Epoch 4/10, Loss: 0.05815\n",
            "Epoch 5/10, Loss: 0.05386\n",
            "Epoch 6/10, Loss: 0.04407\n",
            "Epoch 7/10, Loss: 0.04030\n",
            "Epoch 8/10, Loss: 0.03591\n",
            "Epoch 9/10, Loss: 0.03353\n",
            "Epoch 10/10, Loss: 0.03283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model:"
      ],
      "metadata": {
        "id": "tOmSq1jHqth_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# ارزیابی مدل تمایزگر\n",
        "# ---------------------------\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        images = (images > 0.5).float()\n",
        "        images = images.view(images.size(0), -1)\n",
        "        logits = model(images)\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Discriminative Model Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv84TXVmqwZ5",
        "outputId": "529c198d-a095-4872-9bb7-17f76ced03dd"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminative Model Accuracy: 96.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fgCnmxCnnAtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_lDZKtcSnAf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r9n4kxgSnARO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GwotZx-EnAL-"
      }
    }
  ]
}