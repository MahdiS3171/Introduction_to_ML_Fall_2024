{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Machine Learning\n",
        "## Project Phase 2\n",
        "\n",
        "Sina Fathi | 402111261\n",
        "\n",
        "Seyyed Amirmahdi Sadrzadeh | 401102015"
      ],
      "metadata": {
        "id": "hniNtD-Zew1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Data Preproccessing"
      ],
      "metadata": {
        "id": "tvacMiiXb_2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we import the needed libraries:"
      ],
      "metadata": {
        "id": "8ziJ9zUdcH_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from sympy import mod_inverse\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "metadata": {
        "id": "d3lKKdgBGJQS"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use torch library to load the MNIST dataset and then turn them to numpy arrays:"
      ],
      "metadata": {
        "id": "p_vzRIfdcMhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "x_data = mnist.data.to_numpy()\n",
        "y_data = mnist.target.to_numpy().astype(int)"
      ],
      "metadata": {
        "id": "AyG6f5oKGPHh"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We devide the data into test and train datas an also, use the datas, we binarize them by setting a threshold equal to 128. So all pixels with values higher than 128 will be set to 1 and others will be set to 0. We also flatten the datas to use them in our model:"
      ],
      "metadata": {
        "id": "vyLf42oLclGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = (x_data > 127).astype(np.float32)\n",
        "x_train, x_test = x_data[:60000], x_data[60000:]\n",
        "y_train, y_test = y_data[:60000], y_data[60000:]\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)"
      ],
      "metadata": {
        "id": "H1nb8nHOKoXd"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the labels one-hot:"
      ],
      "metadata": {
        "id": "C4I1exuy1-bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "y_train = ohe.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test = ohe.transform(y_test.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "3-cyphvA19Jh"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot to show the samples:"
      ],
      "metadata": {
        "id": "_hNNKcVhdJL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_samples(samples, nrows=2, ncols=5):\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(10, 5))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(samples[i].reshape(28, 28), cmap='binary')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wlGlqEJ7Kt7M"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we use the above function to show 10 samples from the dataset:"
      ],
      "metadata": {
        "id": "DptyrqJwdPLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_samples(X_train_binary[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "kr0tRRKyKzrV",
        "outputId": "d52e6709-c904-44ac-d9cb-068b0d685aa2"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Boltzman machine:"
      ],
      "metadata": {
        "id": "b0EKwQygdlPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Boltzmann machine is like the one we used in the first phase of the project and follow the same patterns to update the parameters."
      ],
      "metadata": {
        "id": "8rd8PBL8E2LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RBM:\n",
        "    def __init__(self, n_visible, n_hidden, learning_rate=0.1):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.W = np.random.normal(0, 0.1, (n_visible, n_hidden))\n",
        "        self.v_bias = np.zeros(n_visible)\n",
        "        self.h_bias = np.zeros(n_hidden)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (np.random.uniform(size=prob.shape) < prob).astype(np.int32)\n",
        "\n",
        "    def gibbs_sampling(self, hid, k):\n",
        "        v_prob_0 = self.sigmoid(np.dot(hid, self.W.T) + self.v_bias)\n",
        "        v = self.sample(v_prob_0)\n",
        "        for _ in range(k):\n",
        "            h_prob = self.sigmoid(np.dot(v, self.W) + self.h_bias)\n",
        "            h_sample = self.sample(h_prob)\n",
        "            v_prob = self.sigmoid(np.dot(h_sample, self.W.T) + self.v_bias)\n",
        "            v = self.sample(v_prob)\n",
        "        return v\n",
        "\n",
        "    def contrastive_divergence(self, v, k):\n",
        "        h_prob_0 = self.sigmoid(np.dot(v, self.W) + self.h_bias)\n",
        "        h_sample_0 = self.sample(h_prob_0)\n",
        "\n",
        "        v_neg = v.copy()\n",
        "\n",
        "        for _ in range(k):\n",
        "            h_prob = self.sigmoid(np.dot(v_neg, self.W) + self.h_bias)\n",
        "            h_sample = self.sample(h_prob)\n",
        "            v_prob_neg = self.sigmoid(np.dot(h_sample, self.W.T) + self.v_bias)\n",
        "            v_neg = self.sample(v_prob_neg)\n",
        "\n",
        "        h_prob_neg = self.sigmoid(np.dot(v_neg, self.W) + self.h_bias)\n",
        "\n",
        "        positive_grad = np.dot(v.T, h_prob_0)\n",
        "        negative_grad = np.dot(v_neg.T, h_prob_neg)\n",
        "\n",
        "        batch_size = v.shape[0]\n",
        "        self.W += self.learning_rate * (positive_grad - negative_grad) / batch_size\n",
        "        self.v_bias += self.learning_rate * np.mean(v - v_neg, axis=0)\n",
        "        self.h_bias += self.learning_rate * np.mean(h_prob_0 - h_prob_neg, axis=0)\n",
        "\n",
        "        return v_neg\n",
        "\n",
        "    def train(self, data, epochs=10, k=1, batch_size=64):\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(data)\n",
        "            epoch_loss = 0\n",
        "            for batch_start in range(0, len(data), batch_size):\n",
        "                batch = data[batch_start:batch_start + batch_size]\n",
        "                v_neg = self.contrastive_divergence(batch, k)\n",
        "                epoch_loss += np.mean((batch - v_neg) ** 2)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(data):.5f}\")\n",
        "\n",
        "    def generate_samples(self, n_samples=10, k=1):\n",
        "        samples = []\n",
        "        for i in range(n_samples):\n",
        "            sample = np.random.binomial(1, 0.5, self.n_hidden)\n",
        "            sample = self.gibbs_sampling(sample, k)\n",
        "            samples.append(sample)\n",
        "        return np.array(samples)"
      ],
      "metadata": {
        "id": "g6q689osK99h"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the simulations in phase 1 we define the number of hidden layers and k as follow:"
      ],
      "metadata": {
        "id": "W14bINYQFBNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_visible = x_train.shape[1]\n",
        "n_hidden = 32\n",
        "k_values = [1, 5, 10]"
      ],
      "metadata": {
        "id": "xPvmqTsUgXxU"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train the model on the MNIST dataset:"
      ],
      "metadata": {
        "id": "VMdo3cSGLbxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "rbms = {}\n",
        "for k in k_values:\n",
        "    print(f\"Training RBM with k={k}\")\n",
        "    rbm = RBM(n_visible, n_hidden)\n",
        "    rbm.train(x_train, epochs=10, k=k)\n",
        "    rbms[k] = rbm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLmlsavtLPnq",
        "outputId": "c73aa0c6-d342-4721-89da-74c4947dd475"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RBM with k=1\n",
            "Epoch 1/10, Loss: 0.00199\n",
            "Epoch 2/10, Loss: 0.00160\n",
            "Epoch 3/10, Loss: 0.00152\n",
            "Epoch 4/10, Loss: 0.00149\n",
            "Epoch 5/10, Loss: 0.00147\n",
            "Epoch 6/10, Loss: 0.00146\n",
            "Epoch 7/10, Loss: 0.00145\n",
            "Epoch 8/10, Loss: 0.00144\n",
            "Epoch 9/10, Loss: 0.00144\n",
            "Epoch 10/10, Loss: 0.00144\n",
            "Training RBM with k=5\n",
            "Epoch 1/10, Loss: 0.00221\n",
            "Epoch 2/10, Loss: 0.00173\n",
            "Epoch 3/10, Loss: 0.00163\n",
            "Epoch 4/10, Loss: 0.00157\n",
            "Epoch 5/10, Loss: 0.00154\n",
            "Epoch 6/10, Loss: 0.00151\n",
            "Epoch 7/10, Loss: 0.00149\n",
            "Epoch 8/10, Loss: 0.00148\n",
            "Epoch 9/10, Loss: 0.00147\n",
            "Epoch 10/10, Loss: 0.00146\n",
            "Training RBM with k=10\n",
            "Epoch 1/10, Loss: 0.00231\n",
            "Epoch 2/10, Loss: 0.00182\n",
            "Epoch 3/10, Loss: 0.00171\n",
            "Epoch 4/10, Loss: 0.00165\n",
            "Epoch 5/10, Loss: 0.00161\n",
            "Epoch 6/10, Loss: 0.00159\n",
            "Epoch 7/10, Loss: 0.00157\n",
            "Epoch 8/10, Loss: 0.00156\n",
            "Epoch 9/10, Loss: 0.00155\n",
            "Epoch 10/10, Loss: 0.00154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model is converging and the loss is getting lower to a fixed point.\n",
        "\n",
        "As we increase k, the final loss of the model on the training data is not getting better but based on the results of the first phase, we know that the model with a higher k performs better on the test datas and gives better resuls."
      ],
      "metadata": {
        "id": "FuUFNaePFT_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test these models, we use the test dataset of the MNIST:"
      ],
      "metadata": {
        "id": "TUukKdxeLx7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rbm(rbm, data):\n",
        "    loss = 0\n",
        "    for i in range(0, len(data), 64):\n",
        "        batch = data[i:i+64]\n",
        "        v_neg = rbm.contrastive_divergence(batch, k=1)\n",
        "        loss += np.mean((batch - v_neg) ** 2)\n",
        "    avg_loss = loss / (len(data) / 64)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "nTs_QhhmL27i"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_losses = {}\n",
        "for k in k_values:\n",
        "    loss = evaluate_rbm(rbms[k], x_test)\n",
        "    simple_losses[k] = loss\n",
        "    print(f\"Simple RBM with k={k} Test Loss: {loss:.5f}\")"
      ],
      "metadata": {
        "id": "1m7k3Reu8bIc",
        "outputId": "6c7e26c7-a1cf-4af5-bda1-4146d9a65576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple RBM with k=1 Test Loss: 0.09112\n",
            "Simple RBM with k=5 Test Loss: 0.08764\n",
            "Simple RBM with k=10 Test Loss: 0.08872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test loss is getting better by increasing k. This confirms the results discussed in the phase 1."
      ],
      "metadata": {
        "id": "tuADL8LgGBVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Secure Boltzman Machine:"
      ],
      "metadata": {
        "id": "lrbyHJ_mLsIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we learned from the project file, the Secure Boltzmann Machine works with two parties. Each party have a specific number of parameters and each have they're own weights and biases.\n",
        "\n",
        "To update the parameters, we use ElGamal encryption so that both models can calculate loss with needed values (e.g. $v_1h_2$). To implement this, we approach like the model below:"
      ],
      "metadata": {
        "id": "8IELhEF7GPEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sympy import mod_inverse\n",
        "\n",
        "class SecureRBM:\n",
        "    def __init__(self, n_visible, n_hidden, mA, learning_rate=0.001, p=467):\n",
        "        \"\"\"\n",
        "        Initialize SecureRBM for two parties A and B.\n",
        "\n",
        "        Parameters:\n",
        "        - n_visible: total number of visible neurons (e.g., 784 for MNIST).\n",
        "        - n_hidden: number of hidden neurons.\n",
        "        - mA: number of visible features belonging to Party A.\n",
        "          (Then mB = n_visible - mA belong to Party B.)\n",
        "        - learning_rate: learning rate for weight updates.\n",
        "        - p: a prime number used for the ElGamal-based secure operations.\n",
        "        \"\"\"\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.mA = mA\n",
        "        self.mB = n_visible - mA\n",
        "        self.learning_rate = learning_rate\n",
        "        self.p = p\n",
        "\n",
        "        # Xavier initialization for weights (better convergence)\n",
        "        limit = np.sqrt(6 / (self.n_visible + self.n_hidden))\n",
        "        self.W = np.random.uniform(-limit, limit, (n_visible, n_hidden))\n",
        "        self.v_bias = np.zeros(n_visible)\n",
        "        self.h_bias = np.zeros(n_hidden)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -50, 50)))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (np.random.uniform(size=prob.shape) < prob).astype(np.int32)\n",
        "\n",
        "    def generate_keys(self):\n",
        "        \"\"\"\n",
        "        Generate ElGamal public and private keys.\n",
        "        \"\"\"\n",
        "        g = random.randint(2, self.p - 2)\n",
        "        x = random.randint(1, self.p - 2)\n",
        "        y = pow(g, x, self.p)\n",
        "        return (self.p, g, y), x\n",
        "\n",
        "    def encrypt(self, public_key, message):\n",
        "        \"\"\"\n",
        "        Encrypt a message (an integer) using the public key.\n",
        "        \"\"\"\n",
        "        p, g, y = public_key\n",
        "        k = random.randint(1, p - 2)\n",
        "        c1 = pow(g, k, p)\n",
        "        c2 = (int(message) * pow(y, k, p)) % p\n",
        "        return (c1, c2)\n",
        "\n",
        "    def decrypt_partial(self, private_key, public_key, ciphertext):\n",
        "        \"\"\"\n",
        "        Partially decrypt a ciphertext using the private key.\n",
        "        \"\"\"\n",
        "        p, g, y = public_key\n",
        "        c1, c2 = ciphertext\n",
        "        s = pow(c1, private_key, p)\n",
        "        return (c2, s)\n",
        "\n",
        "    def decrypt_final(self, partially_decrypted, p):\n",
        "        \"\"\"\n",
        "        Final decryption using the modular inverse.\n",
        "        \"\"\"\n",
        "        c2, s = partially_decrypted\n",
        "        s_inv = mod_inverse(s, p)\n",
        "        message = (c2 * s_inv) % p\n",
        "        return message\n",
        "\n",
        "    def sample_hidden_secure(self, v, public_key_a, private_key_a, public_key_b, private_key_b, random_r):\n",
        "        \"\"\"\n",
        "        Securely sample the hidden layer using contributions from both parties.\n",
        "\n",
        "        Parameters:\n",
        "        - v: input visible data (batch, n_visible)\n",
        "        - public_key_a, private_key_a: keys for Party A\n",
        "        - public_key_b, private_key_b: keys for Party B\n",
        "        - random_r: random mask array (batch, n_hidden)\n",
        "\n",
        "        Returns:\n",
        "        - Binary samples for hidden layer (batch, n_hidden)\n",
        "        \"\"\"\n",
        "        vA = v[:, :self.mA]\n",
        "        vB = v[:, self.mA:]\n",
        "\n",
        "        WA = self.W[:self.mA, :]\n",
        "        WB = self.W[self.mA:, :]\n",
        "\n",
        "        act_A = np.dot(vA, WA) + self.h_bias\n",
        "        act_B = np.dot(vB, WB)\n",
        "\n",
        "        joint_act = act_A + act_B\n",
        "        final_act = joint_act - random_r\n",
        "        h_prob = self.sigmoid(final_act)\n",
        "        return self.sample(h_prob)\n",
        "\n",
        "    def sample_visible_secure(self, h, public_key_a, private_key_a, public_key_b, private_key_b, random_r):\n",
        "        \"\"\"\n",
        "        Securely sample the visible layer.\n",
        "\n",
        "        Parameters:\n",
        "        - h: hidden layer activations (batch, n_hidden)\n",
        "        - public_key_a, private_key_a: keys for Party A\n",
        "        - public_key_b, private_key_b: keys for Party B\n",
        "        - random_r: random mask array (batch, n_visible)\n",
        "\n",
        "        Returns:\n",
        "        - Binary reconstruction of the visible layer (batch, n_visible)\n",
        "        \"\"\"\n",
        "        WA_T = self.W[:self.mA, :].T\n",
        "        WB_T = self.W[self.mA:, :].T\n",
        "        bA = self.v_bias[:self.mA]\n",
        "        bB = self.v_bias[self.mA:]\n",
        "\n",
        "        act_A = np.dot(h, WA_T) + bA\n",
        "        act_B = np.dot(h, WB_T) + bB\n",
        "\n",
        "        joint_act = np.concatenate([act_A, act_B], axis=1)\n",
        "        final_act = joint_act - random_r\n",
        "        v_prob = self.sigmoid(final_act)\n",
        "        return self.sample(v_prob)\n",
        "\n",
        "    def secure_compute(self, partial_activation, public_key, private_key):\n",
        "        result = np.zeros_like(partial_activation)\n",
        "        for i in range(partial_activation.shape[0]):\n",
        "            for j in range(partial_activation.shape[1]):\n",
        "                value = int(partial_activation[i, j])\n",
        "                ciphertext = self.encrypt(public_key, value)\n",
        "                dec = self.decrypt_final(self.decrypt_partial(private_key, public_key, ciphertext), self.p)\n",
        "                result[i, j] = dec\n",
        "        return result\n",
        "\n",
        "    def reconstruct(self, v, public_key_a, private_key_a, public_key_b, private_key_b, random_r_hidden, random_r_visible):\n",
        "        \"\"\"\n",
        "        Securely reconstruct the visible input using separate random masks for the hidden and visible layers.\n",
        "        \"\"\"\n",
        "        h = self.sample_hidden_secure(v, public_key_a, private_key_a, public_key_b, private_key_b, random_r_hidden)\n",
        "        v_recon = self.sample_visible_secure(h, public_key_a, private_key_a, public_key_b, private_key_b, random_r_visible)\n",
        "        return v_recon\n",
        "\n",
        "    def contrastive_divergence_secure(self, v, k, public_key_a, private_key_a, public_key_b, private_key_b):\n",
        "        \"\"\"\n",
        "        Perform secure Contrastive Divergence.\n",
        "        Generates separate random masks for hidden and visible layers.\n",
        "        \"\"\"\n",
        "        mask_scale = 0.1\n",
        "        batch_size = v.shape[0]\n",
        "        random_r_hidden = np.random.randint(1, int(self.p * mask_scale), size=(batch_size, self.n_hidden))\n",
        "        random_r_visible = np.random.randint(1, int(self.p * mask_scale), size=(batch_size, self.n_visible))\n",
        "\n",
        "        v_neg = v.copy()\n",
        "        for _ in range(k):\n",
        "            mA = self.mA\n",
        "            part_A = np.dot(v_neg[:, :mA], self.W[:mA, :])\n",
        "            part_B = np.dot(v_neg[:, mA:], self.W[mA:, :])\n",
        "\n",
        "            secure_A = self.secure_compute(part_A, public_key_a, private_key_a)\n",
        "            secure_B = self.secure_compute(part_B, public_key_b, private_key_b)\n",
        "\n",
        "            joint_act = secure_A + secure_B + self.h_bias\n",
        "            final_act = joint_act - random_r_hidden\n",
        "            h_prob = self.sigmoid(final_act)\n",
        "            h_sample = self.sample(h_prob)\n",
        "\n",
        "            v_prob = self.sigmoid(np.dot(h_sample, self.W.T) + self.v_bias)\n",
        "            v_neg = self.sample(v_prob)\n",
        "\n",
        "        positive_grad = np.dot(v.T, h_prob)\n",
        "        negative_grad = np.dot(v_neg.T, h_prob)\n",
        "\n",
        "        # Gradient clipping to avoid instability\n",
        "        grad_norm = np.linalg.norm(positive_grad - negative_grad)\n",
        "        if grad_norm > 5.0:\n",
        "            positive_grad = (positive_grad / grad_norm) * 5.0\n",
        "            negative_grad = (negative_grad / grad_norm) * 5.0\n",
        "\n",
        "        # Update weights\n",
        "        self.W += self.learning_rate * (positive_grad - negative_grad) / batch_size\n",
        "        self.v_bias += self.learning_rate * np.mean(v - v_neg, axis=0)\n",
        "        self.h_bias += self.learning_rate * np.mean(h_prob - h_prob, axis=0)\n",
        "        return v_neg\n",
        "\n",
        "    def train_secure(self, data, epochs=10, k=1, batch_size=64,\n",
        "                 public_key_a=None, private_key_a=None,\n",
        "                 public_key_b=None, private_key_b=None):\n",
        "        \"\"\"\n",
        "        Train the SecureRBM model using secure Contrastive Divergence.\n",
        "        \"\"\"\n",
        "        num_batches = len(data) // batch_size\n",
        "        mask_scale = 0.1\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(data)\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                batch = data[i:i+batch_size]\n",
        "                batch_size_local = batch.shape[0]\n",
        "                random_r_hidden = np.random.randint(1, int(self.p * mask_scale), size=(batch_size_local, self.n_hidden))\n",
        "                random_r_visible = np.random.randint(1, int(self.p * mask_scale), size=(batch_size_local, self.n_visible))\n",
        "                v_recon = self.reconstruct(batch, public_key_a, private_key_a, public_key_b, private_key_b, random_r_hidden, random_r_visible)\n",
        "                loss = np.mean((batch - v_recon) ** 2)\n",
        "                epoch_loss += loss\n",
        "                self.contrastive_divergence_secure(batch, k, public_key_a, private_key_a, public_key_b, private_key_b)\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Secure Loss: {avg_loss:.5f}\")\n",
        "\n",
        "    def generate_samples(self, n_samples=10, k=1,\n",
        "                         public_key_a=None, private_key_a=None,\n",
        "                         public_key_b=None, private_key_b=None):\n",
        "        \"\"\"\n",
        "        Generate new samples from the trained SecureRBM model.\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        for _ in range(n_samples):\n",
        "            v = np.random.binomial(1, 0.5, self.n_visible).reshape(1, -1)\n",
        "            mask_scale = 0.1\n",
        "            random_r = np.random.randint(1, int(self.p * mask_scale), size=(1, self.n_visible))\n",
        "            for _ in range(k):\n",
        "                h = self.sample_hidden_secure(v, public_key_a, private_key_a, public_key_b, private_key_b, random_r[:, :self.h_bias.shape[0]])\n",
        "                v = self.sample_visible_secure(h, public_key_a, private_key_a, public_key_b, private_key_b, random_r)\n",
        "            samples.append(v.flatten())\n",
        "        return np.array(samples)\n"
      ],
      "metadata": {
        "id": "BLfmlrpDwl2y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Secure Model:"
      ],
      "metadata": {
        "id": "Dm5pL5zOLlUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize the SecureRBM for both Party A and Party B\n",
        "n_visible = 784  # For MNIST (28x28)\n",
        "n_hidden = 256\n",
        "mA = 392  # Party A gets the first half of the visible data (for example)\n",
        "learning_rate = 0.001\n",
        "p = 467  # Example prime number for ElGamal\n",
        "\n",
        "# Initialize Party A and Party B's models\n",
        "secure_rbm_a = SecureRBM(n_visible, n_hidden, mA, learning_rate=learning_rate, p=p)\n",
        "secure_rbm_b = SecureRBM(n_visible, n_hidden, mA, learning_rate=learning_rate, p=p)\n",
        "\n",
        "# 2. Generate ElGamal public and private keys for both parties\n",
        "public_key_a, private_key_a = secure_rbm_a.generate_keys()\n",
        "public_key_b, private_key_b = secure_rbm_b.generate_keys()\n",
        "\n",
        "# 3. Train the SecureRBM model using the generated keys\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "k = 5  # Number of Gibbs sampling steps per update\n",
        "\n",
        "# Assuming you have the dataset `X_train_binary` ready, as explained earlier\n",
        "# Data should be divided between Party A and Party B\n",
        "X_train_party_a = X_train_binary[:, :mA]  # Party A's part of the visible data\n",
        "X_train_party_b = X_train_binary[:, mA:]  # Party B's part of the visible data\n",
        "\n",
        "# Combine data for secure training\n",
        "V_train = np.concatenate([X_train_party_a, X_train_party_b], axis=1)  # Party A and Party B share the data\n",
        "\n",
        "# 4. Train the model using the shared data, with secure Contrastive Divergence (CD)\n",
        "secure_rbm_a.train_secure(V_train, epochs=epochs, k=k, batch_size=batch_size,\n",
        "                          public_key_a=public_key_a, private_key_a=private_key_a,\n",
        "                          public_key_b=public_key_b, private_key_b=private_key_b)\n",
        "secure_rbm_b.train_secure(V_train, epochs=epochs, k=k, batch_size=batch_size,\n",
        "                          public_key_a=public_key_a, private_key_a=private_key_a,\n",
        "                          public_key_b=public_key_b, private_key_b=private_key_b)\n",
        "\n",
        "# 5. Once training is complete, generate samples or use the trained models for classification\n",
        "# Example for generating samples from Party A's model:\n",
        "samples_a = secure_rbm_a.generate_samples(n_samples=10, k=5,\n",
        "                                          public_key_a=public_key_a, private_key_a=private_key_a,\n",
        "                                          public_key_b=public_key_b, private_key_b=private_key_b)\n",
        "\n",
        "# Example for generating samples from Party B's model:\n",
        "samples_b = secure_rbm_b.generate_samples(n_samples=10, k=5,\n",
        "                                          public_key_a=public_key_a, private_key_a=private_key_a,\n",
        "                                          public_key_b=public_key_b, private_key_b=private_key_b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "0rDM79G0wojO",
        "outputId": "1b7e55b9-7b86-4851-ccee-68f354862658"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-dc0b5e88b04e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# 4. Train the model using the shared data, with secure Contrastive Divergence (CD)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m secure_rbm_a.train_secure(V_train, epochs=epochs, k=k, batch_size=batch_size,\n\u001b[0m\u001b[1;32m     31\u001b[0m                           \u001b[0mpublic_key_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpublic_key_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_key_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate_key_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                           public_key_b=public_key_b, private_key_b=private_key_b)\n",
            "\u001b[0;32m<ipython-input-93-300b15f18051>\u001b[0m in \u001b[0;36mtrain_secure\u001b[0;34m(self, data, epochs, k, batch_size, public_key_a, private_key_a, public_key_b, private_key_b)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv_recon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrastive_divergence_secure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_key_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_key_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_key_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_key_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}, Secure Loss: {avg_loss:.5f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-300b15f18051>\u001b[0m in \u001b[0;36mcontrastive_divergence_secure\u001b[0;34m(self, v, k, public_key_a, private_key_a, public_key_b, private_key_b)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0msecure_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecure_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_key_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_key_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0msecure_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecure_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_key_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_key_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mjoint_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecure_A\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msecure_B\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-300b15f18051>\u001b[0m in \u001b[0;36msecure_compute\u001b[0;34m(self, partial_activation, public_key, private_key)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_activation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mciphertext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencrypt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecrypt_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecrypt_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprivate_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mciphertext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-300b15f18051>\u001b[0m in \u001b[0;36mdecrypt_final\u001b[0;34m(self, partially_decrypted, p)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \"\"\"\n\u001b[1;32m     72\u001b[0m         \u001b[0mc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartially_decrypted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0ms_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_inverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms_inv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/core/intfunc.py\u001b[0m in \u001b[0;36mmod_inverse\u001b[0;34m(a, m)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0migcdex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/utilities/misc.py\u001b[0m in \u001b[0;36mas_int\u001b[0;34m(n, strict)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s is not an integer'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the secure model:"
      ],
      "metadata": {
        "id": "yhRBFpDPL787"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ارزیابی ماشین بولتزمن امن\n",
        "def evaluate_secure_rbm(secure_rbm, data, public_key_a, private_key_a, public_key_b, private_key_b, batch_size=64):\n",
        "    loss = 0.0\n",
        "    num_batches = len(data) // batch_size\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        # ایجاد یک مقدار تصادفی r برای هر دسته\n",
        "        random_r = np.random.randint(1, secure_rbm.p, size=(batch.shape[0], 1))\n",
        "\n",
        "        # بازسازی داده‌ها با استفاده از متدهای امن\n",
        "        v_recon = secure_rbm.reconstruct(batch, public_key_a, private_key_a, public_key_b, private_key_b, random_r)\n",
        "\n",
        "        # محاسبه خطا بین داده‌های اصلی و بازسازی شده\n",
        "        loss += np.mean((batch - v_recon) ** 2)\n",
        "\n",
        "    avg_loss = loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "# ارزیابی ماشین بولتزمن امن\n",
        "secure_loss = evaluate_secure_rbm(\n",
        "    secure_rbm,\n",
        "    X_test_binary,\n",
        "    public_key_a,\n",
        "    private_key_a,\n",
        "    public_key_b,\n",
        "    private_key_b,\n",
        "    batch_size=64\n",
        ")\n",
        "print(f\"Secure RBM Test Loss: {secure_loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muv4Bq4uL7PW",
        "outputId": "efd826e3-d031-4fa4-ed65-37157ba102dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secure RBM Test Loss: 0.13333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison:"
      ],
      "metadata": {
        "id": "G1UpSjguMCJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Comparing the Simple RBM with \")\n",
        "print(f\"Simple RBM with k=1 Test Loss: {simple_losses[1]:.5f}\")\n",
        "print(f\"Simple RBM with k=5 Test Loss: {simple_losses[5]:.5f}\")\n",
        "print(f\"Simple RBM with k=10 Test Loss: {simple_losses[10]:.5f}\")\n",
        "print(f\"Secure RBM Test Loss: {secure_loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q6jIAtMMDI8",
        "outputId": "80edb9c0-56d3-4e88-b87b-b1f2ae6ec4a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "مقایسه عملکرد ماشین بولتزمن ساده و امن:\n",
            "Simple RBM with k=1 Test Loss: 0.09028\n",
            "Simple RBM with k=5 Test Loss: 0.08821\n",
            "Simple RBM with k=10 Test Loss: 0.08768\n",
            "Secure RBM Test Loss: 0.13333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u6sRAhTYHL0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3kZ5maPQHLss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7shky0hbHLkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UeskQDY_HLaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "65nmjePCHLRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenerativeRBM"
      ],
      "metadata": {
        "id": "kncasc4xHK-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerativeRBM:\n",
        "    def __init__(self, n_visible, n_hidden, n_labels, learning_rate=0.01, epochs=20, batch_size=128):\n",
        "        \"\"\"\n",
        "        Initializes a Generative RBM with added label interactions.\n",
        "        \"\"\"\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_labels = n_labels\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W = np.random.randn(n_visible, n_hidden) * 0.01  # Input-Hidden Weights\n",
        "        self.U = np.random.randn(n_labels, n_hidden) * 0.01   # Label-Hidden Weights\n",
        "        self.b = np.zeros((1, n_visible))  # Bias for visible layer\n",
        "        self.c = np.zeros((1, n_hidden))   # ✅ Fix shape for broadcasting\n",
        "        self.d = np.zeros((1, n_labels))   # Bias for labels\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (prob > np.random.rand(*prob.shape)).astype(np.float32)\n",
        "\n",
        "    def contrastive_divergence(self, x_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Performs Contrastive Divergence (CD-1) to update weights.\n",
        "        Returns reconstructed x and y.\n",
        "        \"\"\"\n",
        "        batch_size = x_batch.shape[0]\n",
        "\n",
        "        # **Positive Phase**\n",
        "        h_prob = self.sigmoid(np.dot(x_batch, self.W) + np.dot(y_batch, self.U) + self.c)\n",
        "        h_sample = self.sample(h_prob)\n",
        "\n",
        "        # **Negative Phase (Reconstruction)**\n",
        "        x_recon_prob = self.sigmoid(np.dot(h_sample, self.W.T) + self.b)\n",
        "        x_recon_sample = self.sample(x_recon_prob)\n",
        "\n",
        "        y_recon_prob = self.sigmoid(np.dot(h_sample, self.U.T) + self.d)\n",
        "        y_recon_sample = self.sample(y_recon_prob)\n",
        "\n",
        "        h_recon_prob = self.sigmoid(np.dot(x_recon_sample, self.W) + np.dot(y_recon_sample, self.U) + self.c)\n",
        "\n",
        "        # **Update weights**\n",
        "        self.W += self.learning_rate * ((np.dot(x_batch.T, h_prob) - np.dot(x_recon_sample.T, h_recon_prob)) / batch_size)\n",
        "        self.U += self.learning_rate * ((np.dot(y_batch.T, h_prob) - np.dot(y_recon_sample.T, h_recon_prob)) / batch_size)\n",
        "        self.b += self.learning_rate * np.mean(x_batch - x_recon_sample, axis=0)\n",
        "        self.c += self.learning_rate * np.mean(h_prob - h_recon_prob, axis=0)\n",
        "        self.d += self.learning_rate * np.mean(y_batch - y_recon_sample, axis=0)\n",
        "\n",
        "        return x_recon_sample, y_recon_sample  # Return reconstructed inputs\n",
        "\n",
        "\n",
        "    def train(self, X_train, Y_train):\n",
        "        \"\"\"\n",
        "        Trains the RBM using Contrastive Divergence and prints the loss.\n",
        "        \"\"\"\n",
        "        print(\"Starting training...\")\n",
        "        for epoch in range(self.epochs):\n",
        "            epoch_loss = 0  # Track total loss per epoch\n",
        "            num_batches = X_train.shape[0] // self.batch_size\n",
        "\n",
        "            for i in range(0, X_train.shape[0], self.batch_size):\n",
        "                x_batch = X_train[i:i + self.batch_size]\n",
        "                y_batch = Y_train[i:i + self.batch_size]\n",
        "\n",
        "                # Ensure y_batch has the correct shape\n",
        "                if y_batch.shape[1] != self.n_labels:\n",
        "                    y_batch = y_batch.T  # Ensure shape (batch_size, n_labels)\n",
        "\n",
        "                # **Perform Contrastive Divergence**\n",
        "                x_recon, y_recon = self.contrastive_divergence(x_batch, y_batch)\n",
        "\n",
        "                # **Compute Reconstruction Loss (Mean Squared Error)**\n",
        "                loss_x = np.mean((x_batch - x_recon) ** 2)  # MSE loss for X\n",
        "                loss_y = np.mean((y_batch - y_recon) ** 2)  # MSE loss for Y\n",
        "                batch_loss = (loss_x + loss_y) / 2  # Average loss\n",
        "\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            # **Print Average Loss for Epoch**\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.5f}\")\n",
        "\n",
        "\n",
        "    def classify(self, X_test, Y_test):\n",
        "        \"\"\"\n",
        "        Classifies input data using the trained RBM.\n",
        "        \"\"\"\n",
        "        h_test = self.sigmoid(np.dot(X_test, self.W) + np.dot(Y_test, self.U) + self.c.reshape(1, -1))\n",
        "        y_pred = np.argmax(np.dot(h_test, self.U.T) + self.d, axis=1)\n",
        "        y_true = np.argmax(Y_test, axis=1)\n",
        "\n",
        "        accuracy = np.mean(y_pred == y_true)\n",
        "        print(f\"Classification accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "v6ihNwNJD-IO"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model:"
      ],
      "metadata": {
        "id": "w2YY4nfYZDWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_visible = 784\n",
        "n_labels = 10\n",
        "n_hidden = 256\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "k = 5\n",
        "batch_size = 64\n",
        "\n",
        "# **Initialize and Train RBM**\n",
        "rbm = GenerativeRBM(n_visible, n_hidden, n_labels, learning_rate, epochs=10, batch_size=128)\n",
        "rbm.train(x_train, y_train)\n",
        "\n",
        "# **Evaluate Accuracy**\n",
        "rbm.classify(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnnABfnEZC_4",
        "outputId": "5c09ae3b-ee5f-4c6e-b22a-0fbf722c02b0"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/10, Loss: 0.10358\n",
            "Epoch 2/10, Loss: 0.06196\n",
            "Epoch 3/10, Loss: 0.05279\n",
            "Epoch 4/10, Loss: 0.04770\n",
            "Epoch 5/10, Loss: 0.04451\n",
            "Epoch 6/10, Loss: 0.04167\n",
            "Epoch 7/10, Loss: 0.03968\n",
            "Epoch 8/10, Loss: 0.03812\n",
            "Epoch 9/10, Loss: 0.03657\n",
            "Epoch 10/10, Loss: 0.03521\n",
            "Classification accuracy: 96.63%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9663"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model:"
      ],
      "metadata": {
        "id": "vejnYcIWY8W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_classification_results(x, true_labels, pred_labels, n=10):\n",
        "    fig, axes = plt.subplots(1, n, figsize=(15, 3))\n",
        "    for i, ax in enumerate(axes):\n",
        "        ax.imshow(x[i].reshape(28,28), cmap='binary')\n",
        "        ax.set_title(f\"True: {true_labels[i]}\\nPred: {pred_labels[i]}\")\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "predictions_joint = rbm_joint.classify(X_test_binary)\n",
        "accuracy_joint = np.mean(predictions_joint == Y_test)\n",
        "print(f\"Generative RBM Classification Accuracy: {accuracy_joint*100:.2f}%\")\n",
        "\n",
        "show_classification_results(X_test, Y_test, predictions_joint, n=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "3PXp7rKwY8zC",
        "outputId": "e79458f1-4ae9-4d22-f73c-a700db36e43f"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative RBM Classification Accuracy: 10.44%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DRBM model"
      ],
      "metadata": {
        "id": "m8ZfG7dMqhzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the data:"
      ],
      "metadata": {
        "id": "WxFUGe204xtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# بارگذاری و پیش‌پردازش داده‌ها\n",
        "# ---------------------------\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader  = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "5asN7F9q4wlZ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model:"
      ],
      "metadata": {
        "id": "V3_Vu7uv45dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminativeRBMModel(nn.Module):\n",
        "    def __init__(self, input_dim=784, num_classes=10, hidden_dim=256):\n",
        "        super(DiscriminativeRBMModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # پارامترهای مربوط به بخش تصویر\n",
        "        self.b = nn.Parameter(torch.zeros(input_dim))\n",
        "        # پارامترهای مربوط به برچسب\n",
        "        self.d = nn.Parameter(torch.zeros(num_classes))\n",
        "        # وزن‌های مربوط به اتصال تصویر به لایه پنهان\n",
        "        self.W = nn.Parameter(torch.randn(input_dim, hidden_dim) * 0.1)\n",
        "        # وزن‌های مربوط به اتصال برچسب به لایه پنهان\n",
        "        self.U = nn.Parameter(torch.randn(num_classes, hidden_dim) * 0.1)\n",
        "        # بایاس لایه پنهان\n",
        "        self.c = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    def free_energy(self, x, y):\n",
        "        vbias_term = torch.matmul(x, self.b) + torch.matmul(y, self.d)\n",
        "        hidden_linear = torch.matmul(x, self.W) + torch.matmul(y, self.U) + self.c\n",
        "        # Clamp کردن ورودی به exp برای جلوگیری از overflow\n",
        "        hidden_linear = torch.clamp(hidden_linear, max=50)\n",
        "        hidden_term = torch.sum(torch.log(1 + torch.exp(hidden_linear)), dim=1)\n",
        "        return -vbias_term - hidden_term\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        برای هر نمونه ورودی x (به شکل (batch, 784))، با استفاده از تابع free energy،\n",
        "        برای هر برچسب ممکن (به صورت One-hot) مقادیر -F(x,y) (که معادل logits هستند) محاسبه می‌شود.\n",
        "        خروجی logits به شکل (batch, num_classes) خواهد بود.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        logits = []\n",
        "        for label in range(self.num_classes):\n",
        "            y_candidate = torch.zeros(batch_size, self.num_classes, device=x.device)\n",
        "            y_candidate[:, label] = 1.0\n",
        "            F = self.free_energy(x, y_candidate)   # (batch,)\n",
        "            logits.append((-F).unsqueeze(1))\n",
        "        logits = torch.cat(logits, dim=1)  # (batch, num_classes)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "iL2eC6LrqnM-"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model:"
      ],
      "metadata": {
        "id": "TvKnKXhpq0Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# آموزش مدل تمایزگر\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DiscriminativeRBMModel(input_dim=784, num_classes=10, hidden_dim=256).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "num_epochs = 10\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # بایناریزه کردن تصاویر (اگر لازم باشد)\n",
        "        images = (images > 0.5).float()\n",
        "        images = images.view(images.size(0), -1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)  # (batch, num_classes)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(trainset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgnpYIsfq209",
        "outputId": "cb71561f-5a41-45a2-edee-aad32215abe2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.23599\n",
            "Epoch 2/10, Loss: 0.10353\n",
            "Epoch 3/10, Loss: 0.07536\n",
            "Epoch 4/10, Loss: 0.05815\n",
            "Epoch 5/10, Loss: 0.05386\n",
            "Epoch 6/10, Loss: 0.04407\n",
            "Epoch 7/10, Loss: 0.04030\n",
            "Epoch 8/10, Loss: 0.03591\n",
            "Epoch 9/10, Loss: 0.03353\n",
            "Epoch 10/10, Loss: 0.03283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model:"
      ],
      "metadata": {
        "id": "tOmSq1jHqth_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# ارزیابی مدل تمایزگر\n",
        "# ---------------------------\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        images = (images > 0.5).float()\n",
        "        images = images.view(images.size(0), -1)\n",
        "        logits = model(images)\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Discriminative Model Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv84TXVmqwZ5",
        "outputId": "529c198d-a095-4872-9bb7-17f76ced03dd"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminative Model Accuracy: 96.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fgCnmxCnnAtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_lDZKtcSnAf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r9n4kxgSnARO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GwotZx-EnAL-"
      }
    }
  ]
}