{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Machine Learning\n",
        "## Project Phase 2\n",
        "\n",
        "Sina Fathi | 402111261\n",
        "\n",
        "Seyyed Amirmahdi Sadrzadeh | 401102015"
      ],
      "metadata": {
        "id": "hniNtD-Zew1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Data Preproccessing"
      ],
      "metadata": {
        "id": "tvacMiiXb_2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we import the needed libraries:"
      ],
      "metadata": {
        "id": "8ziJ9zUdcH_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from sympy import mod_inverse\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "metadata": {
        "id": "d3lKKdgBGJQS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use torch library to load the MNIST dataset and then turn them to numpy arrays:"
      ],
      "metadata": {
        "id": "p_vzRIfdcMhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "x_data = mnist.data.to_numpy()\n",
        "y_data = mnist.target.to_numpy().astype(int)"
      ],
      "metadata": {
        "id": "AyG6f5oKGPHh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We devide the data into test and train datas an also, use the datas, we binarize them by setting a threshold equal to 128. So all pixels with values higher than 128 will be set to 1 and others will be set to 0. We also flatten the datas to use them in our model:"
      ],
      "metadata": {
        "id": "vyLf42oLclGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = (x_data > 127).astype(np.float32)\n",
        "x_train, x_test = x_data[:60000], x_data[60000:]\n",
        "y_train, y_test = y_data[:60000], y_data[60000:]\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)"
      ],
      "metadata": {
        "id": "H1nb8nHOKoXd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the labels one-hot:"
      ],
      "metadata": {
        "id": "C4I1exuy1-bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "y_train = ohe.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test = ohe.transform(y_test.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "3-cyphvA19Jh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot to show the samples:"
      ],
      "metadata": {
        "id": "_hNNKcVhdJL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_samples(samples, nrows=2, ncols=5):\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(10, 5))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(samples[i].reshape(28, 28), cmap='binary')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wlGlqEJ7Kt7M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we use the above function to show 10 samples from the dataset:"
      ],
      "metadata": {
        "id": "DptyrqJwdPLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_samples(x_train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "kr0tRRKyKzrV",
        "outputId": "960ea3b0-1849-4abf-8bfe-84ecd15b9ca2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Boltzman machine:"
      ],
      "metadata": {
        "id": "b0EKwQygdlPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Boltzmann machine is like the one we used in the first phase of the project and follow the same patterns to update the parameters."
      ],
      "metadata": {
        "id": "8rd8PBL8E2LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RBM:\n",
        "    def __init__(self, n_visible, n_hidden, learning_rate=0.1):\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.W = np.random.normal(0, 0.1, (n_visible, n_hidden))\n",
        "        self.v_bias = np.zeros(n_visible)\n",
        "        self.h_bias = np.zeros(n_hidden)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (np.random.uniform(size=prob.shape) < prob).astype(np.int32)\n",
        "\n",
        "    def gibbs_sampling(self, hid, k):\n",
        "        v_prob_0 = self.sigmoid(np.dot(hid, self.W.T) + self.v_bias)\n",
        "        v = self.sample(v_prob_0)\n",
        "        for _ in range(k):\n",
        "            h_prob = self.sigmoid(np.dot(v, self.W) + self.h_bias)\n",
        "            h_sample = self.sample(h_prob)\n",
        "            v_prob = self.sigmoid(np.dot(h_sample, self.W.T) + self.v_bias)\n",
        "            v = self.sample(v_prob)\n",
        "        return v\n",
        "\n",
        "    def contrastive_divergence(self, v, k):\n",
        "        h_prob_0 = self.sigmoid(np.dot(v, self.W) + self.h_bias)\n",
        "        h_sample_0 = self.sample(h_prob_0)\n",
        "\n",
        "        v_neg = v.copy()\n",
        "\n",
        "        for _ in range(k):\n",
        "            h_prob = self.sigmoid(np.dot(v_neg, self.W) + self.h_bias)\n",
        "            h_sample = self.sample(h_prob)\n",
        "            v_prob_neg = self.sigmoid(np.dot(h_sample, self.W.T) + self.v_bias)\n",
        "            v_neg = self.sample(v_prob_neg)\n",
        "\n",
        "        h_prob_neg = self.sigmoid(np.dot(v_neg, self.W) + self.h_bias)\n",
        "\n",
        "        positive_grad = np.dot(v.T, h_prob_0)\n",
        "        negative_grad = np.dot(v_neg.T, h_prob_neg)\n",
        "\n",
        "        batch_size = v.shape[0]\n",
        "        self.W += self.learning_rate * (positive_grad - negative_grad) / batch_size\n",
        "        self.v_bias += self.learning_rate * np.mean(v - v_neg, axis=0)\n",
        "        self.h_bias += self.learning_rate * np.mean(h_prob_0 - h_prob_neg, axis=0)\n",
        "\n",
        "        return v_neg\n",
        "\n",
        "    def train(self, data, epochs=10, k=1, batch_size=64):\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(data)\n",
        "            epoch_loss = 0\n",
        "            for batch_start in range(0, len(data), batch_size):\n",
        "                batch = data[batch_start:batch_start + batch_size]\n",
        "                v_neg = self.contrastive_divergence(batch, k)\n",
        "                epoch_loss += np.mean((batch - v_neg) ** 2)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(data):.5f}\")\n",
        "\n",
        "    def generate_samples(self, n_samples=10, k=1):\n",
        "        samples = []\n",
        "        for i in range(n_samples):\n",
        "            sample = np.random.binomial(1, 0.5, self.n_hidden)\n",
        "            sample = self.gibbs_sampling(sample, k)\n",
        "            samples.append(sample)\n",
        "        return np.array(samples)"
      ],
      "metadata": {
        "id": "g6q689osK99h"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the simulations in phase 1 we define the number of hidden layers and k as follow:"
      ],
      "metadata": {
        "id": "W14bINYQFBNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_visible = x_train.shape[1]\n",
        "n_hidden = 32\n",
        "k_values = [1, 5, 10]"
      ],
      "metadata": {
        "id": "xPvmqTsUgXxU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train the model on the MNIST dataset:"
      ],
      "metadata": {
        "id": "VMdo3cSGLbxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "rbms = {}\n",
        "for k in k_values:\n",
        "    print(f\"Training RBM with k={k}\")\n",
        "    rbm = RBM(n_visible, n_hidden)\n",
        "    rbm.train(x_train, epochs=10, k=k)\n",
        "    rbms[k] = rbm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLmlsavtLPnq",
        "outputId": "8867b326-7419-4f06-8919-057c97e359a5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RBM with k=1\n",
            "Epoch 1/10, Loss: 0.00199\n",
            "Epoch 2/10, Loss: 0.00161\n",
            "Epoch 3/10, Loss: 0.00153\n",
            "Epoch 4/10, Loss: 0.00150\n",
            "Epoch 5/10, Loss: 0.00148\n",
            "Epoch 6/10, Loss: 0.00146\n",
            "Epoch 7/10, Loss: 0.00146\n",
            "Epoch 8/10, Loss: 0.00145\n",
            "Epoch 9/10, Loss: 0.00144\n",
            "Epoch 10/10, Loss: 0.00144\n",
            "Training RBM with k=5\n",
            "Epoch 1/10, Loss: 0.00220\n",
            "Epoch 2/10, Loss: 0.00174\n",
            "Epoch 3/10, Loss: 0.00164\n",
            "Epoch 4/10, Loss: 0.00158\n",
            "Epoch 5/10, Loss: 0.00155\n",
            "Epoch 6/10, Loss: 0.00153\n",
            "Epoch 7/10, Loss: 0.00152\n",
            "Epoch 8/10, Loss: 0.00150\n",
            "Epoch 9/10, Loss: 0.00150\n",
            "Epoch 10/10, Loss: 0.00149\n",
            "Training RBM with k=10\n",
            "Epoch 1/10, Loss: 0.00231\n",
            "Epoch 2/10, Loss: 0.00182\n",
            "Epoch 3/10, Loss: 0.00171\n",
            "Epoch 4/10, Loss: 0.00164\n",
            "Epoch 5/10, Loss: 0.00161\n",
            "Epoch 6/10, Loss: 0.00158\n",
            "Epoch 7/10, Loss: 0.00156\n",
            "Epoch 8/10, Loss: 0.00155\n",
            "Epoch 9/10, Loss: 0.00154\n",
            "Epoch 10/10, Loss: 0.00153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model is converging and the loss is getting lower to a fixed point.\n",
        "\n",
        "As we increase k, the final loss of the model on the training data is not getting better but based on the results of the first phase, we know that the model with a higher k performs better on the test datas and gives better resuls."
      ],
      "metadata": {
        "id": "FuUFNaePFT_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test these models, we use the test dataset of the MNIST:"
      ],
      "metadata": {
        "id": "TUukKdxeLx7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rbm(rbm, data):\n",
        "    loss = 0\n",
        "    for i in range(0, len(data), 64):\n",
        "        batch = data[i:i+64]\n",
        "        v_neg = rbm.contrastive_divergence(batch, k=1)\n",
        "        loss += np.mean((batch - v_neg) ** 2)\n",
        "    avg_loss = loss / (len(data) / 64)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "nTs_QhhmL27i"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_losses = {}\n",
        "for k in k_values:\n",
        "    loss = evaluate_rbm(rbms[k], x_test)\n",
        "    simple_losses[k] = loss\n",
        "    print(f\"Simple RBM with k={k} Test Loss: {loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m7k3Reu8bIc",
        "outputId": "f35fef45-2b01-4474-b5ae-0a3ba4ac65ef"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple RBM with k=1 Test Loss: 0.09149\n",
            "Simple RBM with k=5 Test Loss: 0.08900\n",
            "Simple RBM with k=10 Test Loss: 0.08845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test loss is getting better by increasing k. This confirms the results discussed in the phase 1."
      ],
      "metadata": {
        "id": "tuADL8LgGBVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Secure Boltzman Machine:"
      ],
      "metadata": {
        "id": "lrbyHJ_mLsIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we learned from the project file, the Secure Boltzmann Machine works with two parties. Each party have a specific number of parameters and each have they're own weights and biases.\n",
        "\n",
        "To update the parameters, we use ElGamal encryption so that both models can calculate loss with needed values (e.g. $v_1h_2$). To implement this, we approach like the model below:"
      ],
      "metadata": {
        "id": "8IELhEF7GPEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SecureRBM:\n",
        "    def __init__(self, n_visible, n_hidden, mA, learning_rate=0.001, p=467):\n",
        "        \"\"\"\n",
        "        Initialize SecureRBM for two parties A and B.\n",
        "\n",
        "        Parameters:\n",
        "        - n_visible: total number of visible neurons (e.g., 784 for MNIST).\n",
        "        - n_hidden: number of hidden neurons.\n",
        "        - mA: number of visible features belonging to Party A.\n",
        "          (Then mB = n_visible - mA belong to Party B.)\n",
        "        - learning_rate: learning rate for weight updates.\n",
        "        - p: a prime number used for the ElGamal-based secure operations.\n",
        "        \"\"\"\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.mA = mA\n",
        "        self.mB = n_visible - mA\n",
        "        self.learning_rate = learning_rate\n",
        "        self.p = p\n",
        "\n",
        "        limit = np.sqrt(6 / (self.n_visible + self.n_hidden))\n",
        "        self.W = np.random.uniform(-limit, limit, (n_visible, n_hidden))\n",
        "        self.v_bias = np.zeros(n_visible)\n",
        "        self.h_bias = np.zeros(n_hidden)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -50, 50)))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (np.random.uniform(size=prob.shape) < prob).astype(np.int32)\n",
        "\n",
        "    def generate_keys(self):\n",
        "        \"\"\"\n",
        "        Generate ElGamal public and private keys.\n",
        "        \"\"\"\n",
        "        g = random.randint(2, self.p - 2)\n",
        "        x = random.randint(1, self.p - 2)\n",
        "        y = pow(g, x, self.p)\n",
        "        return (self.p, g, y), x\n",
        "\n",
        "    def encrypt(self, public_key, message):\n",
        "        \"\"\"\n",
        "        Encrypt a message (an integer) using the public key.\n",
        "        \"\"\"\n",
        "        p, g, y = public_key\n",
        "        k = random.randint(1, p - 2)\n",
        "        c1 = pow(g, k, p)\n",
        "        c2 = (int(message) * pow(y, k, p)) % p\n",
        "        return (c1, c2)\n",
        "\n",
        "    def decrypt_partial(self, private_key, public_key, ciphertext):\n",
        "        \"\"\"\n",
        "        Partially decrypt a ciphertext using the private key.\n",
        "        \"\"\"\n",
        "        p, g, y = public_key\n",
        "        c1, c2 = ciphertext\n",
        "        s = pow(c1, private_key, p)\n",
        "        return (c2, s)\n",
        "\n",
        "    def decrypt_final(self, partially_decrypted, p):\n",
        "        \"\"\"\n",
        "        Final decryption using the modular inverse.\n",
        "        \"\"\"\n",
        "        c2, s = partially_decrypted\n",
        "        s_inv = mod_inverse(s, p)\n",
        "        message = (c2 * s_inv) % p\n",
        "        return message\n",
        "\n",
        "    def sample_hidden_secure(self, v, public_key_a, private_key_a, public_key_b, private_key_b, random_r):\n",
        "        \"\"\"\n",
        "        Securely sample the hidden layer using contributions from both parties.\n",
        "\n",
        "        Parameters:\n",
        "        - v: input visible data (batch, n_visible)\n",
        "        - public_key_a, private_key_a: keys for Party A\n",
        "        - public_key_b, private_key_b: keys for Party B\n",
        "        - random_r: random mask array (batch, n_hidden)\n",
        "\n",
        "        Returns:\n",
        "        - Binary samples for hidden layer (batch, n_hidden)\n",
        "        \"\"\"\n",
        "        vA = v[:, :self.mA]\n",
        "        vB = v[:, self.mA:]\n",
        "\n",
        "        WA = self.W[:self.mA, :]\n",
        "        WB = self.W[self.mA:, :]\n",
        "\n",
        "        act_A = np.dot(vA, WA) + self.h_bias\n",
        "        act_B = np.dot(vB, WB)\n",
        "\n",
        "        joint_act = act_A + act_B\n",
        "        final_act = joint_act - random_r\n",
        "        h_prob = self.sigmoid(final_act)\n",
        "        return self.sample(h_prob)\n",
        "\n",
        "    def sample_visible_secure(self, h, public_key_a, private_key_a, public_key_b, private_key_b, random_r):\n",
        "        \"\"\"\n",
        "        Securely sample the visible layer.\n",
        "\n",
        "        Parameters:\n",
        "        - h: hidden layer activations (batch, n_hidden)\n",
        "        - public_key_a, private_key_a: keys for Party A\n",
        "        - public_key_b, private_key_b: keys for Party B\n",
        "        - random_r: **Separate random masks for A and B (tuple of two arrays)**\n",
        "\n",
        "        Returns:\n",
        "        - Binary reconstruction of the visible layer (batch, n_visible)\n",
        "        \"\"\"\n",
        "        random_r_A, random_r_B = random_r\n",
        "\n",
        "        WA_T = self.W[:self.mA, :].T\n",
        "        WB_T = self.W[self.mA:, :].T\n",
        "\n",
        "        bA = self.v_bias[:self.mA]\n",
        "        bB = self.v_bias[self.mA:]\n",
        "\n",
        "        act_A = np.dot(h, WA_T) + bA\n",
        "        act_B = np.dot(h, WB_T) + bB\n",
        "\n",
        "        act_A = act_A - random_r_A\n",
        "        act_B = act_B - random_r_B\n",
        "\n",
        "        # Now concatenate securely processed activations\n",
        "        joint_act = np.concatenate([act_A, act_B], axis=1)\n",
        "\n",
        "        v_prob = self.sigmoid(joint_act)\n",
        "        return self.sample(v_prob)\n",
        "\n",
        "\n",
        "    def secure_compute(self, partial_activation, public_key, private_key):\n",
        "        result = np.zeros_like(partial_activation)\n",
        "        for i in range(partial_activation.shape[0]):\n",
        "            for j in range(partial_activation.shape[1]):\n",
        "                value = int(partial_activation[i, j])\n",
        "                ciphertext = self.encrypt(public_key, value)\n",
        "                dec = self.decrypt_final(self.decrypt_partial(private_key, public_key, ciphertext), self.p)\n",
        "                result[i, j] = dec\n",
        "        return result\n",
        "\n",
        "    def reconstruct(self, v, public_key_a, private_key_a, public_key_b, private_key_b, random_r_hidden, random_r_visible):\n",
        "        \"\"\"\n",
        "        Securely reconstruct the visible input using separate random masks for the hidden and visible layers.\n",
        "        \"\"\"\n",
        "        h = self.sample_hidden_secure(v, public_key_a, private_key_a, public_key_b, private_key_b, random_r_hidden)\n",
        "        v_recon = self.sample_visible_secure(h, public_key_a, private_key_a, public_key_b, private_key_b, random_r_visible)\n",
        "        return v_recon\n",
        "\n",
        "    def contrastive_divergence_secure(self, v, k, public_key_a, private_key_a, public_key_b, private_key_b):\n",
        "        \"\"\"\n",
        "        Perform secure Contrastive Divergence.\n",
        "        Generates separate random masks for hidden and visible layers.\n",
        "        \"\"\"\n",
        "        mask_scale = 0.1\n",
        "        batch_size = v.shape[0]\n",
        "        random_r_hidden = np.random.randint(1, int(self.p * mask_scale), size=(batch_size, self.n_hidden))\n",
        "        random_r_visible = np.random.randint(1, int(self.p * mask_scale), size=(batch_size, self.n_visible))\n",
        "\n",
        "        v_neg = v.copy()\n",
        "        for _ in range(k):\n",
        "            mA = self.mA\n",
        "            part_A = np.dot(v_neg[:, :mA], self.W[:mA, :])\n",
        "            part_B = np.dot(v_neg[:, mA:], self.W[mA:, :])\n",
        "\n",
        "            secure_A = self.secure_compute(part_A, public_key_a, private_key_a)\n",
        "            secure_B = self.secure_compute(part_B, public_key_b, private_key_b)\n",
        "\n",
        "            joint_act = secure_A + secure_B + self.h_bias\n",
        "            final_act = joint_act - random_r_hidden\n",
        "            h_prob = self.sigmoid(final_act)\n",
        "            h_sample = self.sample(h_prob)\n",
        "\n",
        "            v_prob = self.sigmoid(np.dot(h_sample, self.W.T) + self.v_bias)\n",
        "            v_neg = self.sample(v_prob)\n",
        "\n",
        "        positive_grad = np.dot(v.T, h_prob)\n",
        "        negative_grad = np.dot(v_neg.T, h_prob)\n",
        "\n",
        "        # Gradient clipping to avoid instability\n",
        "        grad_norm = np.linalg.norm(positive_grad - negative_grad)\n",
        "        if grad_norm > 5.0:\n",
        "            positive_grad = (positive_grad / grad_norm) * 5.0\n",
        "            negative_grad = (negative_grad / grad_norm) * 5.0\n",
        "\n",
        "        # Update weights\n",
        "        self.W += self.learning_rate * (positive_grad - negative_grad) / batch_size\n",
        "        self.v_bias += self.learning_rate * np.mean(v - v_neg, axis=0)\n",
        "        self.h_bias += self.learning_rate * np.mean(h_prob - h_prob, axis=0)\n",
        "        return v_neg\n",
        "\n",
        "    def train_secure(self, X_train_A, X_train_B, epochs=10, k=1, batch_size=64,\n",
        "                 public_key_a=None, private_key_a=None,\n",
        "                 public_key_b=None, private_key_b=None):\n",
        "        \"\"\"\n",
        "        Train SecureRBM using secure Contrastive Divergence for two parties (A and B).\n",
        "        \"\"\"\n",
        "        num_batches = len(X_train_A) // batch_size\n",
        "        mask_scale = 0.1\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, len(X_train_A), batch_size):\n",
        "                batch_A = X_train_A[i:i + batch_size]\n",
        "                batch_B = X_train_B[i:i + batch_size]\n",
        "                batch_size_local = batch_A.shape[0]\n",
        "\n",
        "                # Generate separate random masks for Party A and B\n",
        "                random_r_hidden = np.random.randint(1, int(self.p * mask_scale), size=(batch_size_local, self.n_hidden))\n",
        "                random_r_visible_A = np.random.randint(1, int(self.p * mask_scale), size=(batch_size_local, self.mA))\n",
        "                random_r_visible_B = np.random.randint(1, int(self.p * mask_scale), size=(batch_size_local, self.mB))\n",
        "\n",
        "                # Secure Gibbs Sampling for Hidden Layer\n",
        "                h_sample = self.sample_hidden_secure(\n",
        "                    np.concatenate([batch_A, batch_B], axis=1),\n",
        "                    public_key_a, private_key_a, public_key_b, private_key_b,\n",
        "                    random_r_hidden\n",
        "                )\n",
        "\n",
        "                # Secure Gibbs Sampling for Visible Layer\n",
        "                v_recon_A = self.sample_visible_secure(\n",
        "                    h_sample, public_key_a, private_key_a, public_key_b, private_key_b,\n",
        "                    (random_r_visible_A, random_r_visible_B)\n",
        "                )\n",
        "\n",
        "                v_recon = v_recon_A\n",
        "\n",
        "                loss = np.mean((np.concatenate([batch_A, batch_B], axis=1) - v_recon) ** 2)\n",
        "                epoch_loss += loss\n",
        "\n",
        "                self.contrastive_divergence_secure(\n",
        "                    np.concatenate([batch_A, batch_B], axis=1), k,\n",
        "                    public_key_a, private_key_a, public_key_b, private_key_b\n",
        "                )\n",
        "\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Secure Loss: {avg_loss:.5f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate_samples(self, n_samples=10, k=1,\n",
        "                         public_key_a=None, private_key_a=None,\n",
        "                         public_key_b=None, private_key_b=None):\n",
        "        \"\"\"\n",
        "        Generate new samples from the trained SecureRBM model.\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        for _ in range(n_samples):\n",
        "            v = np.random.binomial(1, 0.5, self.n_visible).reshape(1, -1)\n",
        "            mask_scale = 0.1\n",
        "            random_r = np.random.randint(1, int(self.p * mask_scale), size=(1, self.n_visible))\n",
        "            for _ in range(k):\n",
        "                h = self.sample_hidden_secure(v, public_key_a, private_key_a, public_key_b, private_key_b, random_r[:, :self.h_bias.shape[0]])\n",
        "                v = self.sample_visible_secure(h, public_key_a, private_key_a, public_key_b, private_key_b, random_r)\n",
        "            samples.append(v.flatten())\n",
        "        return np.array(samples)"
      ],
      "metadata": {
        "id": "BLfmlrpDwl2y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Secure Model:"
      ],
      "metadata": {
        "id": "Dm5pL5zOLlUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "n_visible = 784\n",
        "n_hidden = 256\n",
        "mA = 392\n",
        "mB = n_visible - mA\n",
        "learning_rate = 0.1\n",
        "epochs = 5\n",
        "k = 1\n",
        "batch_size = 64\n",
        "\n",
        "# Initialize Secure RBM\n",
        "secure_rbm = SecureRBM(n_visible, n_hidden, mA, learning_rate)\n",
        "\n",
        "# Generate Public and Private Keys for Secure Computation\n",
        "public_key_a, private_key_a = secure_rbm.generate_keys()\n",
        "public_key_b, private_key_b = secure_rbm.generate_keys()\n",
        "\n",
        "X_train_A = x_train[:, :mA]\n",
        "X_train_B = x_train[:, mA:]\n",
        "\n",
        "# Train Secure RBM with Two Parties\n",
        "secure_rbm.train_secure(\n",
        "    X_train_A, X_train_B, epochs=epochs, k=k, batch_size=batch_size,\n",
        "    public_key_a=public_key_a, private_key_a=private_key_a,\n",
        "    public_key_b=public_key_b, private_key_b=private_key_b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rDM79G0wojO",
        "outputId": "44a69cdf-2376-4d81-a770-303aea00102f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Secure Loss: 0.13345\n",
            "Epoch 2/5, Secure Loss: 0.13306\n",
            "Epoch 3/5, Secure Loss: 0.13304\n",
            "Epoch 4/5, Secure Loss: 0.13303\n",
            "Epoch 5/5, Secure Loss: 0.13303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the secure model:"
      ],
      "metadata": {
        "id": "yhRBFpDPL787"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_secure_rbm(secure_rbm, data, public_key_a, private_key_a, public_key_b, private_key_b, batch_size=64):\n",
        "    loss = 0.0\n",
        "    num_batches = len(data) // batch_size\n",
        "    mask_scale = 0.1\n",
        "\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        batch_size_local = batch.shape[0]\n",
        "\n",
        "        random_r_hidden = np.random.randint(1, int(secure_rbm.p * mask_scale), size=(batch_size_local, secure_rbm.n_hidden))\n",
        "        random_r_visible_A = np.random.randint(1, int(secure_rbm.p * mask_scale), size=(batch_size_local, secure_rbm.mA))\n",
        "        random_r_visible_B = np.random.randint(1, int(secure_rbm.p * mask_scale), size=(batch_size_local, secure_rbm.mB))\n",
        "\n",
        "        v_recon = secure_rbm.reconstruct(\n",
        "            batch, public_key_a, private_key_a, public_key_b, private_key_b,\n",
        "            random_r_hidden, (random_r_visible_A, random_r_visible_B)\n",
        "        )\n",
        "\n",
        "        # Compute loss between original and reconstructed data\n",
        "        loss += np.mean((batch - v_recon) ** 2)\n",
        "\n",
        "    avg_loss = loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "secure_loss = evaluate_secure_rbm(\n",
        "    secure_rbm,\n",
        "    x_test,\n",
        "    public_key_a,\n",
        "    private_key_a,\n",
        "    public_key_b,\n",
        "    private_key_b,\n",
        "    batch_size=64\n",
        ")\n",
        "print(f\"Secure RBM Test Loss: {secure_loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muv4Bq4uL7PW",
        "outputId": "a1efa669-9adc-48a8-d8d2-2e3e227d371e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secure RBM Test Loss: 0.13586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison:"
      ],
      "metadata": {
        "id": "G1UpSjguMCJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Comparing the Simple RBM with \")\n",
        "print(f\"Simple RBM with k=1 Test Loss: {simple_losses[1]:.5f}\")\n",
        "print(f\"Simple RBM with k=5 Test Loss: {simple_losses[5]:.5f}\")\n",
        "print(f\"Simple RBM with k=10 Test Loss: {simple_losses[10]:.5f}\")\n",
        "print(f\"Secure RBM Test Loss: {secure_loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q6jIAtMMDI8",
        "outputId": "10ba036b-ca96-4a19-c268-ac18a19277e2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing the Simple RBM with \n",
            "Simple RBM with k=1 Test Loss: 0.09149\n",
            "Simple RBM with k=5 Test Loss: 0.08900\n",
            "Simple RBM with k=10 Test Loss: 0.08845\n",
            "Secure RBM Test Loss: 0.13586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the loss of the secure model is much higher. Aside from that, the time taken to train the secure model is also higher than the simple model. So in order to have a secure connection between two parties, we have to accept a liitle inaccuracy and computational time."
      ],
      "metadata": {
        "id": "u6sRAhTYHL0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GenerativeRBM"
      ],
      "metadata": {
        "id": "kncasc4xHK-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is implemented below:"
      ],
      "metadata": {
        "id": "fqHU9KB0w7Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerativeRBM:\n",
        "    def __init__(self, n_visible, n_hidden, n_labels, learning_rate=0.01, epochs=20, batch_size=128):\n",
        "        \"\"\"\n",
        "        Initializes a Generative RBM with added label interactions.\n",
        "        \"\"\"\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_labels = n_labels\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W = np.random.randn(n_visible, n_hidden) * 0.01\n",
        "        self.U = np.random.randn(n_labels, n_hidden) * 0.01\n",
        "        self.b = np.zeros((1, n_visible))\n",
        "        self.c = np.zeros((1, n_hidden))\n",
        "        self.d = np.zeros((1, n_labels))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (prob > np.random.rand(*prob.shape)).astype(np.float32)\n",
        "\n",
        "    def contrastive_divergence(self, x_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Performs Contrastive Divergence (CD-1) to update weights.\n",
        "        Returns reconstructed x and y.\n",
        "        \"\"\"\n",
        "        batch_size = x_batch.shape[0]\n",
        "\n",
        "        h_prob = self.sigmoid(np.dot(x_batch, self.W) + np.dot(y_batch, self.U) + self.c)\n",
        "        h_sample = self.sample(h_prob)\n",
        "\n",
        "        x_recon_prob = self.sigmoid(np.dot(h_sample, self.W.T) + self.b)\n",
        "        x_recon_sample = self.sample(x_recon_prob)\n",
        "\n",
        "        y_recon_prob = self.sigmoid(np.dot(h_sample, self.U.T) + self.d)\n",
        "        y_recon_sample = self.sample(y_recon_prob)\n",
        "\n",
        "        h_recon_prob = self.sigmoid(np.dot(x_recon_sample, self.W) + np.dot(y_recon_sample, self.U) + self.c)\n",
        "\n",
        "        # Updating weights\n",
        "        self.W += self.learning_rate * ((np.dot(x_batch.T, h_prob) - np.dot(x_recon_sample.T, h_recon_prob)) / batch_size)\n",
        "        self.U += self.learning_rate * ((np.dot(y_batch.T, h_prob) - np.dot(y_recon_sample.T, h_recon_prob)) / batch_size)\n",
        "        self.b += self.learning_rate * np.mean(x_batch - x_recon_sample, axis=0)\n",
        "        self.c += self.learning_rate * np.mean(h_prob - h_recon_prob, axis=0)\n",
        "        self.d += self.learning_rate * np.mean(y_batch - y_recon_sample, axis=0)\n",
        "\n",
        "        return x_recon_sample, y_recon_sample\n",
        "\n",
        "\n",
        "    def train(self, X_train, Y_train):\n",
        "        \"\"\"\n",
        "        Trains the RBM using Contrastive Divergence and prints the loss.\n",
        "        \"\"\"\n",
        "        print(\"Starting training...\")\n",
        "        for epoch in range(self.epochs):\n",
        "            epoch_loss = 0\n",
        "            num_batches = X_train.shape[0] // self.batch_size\n",
        "\n",
        "            for i in range(0, X_train.shape[0], self.batch_size):\n",
        "                x_batch = X_train[i:i + self.batch_size]\n",
        "                y_batch = Y_train[i:i + self.batch_size]\n",
        "\n",
        "                if y_batch.shape[1] != self.n_labels:\n",
        "                    y_batch = y_batch.T\n",
        "\n",
        "                # Performing Contrastive Divergence\n",
        "                x_recon, y_recon = self.contrastive_divergence(x_batch, y_batch)\n",
        "\n",
        "                # Computing Reconstruction Loss\n",
        "                loss_x = np.mean((x_batch - x_recon) ** 2)\n",
        "                loss_y = np.mean((y_batch - y_recon) ** 2)\n",
        "                batch_loss = (loss_x + loss_y) / 2\n",
        "\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.5f}\")\n",
        "\n",
        "\n",
        "    def classify(self, X_test, Y_test):\n",
        "        \"\"\"\n",
        "        Classifies input data using the trained RBM.\n",
        "        \"\"\"\n",
        "        h_test = self.sigmoid(np.dot(X_test, self.W) + np.dot(Y_test, self.U) + self.c.reshape(1, -1))\n",
        "        y_pred = np.argmax(np.dot(h_test, self.U.T) + self.d, axis=1)\n",
        "        y_true = np.argmax(Y_test, axis=1)\n",
        "\n",
        "        accuracy = np.mean(y_pred == y_true)\n",
        "        print(f\"Classification accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "v6ihNwNJD-IO"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we define a function to load the MNIST dataset again in a way to give it to the model:"
      ],
      "metadata": {
        "id": "WGWjdeAimXd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mnist():\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    # Load Train & Test sets\n",
        "    trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    testset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    X_train = trainset.data.numpy()\n",
        "    Y_train = trainset.targets.numpy()\n",
        "    X_test = testset.data.numpy()\n",
        "    Y_test = testset.targets.numpy()\n",
        "\n",
        "    # Binarize Images (Threshold at 128)\n",
        "    X_train = (X_train > 127).astype(np.float32)\n",
        "    X_test = (X_test > 127).astype(np.float32)\n",
        "\n",
        "    # Flatten Images\n",
        "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # One-hot encode labels\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    Y_train = encoder.fit_transform(Y_train.reshape(-1, 1))\n",
        "    Y_test = encoder.transform(Y_test.reshape(-1, 1))\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test"
      ],
      "metadata": {
        "id": "qJU2uxBDmXEc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model:"
      ],
      "metadata": {
        "id": "w2YY4nfYZDWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_test, Y_test = load_mnist()\n",
        "\n",
        "n_visible = 784\n",
        "n_labels = 10\n",
        "n_hidden = 256\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "k = 5\n",
        "batch_size = 128\n",
        "\n",
        "# Initialize and Train RBM\n",
        "rbm = GenerativeRBM(n_visible, n_hidden, n_labels, learning_rate, epochs=epochs, batch_size=batch_size)\n",
        "rbm.train(X_train, Y_train)\n",
        "\n",
        "# Evaluate Accuracy\n",
        "rbm.classify(X_test, Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnnABfnEZC_4",
        "outputId": "19c8a1ef-049f-48f3-99c6-3721f4ed7409"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 468kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.21MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.39MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/10, Loss: 0.10348\n",
            "Epoch 2/10, Loss: 0.06156\n",
            "Epoch 3/10, Loss: 0.05253\n",
            "Epoch 4/10, Loss: 0.04758\n",
            "Epoch 5/10, Loss: 0.04418\n",
            "Epoch 6/10, Loss: 0.04181\n",
            "Epoch 7/10, Loss: 0.03979\n",
            "Epoch 8/10, Loss: 0.03821\n",
            "Epoch 9/10, Loss: 0.03664\n",
            "Epoch 10/10, Loss: 0.03546\n",
            "Classification accuracy: 96.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DRBM model"
      ],
      "metadata": {
        "id": "m8ZfG7dMqhzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the data:"
      ],
      "metadata": {
        "id": "WxFUGe204xtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader  = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "5asN7F9q4wlZ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model:"
      ],
      "metadata": {
        "id": "V3_Vu7uv45dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminativeRBMModel(nn.Module):\n",
        "    def __init__(self, input_dim=784, num_classes=10, hidden_dim=256):\n",
        "        super(DiscriminativeRBMModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.b = nn.Parameter(torch.zeros(input_dim))\n",
        "        self.d = nn.Parameter(torch.zeros(num_classes))\n",
        "        self.W = nn.Parameter(torch.randn(input_dim, hidden_dim) * 0.1)\n",
        "        self.U = nn.Parameter(torch.randn(num_classes, hidden_dim) * 0.1)\n",
        "        self.c = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    def free_energy(self, x, y):\n",
        "        vbias_term = torch.matmul(x, self.b) + torch.matmul(y, self.d)\n",
        "        hidden_linear = torch.matmul(x, self.W) + torch.matmul(y, self.U) + self.c\n",
        "        hidden_linear = torch.clamp(hidden_linear, max=50)\n",
        "        hidden_term = torch.sum(torch.log(1 + torch.exp(hidden_linear)), dim=1)\n",
        "        return -vbias_term - hidden_term\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        logits = []\n",
        "        for label in range(self.num_classes):\n",
        "            y_candidate = torch.zeros(batch_size, self.num_classes, device=x.device)\n",
        "            y_candidate[:, label] = 1.0\n",
        "            F = self.free_energy(x, y_candidate)\n",
        "            logits.append((-F).unsqueeze(1))\n",
        "        logits = torch.cat(logits, dim=1)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "iL2eC6LrqnM-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model:"
      ],
      "metadata": {
        "id": "TvKnKXhpq0Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DiscriminativeRBMModel(input_dim=784, num_classes=10, hidden_dim=256).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "num_epochs = 20\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        images = (images > 0.5).float()\n",
        "        images = images.view(images.size(0), -1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(trainset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgnpYIsfq209",
        "outputId": "174fd12a-f62b-48cc-f52d-8ae634a468b0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.23249\n",
            "Epoch 2/20, Loss: 0.10057\n",
            "Epoch 3/20, Loss: 0.07117\n",
            "Epoch 4/20, Loss: 0.05590\n",
            "Epoch 5/20, Loss: 0.05026\n",
            "Epoch 6/20, Loss: 0.04625\n",
            "Epoch 7/20, Loss: 0.03995\n",
            "Epoch 8/20, Loss: 0.03455\n",
            "Epoch 9/20, Loss: 0.03262\n",
            "Epoch 10/20, Loss: 0.03552\n",
            "Epoch 11/20, Loss: 0.03241\n",
            "Epoch 12/20, Loss: 0.02667\n",
            "Epoch 13/20, Loss: 0.02544\n",
            "Epoch 14/20, Loss: 0.02424\n",
            "Epoch 15/20, Loss: 0.02707\n",
            "Epoch 16/20, Loss: 0.03250\n",
            "Epoch 17/20, Loss: 0.02311\n",
            "Epoch 18/20, Loss: 0.02106\n",
            "Epoch 19/20, Loss: 0.02045\n",
            "Epoch 20/20, Loss: 0.02077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model:"
      ],
      "metadata": {
        "id": "tOmSq1jHqth_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        images = (images > 0.5).float()\n",
        "        images = images.view(images.size(0), -1)\n",
        "        logits = model(images)\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Discriminative Model Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv84TXVmqwZ5",
        "outputId": "dffb8516-42ad-4918-93af-8583bd88055c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminative Model Accuracy: 96.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid RBM"
      ],
      "metadata": {
        "id": "X7u0c_kqxAiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the hybrid model using the previous codes for generative and discriminative models:"
      ],
      "metadata": {
        "id": "SRMqDZzDxhdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "class HybridRBM:\n",
        "    def __init__(self, n_visible, n_hidden, n_labels, learning_rate=0.01, alpha=0.1, epochs=20, batch_size=128):\n",
        "        \"\"\"\n",
        "        Initializes the Hybrid RBM combining Discriminative and Generative Loss.\n",
        "\n",
        "        Parameters:\n",
        "        - n_visible: number of visible neurons (e.g., 784 for MNIST).\n",
        "        - n_hidden: number of hidden neurons.\n",
        "        - n_labels: number of label classes (e.g., 10 for MNIST).\n",
        "        - learning_rate: learning rate for weight updates.\n",
        "        - alpha: weight for the generative loss in the hybrid loss function.\n",
        "        - epochs: number of training epochs.\n",
        "        - batch_size: batch size for training.\n",
        "        \"\"\"\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_labels = n_labels\n",
        "        self.learning_rate = learning_rate\n",
        "        self.alpha = alpha\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W = np.random.randn(n_visible, n_hidden) * 0.01\n",
        "        self.U = np.random.randn(n_labels, n_hidden) * 0.01\n",
        "        self.b = np.zeros(n_visible)\n",
        "        self.c = np.zeros(n_hidden)\n",
        "        self.d = np.zeros(n_labels)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (prob > np.random.rand(*prob.shape)).astype(np.float32)\n",
        "\n",
        "    def contrastive_divergence(self, x_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Performs Contrastive Divergence (CD-1) to update weights.\n",
        "        Returns reconstructed x and y.\n",
        "        \"\"\"\n",
        "        batch_size = x_batch.shape[0]\n",
        "\n",
        "        # Positive Phase\n",
        "        h_prob = self.sigmoid(np.dot(x_batch, self.W) + np.dot(y_batch, self.U) + self.c)\n",
        "        h_sample = self.sample(h_prob)\n",
        "\n",
        "        # Negative Phase (Contrastive Divergence)\n",
        "        x_recon_prob = self.sigmoid(np.dot(h_sample, self.W.T) + self.b)\n",
        "        x_recon_sample = self.sample(x_recon_prob)\n",
        "\n",
        "        y_recon_prob = self.sigmoid(np.dot(h_sample, self.U.T) + self.d)\n",
        "        y_recon_sample = self.sample(y_recon_prob)\n",
        "\n",
        "        h_recon_prob = self.sigmoid(np.dot(x_recon_sample, self.W) + np.dot(y_recon_sample, self.U) + self.c)\n",
        "\n",
        "        # Gradient updates for L_disc and L_gen\n",
        "        grad_W_gen = (np.dot(x_batch.T, h_prob) - np.dot(x_recon_sample.T, h_recon_prob)) / batch_size\n",
        "        grad_U_gen = (np.dot(y_batch.T, h_prob) - np.dot(y_recon_sample.T, h_recon_prob)) / batch_size\n",
        "        grad_b_gen = np.mean(x_batch - x_recon_sample, axis=0)\n",
        "        grad_c_gen = np.mean(h_prob - h_recon_prob, axis=0)\n",
        "        grad_d_gen = np.mean(y_batch - y_recon_sample, axis=0)\n",
        "\n",
        "        # Discriminative Gradient\n",
        "        y_pred_prob = np.exp(np.dot(h_sample, self.U.T) + self.d)\n",
        "        y_pred_prob /= np.sum(y_pred_prob, axis=1, keepdims=True)\n",
        "        grad_U_disc = np.dot((y_batch - y_pred_prob).T, h_sample) / batch_size\n",
        "        grad_d_disc = np.mean(y_batch - y_pred_prob, axis=0)\n",
        "\n",
        "        # Update weights using hybrid loss\n",
        "        self.W += self.learning_rate * grad_W_gen\n",
        "        self.U += self.learning_rate * (grad_U_disc + self.alpha * grad_U_gen)\n",
        "        self.b += self.learning_rate * grad_b_gen\n",
        "        self.c += self.learning_rate * grad_c_gen\n",
        "        self.d += self.learning_rate * (grad_d_disc + self.alpha * grad_d_gen)\n",
        "\n",
        "        return x_recon_sample, y_recon_sample\n",
        "\n",
        "    def train(self, X_train, Y_train):\n",
        "        \"\"\"\n",
        "        Trains the Hybrid RBM model using Contrastive Divergence with Hybrid Loss.\n",
        "        \"\"\"\n",
        "        print(\"Starting training...\")\n",
        "        for epoch in range(self.epochs):\n",
        "            epoch_loss = 0\n",
        "            num_batches = X_train.shape[0] // self.batch_size\n",
        "\n",
        "            for i in range(0, X_train.shape[0], self.batch_size):\n",
        "                x_batch = X_train[i:i + self.batch_size]\n",
        "                y_batch = Y_train[i:i + self.batch_size]\n",
        "\n",
        "                # Performing Contrastive Divergence\n",
        "                x_recon, y_recon = self.contrastive_divergence(x_batch, y_batch)\n",
        "\n",
        "                # Computing Reconstruction Loss\n",
        "                loss_x = np.mean((x_batch - x_recon) ** 2)\n",
        "                loss_y = np.mean((y_batch - y_recon) ** 2)\n",
        "                batch_loss = (loss_x + loss_y) / 2\n",
        "\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.5f}\")\n",
        "\n",
        "    def classify(self, X_test, Y_test):\n",
        "        \"\"\"\n",
        "        Classifies input data using the trained Hybrid RBM.\n",
        "        \"\"\"\n",
        "        h_test = self.sigmoid(np.dot(X_test, self.W) + np.dot(Y_test, self.U) + self.c)\n",
        "        y_pred = np.argmax(np.dot(h_test, self.U.T) + self.d, axis=1)\n",
        "        y_true = np.argmax(Y_test, axis=1)\n",
        "\n",
        "        accuracy = np.mean(y_pred == y_true)\n",
        "        print(f\"Classification accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "aW6wkCtnxDpG"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model:"
      ],
      "metadata": {
        "id": "k8kIzChqxxsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load MNIST dataset from OpenML\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "x_data = mnist.data.to_numpy()\n",
        "y_data = mnist.target.to_numpy().astype(int)\n",
        "\n",
        "# Preprocess data: Binarize the images (convert to 0 and 1)\n",
        "x_data = (x_data > 127).astype(np.float32)\n",
        "\n",
        "# One-hot encode labels\n",
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "y_data = ohe.fit_transform(y_data.reshape(-1, 1))\n",
        "\n",
        "# Split into train and test sets\n",
        "x_train, x_test = x_data[:60000], x_data[60000:]\n",
        "y_train, y_test = y_data[:60000], y_data[60000:]"
      ],
      "metadata": {
        "id": "BvPKtYPvx5Ga"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters an training for alpha = 0.01\n",
        "n_visible = 784\n",
        "n_hidden = 256\n",
        "n_labels = 10\n",
        "learning_rate = 0.1\n",
        "alpha = 0.01\n",
        "epochs = 15\n",
        "batch_size = 128\n",
        "\n",
        "hybrid_rbm_a001 = HybridRBM(n_visible, n_hidden, n_labels, learning_rate, alpha, epochs, batch_size)\n",
        "hybrid_rbm_a001.train(x_train, y_train)"
      ],
      "metadata": {
        "id": "eEwdfdDA5DkC",
        "outputId": "ebca864b-ce17-4d7b-f899-4424f671f30f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/15, Loss: 0.15441\n",
            "Epoch 2/15, Loss: 0.08110\n",
            "Epoch 3/15, Loss: 0.06630\n",
            "Epoch 4/15, Loss: 0.05909\n",
            "Epoch 5/15, Loss: 0.05498\n",
            "Epoch 6/15, Loss: 0.05189\n",
            "Epoch 7/15, Loss: 0.04921\n",
            "Epoch 8/15, Loss: 0.04744\n",
            "Epoch 9/15, Loss: 0.04603\n",
            "Epoch 10/15, Loss: 0.04465\n",
            "Epoch 11/15, Loss: 0.04369\n",
            "Epoch 12/15, Loss: 0.04261\n",
            "Epoch 13/15, Loss: 0.04185\n",
            "Epoch 14/15, Loss: 0.04076\n",
            "Epoch 15/15, Loss: 0.04024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters an training for alpha = 0.1\n",
        "n_visible = 784\n",
        "n_hidden = 256\n",
        "n_labels = 10\n",
        "learning_rate = 0.1\n",
        "alpha = 0.1\n",
        "epochs = 15\n",
        "batch_size = 128\n",
        "\n",
        "hybrid_rbm_a010 = HybridRBM(n_visible, n_hidden, n_labels, learning_rate, alpha, epochs, batch_size)\n",
        "hybrid_rbm_a010.train(x_train, y_train)"
      ],
      "metadata": {
        "id": "AQGqP5Rzxy1d",
        "outputId": "4439fc8f-77d4-4933-f490-a95248c70073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/15, Loss: 0.11400\n",
            "Epoch 2/15, Loss: 0.06849\n",
            "Epoch 3/15, Loss: 0.05913\n",
            "Epoch 4/15, Loss: 0.05406\n",
            "Epoch 5/15, Loss: 0.05052\n",
            "Epoch 6/15, Loss: 0.04806\n",
            "Epoch 7/15, Loss: 0.04610\n",
            "Epoch 8/15, Loss: 0.04433\n",
            "Epoch 9/15, Loss: 0.04290\n",
            "Epoch 10/15, Loss: 0.04183\n",
            "Epoch 11/15, Loss: 0.04075\n",
            "Epoch 12/15, Loss: 0.03977\n",
            "Epoch 13/15, Loss: 0.03886\n",
            "Epoch 14/15, Loss: 0.03812\n",
            "Epoch 15/15, Loss: 0.03762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters an training for alpha = 1\n",
        "n_visible = 784\n",
        "n_hidden = 256\n",
        "n_labels = 10\n",
        "learning_rate = 0.1\n",
        "alpha = 1\n",
        "epochs = 15\n",
        "batch_size = 128\n",
        "\n",
        "hybrid_rbm_a100 = HybridRBM(n_visible, n_hidden, n_labels, learning_rate, alpha, epochs, batch_size)\n",
        "hybrid_rbm_a100.train(x_train, y_train)"
      ],
      "metadata": {
        "id": "MIwwAUJH5c-R",
        "outputId": "c6e24624-e5ea-4813-98f3-1c78f29c6c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/15, Loss: 0.09945\n",
            "Epoch 2/15, Loss: 0.05959\n",
            "Epoch 3/15, Loss: 0.05115\n",
            "Epoch 4/15, Loss: 0.04666\n",
            "Epoch 5/15, Loss: 0.04341\n",
            "Epoch 6/15, Loss: 0.04107\n",
            "Epoch 7/15, Loss: 0.03920\n",
            "Epoch 8/15, Loss: 0.03731\n",
            "Epoch 9/15, Loss: 0.03609\n",
            "Epoch 10/15, Loss: 0.03484\n",
            "Epoch 11/15, Loss: 0.03396\n",
            "Epoch 12/15, Loss: 0.03283\n",
            "Epoch 13/15, Loss: 0.03198\n",
            "Epoch 14/15, Loss: 0.03129\n",
            "Epoch 15/15, Loss: 0.03062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now evaluate the models and comparing:"
      ],
      "metadata": {
        "id": "YlJaFdJ95vtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "print(\"Accuracy for alpha = 0.01 : \")\n",
        "hybrid_rbm_a001.classify(x_test, y_test)\n",
        "print(\"Accuracy for alpha = 0.1 : \")\n",
        "hybrid_rbm_a010.classify(x_test, y_test)\n",
        "print(\"Accuracy for alpha = 1 : \")\n",
        "hybrid_rbm_a100.classify(x_test, y_test)"
      ],
      "metadata": {
        "id": "0fwqyDfM5vZu",
        "outputId": "a7e9df54-6675-4c8c-fa60-433fe430ef75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for alpha = 0.01 : \n",
            "Classification accuracy: 96.11%\n",
            "Accuracy for alpha = 0.1 : \n",
            "Classification accuracy: 96.51%\n",
            "Accuracy for alpha = 1 : \n",
            "Classification accuracy: 97.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, when we increase the $\\alpha$, the accuracy increases."
      ],
      "metadata": {
        "id": "yKjJdVfK52Xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semi-Supervised"
      ],
      "metadata": {
        "id": "xat-z6bABf_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implemented model is as follows:"
      ],
      "metadata": {
        "id": "0R31JqkHBocb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SemiSupervisedRBM:\n",
        "    def __init__(self, n_visible, n_hidden, n_labels, learning_rate=0.01, alpha=0.1, epochs=20, batch_size=128):\n",
        "        \"\"\"\n",
        "        Initializes a Semi-Supervised RBM with generative and discriminative losses.\n",
        "        \"\"\"\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_labels = n_labels\n",
        "        self.learning_rate = learning_rate\n",
        "        self.alpha = alpha\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W = np.random.randn(n_visible, n_hidden) * 0.01\n",
        "        self.U = np.random.randn(n_labels, n_hidden) * 0.01\n",
        "        self.b = np.zeros(n_visible)\n",
        "        self.c = np.zeros(n_hidden)\n",
        "        self.d = np.zeros(n_labels)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample(self, prob):\n",
        "        return (prob > np.random.rand(*prob.shape)).astype(np.float32)\n",
        "\n",
        "    def contrastive_divergence(self, x_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Performs Contrastive Divergence (CD-1) to update weights.\n",
        "        Returns reconstructed x and y.\n",
        "        \"\"\"\n",
        "        batch_size = x_batch.shape[0]\n",
        "\n",
        "        # Positive phase\n",
        "        h_prob = self.sigmoid(np.dot(x_batch, self.W) + np.dot(y_batch, self.U) + self.c)\n",
        "        h_sample = self.sample(h_prob)\n",
        "\n",
        "        # Reconstruction of x and y\n",
        "        x_recon_prob = self.sigmoid(np.dot(h_sample, self.W.T) + self.b)\n",
        "        x_recon_sample = self.sample(x_recon_prob)\n",
        "\n",
        "        y_recon_prob = self.sigmoid(np.dot(h_sample, self.U.T) + self.d)\n",
        "        y_recon_sample = self.sample(y_recon_prob)\n",
        "\n",
        "        # Reconstruction of h\n",
        "        h_recon_prob = self.sigmoid(np.dot(x_recon_sample, self.W) + np.dot(y_recon_sample, self.U) + self.c)\n",
        "\n",
        "        # Updating weights\n",
        "        self.W += self.learning_rate * ((np.dot(x_batch.T, h_prob) - np.dot(x_recon_sample.T, h_recon_prob)) / batch_size)\n",
        "        self.U += self.learning_rate * ((np.dot(y_batch.T, h_prob) - np.dot(y_recon_sample.T, h_recon_prob)) / batch_size)\n",
        "        self.b += self.learning_rate * np.mean(x_batch - x_recon_sample, axis=0)\n",
        "        self.c += self.learning_rate * np.mean(h_prob - h_recon_prob, axis=0)\n",
        "        self.d += self.learning_rate * np.mean(y_batch - y_recon_sample, axis=0)\n",
        "\n",
        "        return x_recon_sample, y_recon_sample\n",
        "\n",
        "    def compute_loss(self, x_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Computes the hybrid loss with discriminative and generative terms.\n",
        "        \"\"\"\n",
        "        # Discriminative Loss (L_disc) - Softmax cross-entropy for classification\n",
        "        h_prob = self.sigmoid(np.dot(x_batch, self.W) + self.c)  # Hidden layer probabilities\n",
        "        logits = np.dot(h_prob, self.U.T) + self.d  # Logits for each class (batch_size, n_labels)\n",
        "\n",
        "        # Apply softmax to get probabilities for each class\n",
        "        y_pred_prob = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
        "\n",
        "        # Discriminative Loss (cross-entropy)\n",
        "        loss_disc = -np.mean(np.sum(y_batch * np.log(y_pred_prob + 1e-8), axis=1))\n",
        "\n",
        "        # Generative Loss (L_gen)\n",
        "        x_recon_sample, y_recon_sample = self.contrastive_divergence(x_batch, y_batch)\n",
        "\n",
        "        loss_gen_x = np.mean((x_batch - x_recon_sample) ** 2)\n",
        "        loss_gen_y = np.mean((y_batch - y_recon_sample) ** 2)\n",
        "        loss_gen = (loss_gen_x + loss_gen_y) / 2\n",
        "\n",
        "        # Hybrid Loss\n",
        "        loss = loss_disc + self.alpha * loss_gen\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train(self, X_train, Y_train):\n",
        "        \"\"\"\n",
        "        Trains the Semi-Supervised RBM using both labeled and unlabeled data.\n",
        "        \"\"\"\n",
        "        print(\"Starting training...\")\n",
        "        for epoch in range(self.epochs):\n",
        "            epoch_loss = 0\n",
        "            num_batches = X_train.shape[0] // self.batch_size\n",
        "\n",
        "            for i in range(0, X_train.shape[0], self.batch_size):\n",
        "                x_batch = X_train[i:i + self.batch_size]\n",
        "                y_batch = Y_train[i:i + self.batch_size]\n",
        "\n",
        "                if y_batch.shape[1] != self.n_labels:\n",
        "                    y_batch = y_batch.T\n",
        "\n",
        "                # Perform Contrastive Divergence and compute hybrid loss\n",
        "                x_recon_sample, y_recon_sample = self.contrastive_divergence(x_batch, y_batch)\n",
        "                loss = self.compute_loss(x_batch, y_batch)\n",
        "                epoch_loss += loss\n",
        "\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {avg_loss:.5f}\")\n",
        "\n",
        "    def classify(self, X_test, Y_test):\n",
        "        \"\"\"\n",
        "        Classifies input data using the trained Semi-Supervised RBM.\n",
        "        \"\"\"\n",
        "        h_test = self.sigmoid(np.dot(X_test, self.W) + np.dot(Y_test, self.U) + self.c.reshape(1, -1))\n",
        "        y_pred = np.argmax(np.dot(h_test, self.U.T) + self.d, axis=1)\n",
        "        y_true = np.argmax(Y_test, axis=1)\n",
        "\n",
        "        accuracy = np.mean(y_pred == y_true)\n",
        "        print(f\"Classification accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "HiB45AcR8FXu"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dataset in the way needed:"
      ],
      "metadata": {
        "id": "Tbg5d-bpBzmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "x_data = mnist.data.to_numpy()\n",
        "y_data = mnist.target.to_numpy().astype(int)\n",
        "\n",
        "# Preprocess data: Binarize the images\n",
        "x_data = (x_data > 127).astype(np.float32)\n",
        "\n",
        "# One-hot encode labels\n",
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "y_data = ohe.fit_transform(y_data.reshape(-1, 1))\n",
        "\n",
        "# Split into train and test sets\n",
        "x_train, x_test = x_data[:60000], x_data[60000:]\n",
        "y_train, y_test = y_data[:60000], y_data[60000:]\n",
        "\n",
        "# Number of samples to use from labeled data (n)\n",
        "n = int(0.1 * len(x_train))\n",
        "\n",
        "# Split the labeled (D_sup) and unlabeled (D_unsup) datasets\n",
        "x_train_sup = x_train[:n]\n",
        "y_train_sup = y_train[:n]\n",
        "x_train_unsup = x_train[n:]\n",
        "y_train_unsup = y_train[n:]"
      ],
      "metadata": {
        "id": "OIeIvReuB2eV"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train the model and evaluate it:"
      ],
      "metadata": {
        "id": "0mhmLlnWB-ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "n_visible = 784\n",
        "n_hidden = 256\n",
        "n_classes = 10\n",
        "learning_rate = 0.1\n",
        "alpha = 0.1\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "semi_supervised_rbm = SemiSupervisedRBM(n_visible, n_hidden, n_classes, learning_rate, alpha, epochs, batch_size)\n",
        "semi_supervised_rbm.train(x_train_sup, y_train_sup)\n",
        "\n",
        "# Evaluate the model\n",
        "semi_supervised_rbm.classify(x_test, y_test)\n"
      ],
      "metadata": {
        "id": "7gPnrF-bB_5-",
        "outputId": "92114875-7a1d-48fd-862d-ef8ca6a7b992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/20, Loss: 1.20634\n",
            "Epoch 2/20, Loss: 0.51618\n",
            "Epoch 3/20, Loss: 0.39001\n",
            "Epoch 4/20, Loss: 0.34088\n",
            "Epoch 5/20, Loss: 0.31114\n",
            "Epoch 6/20, Loss: 0.29344\n",
            "Epoch 7/20, Loss: 0.27572\n",
            "Epoch 8/20, Loss: 0.26166\n",
            "Epoch 9/20, Loss: 0.25385\n",
            "Epoch 10/20, Loss: 0.24632\n",
            "Epoch 11/20, Loss: 0.23986\n",
            "Epoch 12/20, Loss: 0.23468\n",
            "Epoch 13/20, Loss: 0.22540\n",
            "Epoch 14/20, Loss: 0.22221\n",
            "Epoch 15/20, Loss: 0.21702\n",
            "Epoch 16/20, Loss: 0.21501\n",
            "Epoch 17/20, Loss: 0.20645\n",
            "Epoch 18/20, Loss: 0.20083\n",
            "Epoch 19/20, Loss: 0.19781\n",
            "Epoch 20/20, Loss: 0.19266\n",
            "Classification accuracy: 95.45%\n"
          ]
        }
      ]
    }
  ]
}